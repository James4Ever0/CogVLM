{
    "0": {
        "file_id": 0,
        "content": "/README.md",
        "type": "filepath"
    },
    "1": {
        "file_id": 0,
        "content": "CogVLM and CogAgent AI models have enhanced visual, dialogue, and Grounding capabilities with HuggingFace training support. CogAgent is open-source, GUI-supported, and utilizes templates for single-round dialogues.",
        "type": "summary"
    },
    "2": {
        "file_id": 0,
        "content": "# CogVLM & CogAgent\nüìó [‰∏≠ÊñáÁâàREADME](./README_zh.md)\nüî•üî•üî•  üÜï: ```2023/12/26```: We have released the [CogVLM-SFT-311K](dataset.md) dataset, \nwhich contains over 150,000 pieces of data that we used for **CogVLM v1.0 only** training. \nWelcome to follow and use.\nüåü **Jump to detailed introduction: [Introduction to CogVLM](#introduction-to-cogvlm)Ôºå\nüÜï [Introduction to CogAgent](#introduction-to-cogagent)**\nüìî For more detailed usage information, please refer to: [CogVLM & CogAgent's technical documentation (in Chinese)](https://zhipu-ai.feishu.cn/wiki/LXQIwqo1OiIVTykMh9Lc3w1Fn7g) \n<table>\n  <tr>\n    <td>\n      <h2> CogVLM </h2>\n      <p> üìñ  Paper: <a href=\"https://arxiv.org/abs/2311.03079\">CogVLM: Visual Expert for Pretrained Language Models</a></p>\n      <p><b>CogVLM</b> is a powerful open-source visual language model (VLM). CogVLM-17B has 10 billion visual parameters and 7 billion language parameters, <b>supporting image understanding and multi-turn dialogue with a resolution of 490*490</b>.</p>\n      <p><b>",
        "type": "code",
        "location": "/README.md:1-19"
    },
    "3": {
        "file_id": 0,
        "content": "This code seems to be the introductory part of a README file, which is describing CogVLM and CogAgent. It mentions that there's a Chinese version available, and it provides links for more detailed technical documentation. The code also highlights the release of a new dataset called CogVLM-SFT-311K.",
        "type": "comment"
    },
    "4": {
        "file_id": 0,
        "content": "CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks</b>, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC.</p>\n    </td>\n    <td>\n      <h2> CogAgent </h2>\n      <p> üìñ  Paper: <a href=\"https://arxiv.org/abs/2312.08914\">CogAgent: A Visual Language Model for GUI Agents </a></p>\n      <p><b>CogAgent</b> is an open-source visual language model improved based on CogVLM. CogAgent-18B has 11 billion visual parameters and 7 billion language parameters, <b>supporting image understanding at a resolution of 1120*1120</b>. <b>On top of the capabilities of CogVLM, it further possesses GUI image Agent capabilities</b>.</p>\n      <p> <b>CogAgent-18B achieves state-of-the-art generalist performance on 9 classic cross-modal benchmarks</b>, including VQAv2, OK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. <b>It significantly surpasses existing models on GUI operation datasets</b> including AITW and Mind2Web.</p>",
        "type": "code",
        "location": "/README.md:19-25"
    },
    "5": {
        "file_id": 0,
        "content": "CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC.\"\n\n\"CogAgent is an open-source visual language model improved based on CogVLM. CogAgent-18B has 11 billion visual parameters and 7 billion language parameters, supporting image understanding at a resolution of 1120*1120. On top of the capabilities of CogVLM, it further possesses GUI image Agent capabilities.\"\n\n\"CogAgent-18B achieves state-of-the-art generalist performance on 9 classic cross-modal benchmarks, including VQAv2, OK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. It significantly surpasses existing models on GUI operation datasets including AITW and Mind2Web.",
        "type": "comment"
    },
    "6": {
        "file_id": 0,
        "content": "    </td>\n  </tr>\n  <tr>\n    <td colspan=\"2\" align=\"center\">\n      <p>üåê Web Demo for both CogVLM and CogAgent: <a href=\"http://36.103.203.44:7861\">this link</a></p>\n    </td>\n  </tr>\n</table>\n**Table of Contents**\n- [CogVLM \\& CogAgent](#cogvlm--cogagent)\n    - [Release](#release)\n    - [Get Started](#get-started)\n        - [Option 1: Inference Using Web Demo.](#option-1-inference-using-web-demo)\n        - [Option 2ÔºöDeploy CogVLM / CogAgent by yourself](#option-2deploy-cogvlm--cogagent-by-yourself)\n            - [Situation 2.1 CLI (SAT version)](#situation-21-cli-sat-version)\n            - [Situation 2.2 CLI (Huggingface version)](#situation-22-cli-huggingface-version)\n            - [Situation 2.3 Web Demo](#situation-23-web-demo)\n        - [Option 3ÔºöFinetuning CogAgent / CogVLM](#option-3finetuning-cogagent--cogvlm)\n        - [Option 4: OpenAI Vision format](#option-4-openai-vision-format)\n        - [Hardware requirement](#hardware-requirement)\n        - [Model checkpoints](#model-checkpoints)\n    - [Introduction to CogVLM](#introduction-to-cogvlm)",
        "type": "code",
        "location": "/README.md:26-50"
    },
    "7": {
        "file_id": 0,
        "content": "This code appears to be a table of contents for a README file, listing various sections and options related to CogVLM and CogAgent. It provides links and brief descriptions for each section, such as getting started with the web demo or deploying the models by yourself using different scenarios like CLI or web demo. The code also mentions finetuning, hardware requirements, and available model checkpoints.",
        "type": "comment"
    },
    "8": {
        "file_id": 0,
        "content": "        - [Examples](#examples)\n    - [Introduction to CogAgent](#introduction-to-cogagent)\n        - [GUI Agent Examples](#gui-agent-examples)\n    - [Cookbook](#cookbook)\n        - [Task Prompts](#task-prompts)\n        - [Which --version to use](#which---version-to-use)\n        - [FAQ](#faq)\n    - [License](#license)\n    - [Citation \\& Acknowledgements](#citation--acknowledgements)\n## Release\n- üî•üî•üî•  **News**: ```2023/12/26```: We have released the [CogVLM-SFT-311K](dataset.md) dataset, \n  which contains over 150,000 pieces of data that we used for **CogVLM v1.0 only** training. Welcome to follow and use.\n- üî•üî• **News**: ```2023/12/18```: **New Web UI Launched!** We have launched a new web UI based on Streamlit,\n  users can painlessly talk to CogVLM, CogAgent in our UI. Have a better user experience.\n- üî• **News**: ```2023/12/15```: **CogAgent Officially Launched!** CogAgent is an image understanding model developed\n  based on CogVLM. It features **visual-based GUI Agent capabilities** and has further enhancements in image",
        "type": "code",
        "location": "/README.md:51-67"
    },
    "9": {
        "file_id": 0,
        "content": "Changelog: Release updates, new web UI launched using Streamlit, and the launch of CogAgent.\n```\n\nComment for code:\nNew features added to the CogVLM model, including a web interface and an image understanding model called CogAgent.",
        "type": "comment"
    },
    "10": {
        "file_id": 0,
        "content": "  understanding. It supports image input with a resolution of 1120*1120, and possesses multiple abilities including\n  multi-turn dialogue with images, GUI Agent, Grounding, and more.\n- **News**: ```2023/12/8``` We have updated the checkpoint of cogvlm-grounding-generalist to\n  cogvlm-grounding-generalist-v1.1, with image augmentation during training, therefore more robust.\n  See [details](#introduction-to-cogvlm).\n- **News**: ```2023/12/7``` CogVLM supports **4-bit quantization** now! You can inference with just **11GB** GPU memory!\n  See [details](#CLI).\n- **News**: ```2023/11/20``` We have updated the checkpoint of cogvlm-chat to cogvlm-chat-v1.1, unified the versions of\n  chat and VQA, and refreshed the SOTA on various datasets. See [details](#introduction-to-cogvlm)\n- **News**: ```2023/11/20``` We release **[cogvlm-chat](https://huggingface.co/THUDM/cogvlm-chat-hf)**, **[cogvlm-grounding-generalist](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)/[base](https://huggingface.co/T",
        "type": "code",
        "location": "/README.md:68-81"
    },
    "11": {
        "file_id": 0,
        "content": "This code provides updates and news about the CogVLM, a versatile AI model that supports image input with 1120*1120 resolution. It has multiple abilities like multi-turn dialogue with images, GUI Agent, Grounding, etc. The code mentions recent updates like image augmentation during training for a more robust checkpoint, support for 4-bit quantization to save GPU memory, and updated versions of chat and VQA models.",
        "type": "comment"
    },
    "12": {
        "file_id": 0,
        "content": "HUDM/cogvlm-grounding-base-hf)**, **[cogvlm-base-490](https://huggingface.co/THUDM/cogvlm-base-490-hf)/[224](https://huggingface.co/THUDM/cogvlm-base-224-hf)** on ü§óHuggingface. you can infer with transformers in [a few lines of code](#situation-22-cli-huggingface-version)now!\n- ```2023/10/27``` CogVLM bilingual version is available [online](https://chatglm.cn/)! Welcome to try it out!\n- ```2023/10/5``` CogVLM-17B released„ÄÇ\n## Get Started\n### Option 1: Inference Using Web Demo.\n* Click here to enter [CogVLM & CogAgent Web Demo](http://36.103.203.44:7861/)„ÄÇ\nIf you need to use Agent and Grounding functions, please refer to [Cookbook - Task Prompts](#task-prompts)\n### Option 2ÔºöDeploy CogVLM / CogAgent by yourself\nWe support two GUIs for model inference, **CLI** and **web demo** . If you want to use it in your python code, it is\neasy to modify the CLI scripts for your case.\n<!-- ### Online Web Demo\nWe provide a [web demo](http://36.103.203.44:7861/) based on [Gradio](https://gradio.app). -->\nFirst, we need to install the dependencies.",
        "type": "code",
        "location": "/README.md:81-105"
    },
    "13": {
        "file_id": 0,
        "content": "Code: The code snippet provides instructions for installing dependencies and using CogVLM/CogAgent in various ways. It offers two options - Option 1: Inference Using Web Demo, and Option 2: Deploy CogVLM / CogAgent by yourself. Option 1 directs users to a web demo (http://36.103.203.44:7861/) where they can try out the models without needing to install anything. Option 2 provides guidance on deploying the models using either command-line interface (CLI) or a web demo. The code also mentions that it's easy to modify the CLI scripts for specific use cases in Python code.",
        "type": "comment"
    },
    "14": {
        "file_id": 0,
        "content": "```bash\n# CUDA >= 11.8\npip install -r requirements.txt\npython -m spacy download en_core_web_sm\n```\n**All code for inference is located under the ``basic_demo/`` directory. Please switch to this directory first before\nproceeding with further operations.**\n#### Situation 2.1 CLI (SAT version)\nRun CLI demo via:\n```bash\n# CogAgent\npython cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogagent-vqa --version chat_old --bf16  --stream_chat\n# CogVLM\npython cli_demo_sat.py --from_pretrained cogvlm-chat --version chat_old --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogvlm-grounding-generalist --version base --bf16  --stream_chat\n```\nThe program will automatically download the sat model and interact in the command line. You can generate replies by\nentering instructions and pressing enter.\nEnter `clear` to clear the conversation history and `stop` to stop the program.\nWe also support model parallel inference, which splits model to multiple (2/4/8) GPUs. `--nproc-per-node=[n]` in the",
        "type": "code",
        "location": "/README.md:107-134"
    },
    "15": {
        "file_id": 0,
        "content": "This code provides instructions for running a CLI demo using CogAgent and CogVLM models. The user can choose between different model versions, such as 'chat', 'chat_old', or 'base'. The script also supports model parallel inference by specifying `--nproc-per-node=[n]` to split the model across multiple GPUs. To use the demo, the user should navigate to the `basic_demo/` directory and run the provided commands.",
        "type": "comment"
    },
    "16": {
        "file_id": 0,
        "content": "following command controls the number of used GPUs.\n```\ntorchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16\n```\n- If you want to manually download the weights, you can replace the path after ``--from_pretrained`` with the model\n  path.\n- Our model supports SAT's **4-bit quantization** and **8-bit quantization**.\n  You can change ``--bf16`` to ``--fp16``, or ``--fp16 --quant 4``, or ``--fp16 --quant 8``.\n  For example\n    ```bash\n    python cli_demo_sat.py --from_pretrained cogagent-chat --fp16 --quant 8 --stream_chat\n    python cli_demo_sat.py --from_pretrained cogvlm-chat-v1.1 --fp16 --quant 4 --stream_chat\n    # In SAT versionÔºå--quant should be used with --fp16\n    ```\n- The program provides the following hyperparameters to control the generation process:\n    ```\n    usage: cli_demo_sat.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE]\n    optional arguments:\n    -h, --help            show this help message and exit",
        "type": "code",
        "location": "/README.md:135-160"
    },
    "17": {
        "file_id": 0,
        "content": "The code demonstrates how to run the CogVLM's SAT model for text generation. It uses torchrun command to control the number of GPUs, supports 4-bit and 8-bit quantization, and provides hyperparameters for controlling the generation process such as max_length, top_p, top_k, and temperature.",
        "type": "comment"
    },
    "18": {
        "file_id": 0,
        "content": "    --max_length MAX_LENGTH\n                            max length of the total sequence\n    --top_p TOP_P         top p for nucleus sampling\n    --top_k TOP_K         top k for top k sampling\n    --temperature TEMPERATURE\n                            temperature for sampling\n    ```\n- Click [here](#which---version-to-use) to view the correspondence between different models and the ``--version``\n  parameter.\n#### Situation 2.2 CLI (Huggingface version)\nRun CLI demo via:\n```bash\n# CogAgent\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-vqa-hf --bf16\n# CogVLM\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-grounding-generalist --bf16\n```\n- If you want to manually download the weights, you can replace the path after ``--from_pretrained`` with the model\n  path.\n- You can change ``--bf16`` to ``--fp16``, or ``--quant 4``. For example, our model supports Huggingface's **4-bit",
        "type": "code",
        "location": "/README.md:161-189"
    },
    "19": {
        "file_id": 0,
        "content": "This code provides command-line interface (CLI) examples to run demos for CogAgent and CogVLM models using the HuggingFace version. It shows how to use the --from_pretrained option with different model paths, and also mentions optional parameters such as --bf16, --fp16, or --quant 4 for changing the precision of the model.",
        "type": "comment"
    },
    "20": {
        "file_id": 0,
        "content": "  quantization**:\n    ```bash\n    python cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --quant 4\n    ```\n#### Situation 2.3 Web Demo\nWe also offer a local web demo based on Gradio. First, install Gradio by running: `pip install gradio`. Then download\nand enter this repository and run `web_demo.py`. See the next section for detailed usage:\n```bash\npython web_demo.py --from_pretrained cogagent-chat --version chat --bf16\npython web_demo.py --from_pretrained cogagent-vqa --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-chat-v1.1 --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --bf16\n```\nThe GUI of the web demo looks like:\n<div align=\"center\">\n    <img src=assets/web_demo-min.png width=70% />\n</div>\n### Option 3ÔºöFinetuning CogAgent / CogVLM\nYou may want to use CogVLM in your own task, which needs a **different output style or domain knowledge**. **All code\nfor finetuning is located under the ``finetune_demo/`` directory.**\nWe here provide a finetuning example for **Captcha Recognition** using lora.",
        "type": "code",
        "location": "/README.md:190-219"
    },
    "21": {
        "file_id": 0,
        "content": "This code demonstrates how to finetune CogVLM/CogAgent for a specific task, such as Captcha Recognition using lora.",
        "type": "comment"
    },
    "22": {
        "file_id": 0,
        "content": "1. Start by downloading the [Captcha Images dataset](https://www.kaggle.com/datasets/aadhavvignesh/captcha-images). Once\n   downloaded, extract the contents of the ZIP file.\n2. To create a train/validation/test split in the ratio of 80/5/15, execute the following:\n    ```bash\n    python utils/split_dataset.py\n    ```\n3. Start the fine-tuning process with this command:\n    ```bash\n    bash finetune_demo/finetune_(cogagent/cogvlm)_lora.sh\n    ```\n4. Merge the model to `model_parallel_size=1`: (replace the 4 below with your training `MP_SIZE`)\n    ```bash\n    torchrun --standalone --nnodes=1 --nproc-per-node=4 utils/merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(cogagent/cogvlm490/cogvlm224)\n    ```\n5. Evaluate the performance of your model.\n    ```bash\n    bash finetune_demo/evaluate_(cogagent/cogvlm).sh\n    ```\n### Option 4: OpenAI Vision format\nWe provide the same API examples as `GPT-4V`, which you can view in `openai_demo`.\n1. First, start the node\n```\npython openai_demo/openai_api.py",
        "type": "code",
        "location": "/README.md:221-253"
    },
    "23": {
        "file_id": 0,
        "content": "1. Download the Captcha Images dataset from Kaggle and extract it.\n2. Split the dataset into train, validation, and test sets in 80/5/15 ratio using `split_dataset.py`.\n3. Fine-tune the model with `finetune_(cogagent/cogvlm)_lora.sh`.\n4. Merge the model to MP_SIZE using `merge_model.py`, replacing 4 with your training MP_SIZE.\n5. Evaluate the model's performance with `evaluate_(cogagent/cogvlm).sh`.",
        "type": "comment"
    },
    "24": {
        "file_id": 0,
        "content": "```\n2. Next, run the request example node, which is an example of a continuous dialogue\n```\npython openai_demo/openai_api_request.py\n```\n3. You will get output similar to the following\n```\nThis image showcases a tranquil natural scene with a wooden pathway leading through a field of lush green grass. In the distance, there are trees and some scattered structures, possibly houses or small buildings. The sky is clear with a few scattered clouds, suggesting a bright and sunny day.\n```\n### Hardware requirement\n* Model Inference:\n  For INT4 quantization: 1 * RTX 3090(24G)   (CogAgent takes ~ 12.6GB, CogVLM takes ~ 11GB)\n  For FP16: 1 * A100(80G) or 2 * RTX 3090(24G)\n* Finetuning:\n  For FP16: 4 * A100(80G) *[Recommend]* or 8* RTX 3090(24G).\n### Model checkpoints\nIf you run the `basic_demo/cli_demo*.py` from the code repository, it will automatically download SAT or Hugging Face\nweights. Alternatively, you can choose to manually download the necessary weights.\n- CogAgent\n  |   Model name    | Input resolution |",
        "type": "code",
        "location": "/README.md:254-287"
    },
    "25": {
        "file_id": 0,
        "content": "This code snippet provides information about running a request example node to generate a description of an image. The output is expected to be similar to the provided text description. Hardware requirements are mentioned for model inference and finetuning, along with the recommended number of GPUs. Additionally, the code mentions that automatic weight downloads can occur when running specific scripts or manual downloads can be done as well.",
        "type": "comment"
    },
    "26": {
        "file_id": 0,
        "content": "                             Introduction                             | Huggingface model | SAT model |\n  | :-----------: | :----: | :----------------------------------------------------------: | :------: | :-------: |\n  | cogagent-chat |  1120  | Chat version of CogAgent. Supports GUI Agent, multiple-round  chat and visual grounding. |  [link](https://huggingface.co/THUDM/cogagent-chat-hf)       |    [link](https://huggingface.co/THUDM/CogAgent/tree/main)       |\n  | cogagent-vqa |  1120  | VQA version of CogAgent. Has stronger capabilities in single-turn visual dialogue. Recommended for VQA benchmarks. |  [link](https://huggingface.co/THUDM/cogagent-vqa-hf)       |    [link](https://huggingface.co/THUDM/CogAgent/tree/main)       |\n- CogVLM\n  |          Model name           | Input resolution |                           Introduction                            | Huggingface model | SAT model |\n  | :-------------------------: | :----: | :-------------------------------------------------------: | :------: | :-------: |",
        "type": "code",
        "location": "/README.md:287-295"
    },
    "27": {
        "file_id": 0,
        "content": "This code appears to be a table, specifically a markdown table. It compares several models, categorizing them by their name, input resolution, and providing an introduction and links to the HuggingFace model and SAT (Semantic Analysis Toolkit) model repositories. The table is part of a larger README file for the CogVLM project, likely outlining available models and their features.",
        "type": "comment"
    },
    "28": {
        "file_id": 0,
        "content": "  |         cogvlm-chat-v1.1         |  490   |  Supports multiple rounds of chat and vqa simultaneously, with different prompts.   |  [link](https://huggingface.co/THUDM/cogvlm-chat-hf)        |    [link](https://huggingface.co/THUDM/CogVLM/tree/main)        |\n  |       cogvlm-base-224       |  224   |               The original checkpoint after text-image pretraining.               |   [link](https://huggingface.co/THUDM/cogvlm-base-224-hf)       |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |\n  |       cogvlm-base-490       |  490   |      Amplify the resolution to 490 through position encoding interpolation from `cogvlm-base-224`.      |   [link](https://huggingface.co/THUDM/cogvlm-base-490-hf)       |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |\n  | cogvlm-grounding-generalist |  490   | This checkpoint supports different visual grounding tasks, e.g. REC, Grounding Captioning, etc.  |    [link](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)      |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |",
        "type": "code",
        "location": "/README.md:296-299"
    },
    "29": {
        "file_id": 0,
        "content": "This code provides information about different versions of the CogVLM model available on Hugging Face. It lists their names, sizes, main features, and links to their respective pages for more information and accessing the models.",
        "type": "comment"
    },
    "30": {
        "file_id": 0,
        "content": "## Introduction to CogVLM\n- CogVLM is a powerful **open-source visual language model** (**VLM**). CogVLM-17B has 10 billion vision parameters and\n  7 billion language parameters.\n- CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k\n  captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and rank the 2nd on VQAv2,\n  OKVQA, TextVQA, COCO captioning, etc., **surpassing or matching PaLI-X 55B**. CogVLM can\n  also [chat with you](http://36.103.203.44:7861) about images.\n<div align=\"center\">\n    <img src=assets/metrics-min.png width=50% />\n</div>\n<details>\n<summary>Click to view results on MM-VET, POPE, TouchStone. </summary>\n<table>\n    <tr>\n        <td>Method</td>\n        <td>LLM</td>\n        <td>MM-VET</td>\n        <td>POPE(adversarial)</td>\n        <td>TouchStone</td>\n    </tr>\n    <tr>\n        <td>BLIP-2</td>\n        <td>Vicuna-13B</td>\n        <td>22.4</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Otter</td>",
        "type": "code",
        "location": "/README.md:301-334"
    },
    "31": {
        "file_id": 0,
        "content": "CogVLM is an open-source, powerful VLM with 10B vision and 7B language parameters. It achieves state-of-the-art performance on 10 benchmarks and surpasses/matches PaLI-X 55B.",
        "type": "comment"
    },
    "32": {
        "file_id": 0,
        "content": "        <td>MPT-7B</td>\n        <td>24.7</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>MiniGPT4</td>\n        <td>Vicuna-13B</td>\n        <td>24.4</td>\n        <td>70.4</td>\n        <td>531.7</td>\n    </tr>\n    <tr>\n        <td>InstructBLIP</td>\n        <td>Vicuna-13B</td>\n        <td>25.6</td>\n        <td>77.3</td>\n        <td>552.4</td>\n    </tr>\n    <tr>\n        <td>LLaMA-Adapter v2</td>\n        <td>LLaMA-7B</td>\n        <td>31.4</td>\n        <td>-</td>\n        <td>590.1</td>\n    </tr>\n    <tr>\n        <td>LLaVA</td>\n        <td>LLaMA2-7B</td>\n        <td>28.1</td>\n        <td>66.3</td>\n        <td>602.7</td>\n    </tr>\n    <tr>\n        <td>mPLUG-Owl</td>\n        <td>LLaMA-7B</td>\n        <td>-</td>\n        <td>66.8</td>\n        <td>605.4</td>\n    </tr>\n    <tr>\n        <td>LLaVA-1.5</td>\n        <td>Vicuna-13B</td>\n        <td>36.3</td>\n        <td>84.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Emu</td>\n        <td>LLaMA-13B</td>\n        <td>36.3</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>",
        "type": "code",
        "location": "/README.md:335-388"
    },
    "33": {
        "file_id": 0,
        "content": "This code snippet represents a table with different language models and their corresponding performance metrics. Each row represents a model with columns for the name, base model, inference speed (perplexity), image captioning accuracy (CIDEr), and video question answering accuracy (QA F1). The table provides a comparison of various language models based on these performance metrics.",
        "type": "comment"
    },
    "34": {
        "file_id": 0,
        "content": "    <tr>\n        <td>Qwen-VL-Chat</td>\n        <td>-</td>\n        <td>-</td>\n        <td>-</td>\n        <td>645.2</td>\n    </tr>\n    <tr>\n        <td>DreamLLM</td>\n        <td>Vicuna-7B</td>\n        <td>35.9</td>\n        <td>76.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>CogVLM</td>\n        <td>Vicuna-7B</td>\n        <td> <b>52.8</b> </td>\n        <td><b>87.6</b></td>\n        <td><b>742.0</b></td>\n    </tr>\n</table>\n</details>\n<details>\n<summary>Click to view results of cogvlm-grounding-generalist-v1.1. </summary>\n<table>\n    <tr>\n        <td></td>\n        <td>RefCOCO</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCO+</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCOg</td>\n        <td></td>\n        <td>Visual7W</td>\n    </tr>\n    <tr>\n        <td></td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>test</td>\n        <td>test</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist</td>",
        "type": "code",
        "location": "/README.md:389-443"
    },
    "35": {
        "file_id": 0,
        "content": "This code is displaying the results of various language models in a table format, including Qwen-VL-Chat, DreamLLM, and CogVLM. The table shows performance metrics like accuracy and speed for different tasks and datasets. The summary sections allow users to view specific results by clicking on them.",
        "type": "comment"
    },
    "36": {
        "file_id": 0,
        "content": "        <td>92.51</td>\n        <td>93.95</td>\n        <td>88.73</td>\n        <td>87.52</td>\n        <td>91.81</td>\n        <td>81.43</td>\n        <td>89.46</td>\n        <td>90.09</td>\n        <td>90.96</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist-v1.1</td>\n        <td>**92.76**</td>\n        <td>**94.75**</td>\n        <td>**88.99**</td>\n        <td>**88.68**</td>\n        <td>**92.91**</td>\n        <td>**83.39**</td>\n        <td>**89.75**</td>\n        <td>**90.79**</td>\n        <td>**91.05**</td>\n    </tr>\n</table>\n</details>\n### Examples\n<!-- CogVLM is powerful for answering various types of visual questions, including **Detailed Description & Visual Question Answering**,  **Complex Counting**, **Visual Math Problem Solving**, **OCR-Free Reasonging**, **OCR-Free Visual Question Answering**, **World Knowledge**, **Referring Expression Comprehension**, **Programming with Visual Input**, **Grounding with Caption**, **Grounding Visual Question Answering**, etc. -->\n* CogVLM can accurately describe images in details with **very few hallucinations**.",
        "type": "code",
        "location": "/README.md:444-473"
    },
    "37": {
        "file_id": 0,
        "content": "This code is displaying a table comparing the performance of different models in various tasks. The model 'cogvim-grounding-generalist-v1.1' has the highest accuracy in most categories.",
        "type": "comment"
    },
    "38": {
        "file_id": 0,
        "content": "    <details>\n    <summary>Click for comparison with LLAVA-1.5 and MiniGPT-4.</summary>\n    <img src=assets/llava-comparison-min.png width=50% />\n    </details>\n    <br>\n* CogVLM can understand and answer various types of questions, and has a **visual grounding** version.\n<div align=\"center\">\n    <img src=assets/pear_grounding.png width=50% />\n</div>\n<br>\n* CogVLM sometimes captures more detailed content than GPT-4V(ision).\n<div align=\"center\">\n    <img src=assets/compare-min.png width=50% />\n</div>\n<!-- ![compare](assets/compare.png) -->\n<br> \n<details>\n<summary>Click to expand more examples.</summary>\n![Chat Examples](assets/chat.png)\n</details>\n## Introduction to CogAgent\nCogAgent is an open-source visual language model improved based on CogVLM. CogAgent-18B has 11 billion visual parameters\nand 7 billion language parameters\nCogAgent-18B achieves state-of-the-art generalist performance on 9 classic cross-modal benchmarks, including VQAv2,\nOK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. It significantly surpasses existing models on GUI",
        "type": "code",
        "location": "/README.md:474-512"
    },
    "39": {
        "file_id": 0,
        "content": "CogVLM can understand and answer various types of questions with a visual grounding version. It sometimes captures more detailed content than GPT-4V. CogAgent is an open-source visual language model improved based on CogVLM, achieving state-of-the-art generalist performance on 9 classic cross-modal benchmarks.",
        "type": "comment"
    },
    "40": {
        "file_id": 0,
        "content": "operation datasets such as AITW and Mind2Web.\nIn addition to all the features already present in CogVLM (visual multi-round dialogue, visual grounding), CogAgent:\n1. Supports higher resolution visual input and dialogue question-answering. **It supports ultra-high-resolution image\n   inputs of 1120x1120.**\n2. **Possesses the capabilities of a visual Agent**, being able to return a plan, next action, and specific operations\n   with coordinates for any given task on any GUI screenshot.\n3. **Enhanced GUI-related question-answering capabilities**, allowing it to handle questions about any GUI screenshot,\n   such as web pages, PC apps, mobile applications, etc.\n4. Enhanced capabilities in OCR-related tasks through improved pre-training and fine-tuning.\n<div align=\"center\">\n    <img src=assets/cogagent_function.jpg width=60% />\n</div>\n### GUI Agent Examples\n<div align=\"center\">\n    <img src=assets/cogagent_main_demo.jpg width=90% />\n</div>\n## Cookbook\n### Task Prompts\n1. **General Multi-Round Dialogue**: Say whatever you want.",
        "type": "code",
        "location": "/README.md:513-542"
    },
    "41": {
        "file_id": 0,
        "content": "This code describes CogAgent, an advanced AI agent with enhanced capabilities in visual input, dialogue question-answering, GUI-related tasks, and OCR-related tasks. It supports ultra-high-resolution image inputs of 1120x1120 and can handle various GUI screenshots.",
        "type": "comment"
    },
    "42": {
        "file_id": 0,
        "content": "2. **GUI Agent Task**: Use the [Agent template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761)\n   and replace \\<TASK\\> with the task instruction enclosed in double quotes. This query can make CogAgent infer Plan and\n   Next Action. If adding ``(with grounding)`` at the end of the query, the model will return a formalized action\n   representation with coordinates.\nFor example, to ask the model how to complete the task \"Search for CogVLM\" on a current GUI screenshot, follow these\nsteps:\n1. Randomly select a template from\n   the [Agent template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761). Here, we\n   choose ``What steps do I need to take to <TASK>?``.\n2. Replace <TASK> with the task instruction enclosed in double quotes, for\n   example, ``What steps do I need to take to \"Search for CogVLM\"?`` . Inputting this to the model yields:\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.",
        "type": "code",
        "location": "/README.md:544-560"
    },
    "43": {
        "file_id": 0,
        "content": "This code describes the process of using a pre-defined template to ask CogAgent questions about how to complete a specific task. The user selects a template, replaces <TASK> with the desired action enclosed in double quotes, and then asks the model for advice on completing the task. If adding \"(with grounding)\", the model will provide an action representation with coordinates.",
        "type": "comment"
    },
    "44": {
        "file_id": 0,
        "content": ">\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n3. If adding ``(with grounding)`` at the end, i.e. changing the input\n   to ``What steps do I need to take to \"Search for CogVLM\"?(with grounding)``, the output of CogAgent would be:\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.\n>\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n> Grounded Operation:[combobox] Search -> TYPE: CogVLM at the box [[212,498,787,564]]\nTip: For GUI Agent tasks, it is recommended to conduct only single-round dialogues for each image for better results.\n3. **Visual Grounding**. Three modes of grounding are supported:\n    - Image description with grounding coordinates (bounding box). Use any template\n      from [caption_with_box template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L537) as model\n      input. For example:",
        "type": "code",
        "location": "/README.md:561-579"
    },
    "45": {
        "file_id": 0,
        "content": "Code snippet explains how to search for \"CogVLM\" on Google and provides a plan to complete this task. The next action is to type 'CogVLM' into the Google search bar. Grounded Operation shows the coordinates of where to type in the search bar ([[212,498,787,564]]). It also mentions that GUI Agent tasks are recommended to be conducted with single-round dialogues for better results and introduces three modes of grounding: image description with grounding coordinates.\n\nCode snippet is from \"CogVLM/README.md\":560-578, in the section about grounded operation and visual grounding.",
        "type": "comment"
    },
    "46": {
        "file_id": 0,
        "content": "   > Can you provide a description of the image and include the coordinates [[x0,y0,x1,y1]] for each mentioned object?\n    - Returning grounding coordinates (bounding box) based on the description of objects. Use any template\n      from [caption2box template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L345),\n      replacing ``<expr>`` with the object's description. For example:\n   > Can you point out *children in blue T-shirts* in the image and provide the bounding boxes of their location?\n    - Providing a description based on bounding box coordinates. Use a template\n      from [box2caption template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L400),\n      replacing ``<objs>`` with the position coordinates. For example:\n   > Tell me what you see within the designated area *[[086,540,400,760]]* in the picture.\n**Format of coordination:** The bounding box coordinates in the model's input and output use the\nformat ``[[x1, y1, x2, y2]]``, with the origin at the top left corner, the x-axis to the right, and the y-axis",
        "type": "code",
        "location": "/README.md:581-596"
    },
    "47": {
        "file_id": 0,
        "content": "This code is related to image grounding, where the model receives a description of objects in an image and returns the bounding box coordinates for those objects. It uses templates from the caption2box and box2caption classes for this purpose. The coordinates are in the format [[x1, y1, x2, y2]], with the origin at the top left corner and the x-axis going right and the y-axis down.",
        "type": "comment"
    },
    "48": {
        "file_id": 0,
        "content": "downward. (x1, y1) and (x2, y2) are the top-left and bottom-right corners, respectively, with values as relative\ncoordinates multiplied by 1000 (prefixed with zeros to three digits).\n### Which --version to use\nDue to differences in model functionalities, different model versions may have distinct ``--version`` specifications for\nthe text processor, meaning the format of the prompts used varies.\n|         model name          | --version |\n|:---------------------------:|:---------:|\n|        cogagent-chat        |   chat    |\n|        cogagent-vqa         | chat_old  |\n|         cogvlm-chat         | chat_old  |\n|      cogvlm-chat-v1.1       | chat_old  |\n| cogvlm-grounding-generalist |   base    |\n|       cogvlm-base-224       |   base    |\n|       cogvlm-base-490       |   base    |\n### FAQ\n* If you have trouble in accessing huggingface.co, you can add `--local_tokenizer /path/to/vicuna-7b-v1.5` to load the\n  tokenizer.\n* If you have trouble in automatically downloading model with üî®[SAT](https://github.com/THUDM/SwissArmyTransformer), try",
        "type": "code",
        "location": "/README.md:597-619"
    },
    "49": {
        "file_id": 0,
        "content": "This code snippet provides information about different versions of models and their corresponding --version specifications for text processor. It also mentions a possible solution for trouble in accessing huggingface.co and automatically downloading models using SAT.",
        "type": "comment"
    },
    "50": {
        "file_id": 0,
        "content": "  downloading from ü§ñ[modelscope](https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary) or\n  ü§ó[huggingface](https://huggingface.co/THUDM/CogVLM) or üí°[wisemodel](https://www.wisemodel.cn/models/ZhipuAI/CogVLM)\n  manually.\n* Download model using üî®[SAT](https://github.com/THUDM/SwissArmyTransformer), the model will be saved to the default\n  location `~/.sat_models`. Change the default location by setting the environment variable `SAT_HOME`. For example, if\n  you want to save the model to `/path/to/my/models`, you can run `export SAT_HOME=/path/to/my/models` before running\n  the python command.\n## License\nThe code in this repository is open source under the [Apache-2.0 license](./LICENSE), while the use of the CogVLM model\nweights must comply with the [Model License](./MODEL_LICENSE).\n## Citation & Acknowledgements\nIf you find our work helpful, please consider citing the following papers\n```\n@misc{wang2023cogvlm,\n      title={CogVLM: Visual Expert for Pretrained Language Models}, \n      author={Weihan ",
        "type": "code",
        "location": "/README.md:620-640"
    },
    "51": {
        "file_id": 0,
        "content": "Downloading CogVLM model from ModelScope, HuggingFace, or Wisemodel. Use SAT for manual download and save to default location ~/.sat_models. Change SAT_HOME environment variable for custom locations. Code is open source under Apache-2.0 license, while Model License applies to CogVLM weights usage. Consider citing \"CogVLM: Visual Expert for Pretrained Language Models\" when helpful.",
        "type": "comment"
    },
    "52": {
        "file_id": 0,
        "content": "Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2311.03079},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n@misc{hong2023cogagent,\n      title={CogAgent: A Visual Language Model for GUI Agents}, \n      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2312.08914},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\nIn the instruction fine-tuning phase of the CogVLM, there are some English image-text data from\nthe [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [LLAVA](https://github.com/haotian-liu/LLaVA), [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction), [LLaVAR](https://github.com/SALT-NLP/LLaVAR)\nand [Shikr",
        "type": "code",
        "location": "/README.md:640-660"
    },
    "53": {
        "file_id": 0,
        "content": "This code appears to be providing citation information for two research papers. The first paper, \"CogAgent: A Visual Language Model for GUI Agents,\" is authored by Wenyi Hong and others, with a year of 2023 and an arXiv eprint number of 2312.08914. The second paper, \"CogVLM/README.md\", also includes English image-text data from several sources: MiniGPT-4, LLAVA, LRV-Instruction, LLaVAR, and Shikr.",
        "type": "comment"
    },
    "54": {
        "file_id": 0,
        "content": "a](https://github.com/shikras/shikra) projects, as well as many classic cross-modal work datasets. We\nsincerely thank them for their contributions.",
        "type": "code",
        "location": "/README.md:660-661"
    },
    "55": {
        "file_id": 0,
        "content": "This code is acknowledging the contributions of various projects and datasets to the CogVLM, expressing gratitude.",
        "type": "comment"
    },
    "56": {
        "file_id": 1,
        "content": "/basic_demo/cli_demo_hf.py",
        "type": "filepath"
    },
    "57": {
        "file_id": 1,
        "content": "This code demonstrates a CLI using CogAgent and CogVLM, utilizing the Vicuna-7b-v1.5 tokenizer for chat interaction with users. It supports GPU with bfloat16 for performance, processes one image at a time, uses argparse for command line arguments, and includes a loop for user input.",
        "type": "summary"
    },
    "58": {
        "file_id": 1,
        "content": "\"\"\"\nThis is a demo for using CogAgent and CogVLM in CLI\nMake sure you have installed vicuna-7b-v1.5 tokenizer model (https://huggingface.co/lmsys/vicuna-7b-v1.5), full checkpoint of vicuna-7b-v1.5 LLM is not required.\nIn this demo, We us chat template, you can use others to replace such as 'vqa'.\nStrongly suggest to use GPU with bfloat16 support, otherwise, it will be slow.\nMention that only one picture can be processed at one conversation, which means you can not replace or insert another picture during the conversation.\n\"\"\"\nimport argparse\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--quant\", choices=[4], type=int, default=None, help='quantization bits')\nparser.add_argument(\"--from_pretrained\", type=str, default=\"THUDM/cogagent-chat-hf\", help='pretrained ckpt')\nparser.add_argument(\"--local_tokenizer\", type=str, default=\"lmsys/vicuna-7b-v1.5\", help='tokenizer path')\nparser.add_argument(\"--fp16\", action=\"store_true\")",
        "type": "code",
        "location": "/basic_demo/cli_demo_hf.py:1-19"
    },
    "59": {
        "file_id": 1,
        "content": "This code is for a command-line interface (CLI) demo using CogAgent and CogVLM. It requires the Vicuna-7b-v1.5 tokenizer model and suggests using GPU with bfloat16 support for better performance. Only one picture can be processed at a time in a conversation, and it supports 4-bit quantization. The code uses argparse to handle command-line arguments.",
        "type": "comment"
    },
    "60": {
        "file_id": 1,
        "content": "parser.add_argument(\"--bf16\", action=\"store_true\")\nargs = parser.parse_args()\nMODEL_PATH = args.from_pretrained\nTOKENIZER_PATH = args.local_tokenizer\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = LlamaTokenizer.from_pretrained(TOKENIZER_PATH)\nif args.bf16:\n    torch_type = torch.bfloat16\nelse:\n    torch_type = torch.float16\nprint(\"========Use torch type as:{} with device:{}========\\n\\n\".format(torch_type, DEVICE))\nif args.quant:\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_PATH,\n        torch_dtype=torch_type,\n        low_cpu_mem_usage=True,\n        load_in_4bit=True,\n        trust_remote_code=True\n    ).eval()\nelse:\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_PATH,\n        torch_dtype=torch_type,\n        low_cpu_mem_usage=True,\n        load_in_4bit=args.quant is not None,\n        trust_remote_code=True\n    ).to(DEVICE).eval()\ntext_only_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\"",
        "type": "code",
        "location": "/basic_demo/cli_demo_hf.py:20-52"
    },
    "61": {
        "file_id": 1,
        "content": "This code is parsing arguments and setting up a Hugging Face AutoModelForCausalLM. It first checks if the --bf16 argument is set, then sets the torch type accordingly (either bfloat16 or float16). If the quant argument is also set, it loads a 4-bit quantized model. The model is then loaded and moved to the appropriate device (CPU or CUDA, if available). Finally, it sets up a text_only_template for chat interaction with the user.",
        "type": "comment"
    },
    "62": {
        "file_id": 1,
        "content": "while True:\n    image_path = input(\"image path >>>>> \")\n    if image_path == '':\n        print('You did not enter image path, the following will be a plain text conversation.')\n        image = None\n        text_only_first_query = True    \n    else:\n        image = Image.open(image_path).convert('RGB')\n    history = []\n    while True:\n        query = input(\"Human:\")\n        if query == \"clear\":\n            break\n        if image is None:\n            if text_only_first_query:\n                query = text_only_template.format(query)\n                text_only_first_query = False\n            else:\n                old_prompt = ''\n                for _, (old_query, response) in enumerate(history):\n                    old_prompt += old_query + \" \" + response + \"\\n\"\n                query = old_prompt + \"USER: {} ASSISTANT:\".format(query)\n        if image is None:\n            input_by_model = model.build_conversation_input_ids(tokenizer, query=query, history=history, template_version='base')\n        else:\n            in",
        "type": "code",
        "location": "/basic_demo/cli_demo_hf.py:54-83"
    },
    "63": {
        "file_id": 1,
        "content": "This code is creating a loop where the user can input either an image path or a query. If an image path is not entered, it will default to plain text conversation. The loop continues until the user enters \"clear\". If no image is provided and it's the first query, it formats the query as per the 'text_only_template'. If there is already a history of queries, it appends all previous queries with their corresponding responses, then adds the current query. It then builds the conversation input for the model based on whether an image is present or not.",
        "type": "comment"
    },
    "64": {
        "file_id": 1,
        "content": "put_by_model = model.build_conversation_input_ids(tokenizer, query=query, history=history, images=[image])\n        inputs = {\n            'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n            'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n            'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n            'images': [[input_by_model['images'][0].to(DEVICE).to(torch_type)]] if image is not None else None,\n        }\n        if 'cross_images' in input_by_model and input_by_model['cross_images']:\n            inputs['cross_images'] = [[input_by_model['cross_images'][0].to(DEVICE).to(torch_type)]]\n        # add any transformers params here.\n        gen_kwargs = {\"max_length\": 2048,\n                      \"do_sample\": False} # \"temperature\": 0.9\n        with torch.no_grad():\n            outputs = model.generate(**inputs, **gen_kwargs)\n            outputs = outputs[:, inputs['input_ids'].shape[1]:]\n            response = tokenizer.decode(outputs[0])",
        "type": "code",
        "location": "/basic_demo/cli_demo_hf.py:83-100"
    },
    "65": {
        "file_id": 1,
        "content": "Building conversation input data and preparing for model generation.",
        "type": "comment"
    },
    "66": {
        "file_id": 1,
        "content": "            response = response.split(\"</s>\")[0]\n            print(\"\\nCog:\", response)\n        history.append((query, response))",
        "type": "code",
        "location": "/basic_demo/cli_demo_hf.py:101-103"
    },
    "67": {
        "file_id": 1,
        "content": "Split response by \" Industrially \" and print the first part, then store in history.",
        "type": "comment"
    },
    "68": {
        "file_id": 2,
        "content": "/basic_demo/cli_demo_sat.py",
        "type": "filepath"
    },
    "69": {
        "file_id": 2,
        "content": "The code provides a command-line interface for a text generation model, allowing users to interact with it through parameters and environment variables. It also contributes to a distributed multi-process chat app handling user input, commands, translations in multiple languages, and broadcasting queries across GPUs if running on multiple devices.",
        "type": "summary"
    },
    "70": {
        "file_id": 2,
        "content": "# -*- encoding: utf-8 -*-\nimport os, sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nimport torch\nimport argparse\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.quantization.kernels import quantize\nfrom sat.model import AutoModel\nfrom utils.utils import chat, llama2_tokenizer, llama2_text_processor_inference, get_image_processor\nfrom utils.models import CogAgentModel, CogVLMModel\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--max_length\", type=int, default=2048, help='max length of the total sequence')\n    parser.add_argument(\"--top_p\", type=float, default=0.4, help='top p for nucleus sampling')\n    parser.add_argument(\"--top_k\", type=int, default=1, help='top k for top k sampling')\n    parser.add_argument(\"--temperature\", type=float, default=.8, help='temperature for sampling')\n    parser.add_argument(\"--chinese\", action='store_true', help='Chinese interface')\n    parser.add_argument(\"--version\", type=str, default=\"chat\",",
        "type": "code",
        "location": "/basic_demo/cli_demo_sat.py:1-22"
    },
    "71": {
        "file_id": 2,
        "content": "This code imports necessary libraries and defines the main function that uses an argument parser to handle user input for prompt length, sampling strategies, temperature, language preference, and model version. It appears to be a command-line interface for interacting with a text generation model.",
        "type": "comment"
    },
    "72": {
        "file_id": 2,
        "content": " choices=['chat', 'vqa', 'chat_old', 'base'], help='version of language process. if there is \\\"text_processor_version\\\" in model_config.json, this option will be overwritten')\n    parser.add_argument(\"--quant\", choices=[8, 4], type=int, default=None, help='quantization bits')\n    parser.add_argument(\"--from_pretrained\", type=str, default=\"cogagent-chat\", help='pretrained ckpt')\n    parser.add_argument(\"--local_tokenizer\", type=str, default=\"lmsys/vicuna-7b-v1.5\", help='tokenizer path')\n    parser.add_argument(\"--fp16\", action=\"store_true\")\n    parser.add_argument(\"--bf16\", action=\"store_true\")\n    parser.add_argument(\"--stream_chat\", action=\"store_true\")\n    args = parser.parse_args()\n    rank = int(os.environ.get('RANK', 0))\n    world_size = int(os.environ.get('WORLD_SIZE', 1))\n    args = parser.parse_args()\n    # load model\n    model, model_args = AutoModel.from_pretrained(\n        args.from_pretrained,\n        args=argparse.Namespace(\n        deepspeed=None,\n        local_rank=rank,\n        rank=rank,",
        "type": "code",
        "location": "/basic_demo/cli_demo_sat.py:22-41"
    },
    "73": {
        "file_id": 2,
        "content": "The code is parsing command-line arguments for a language model. The options include the language processing version, quantization bits, pretrained checkpoint, tokenizer path, and whether to use FP16 or BF16 precision. The script also checks environment variables RANK and WORLD_SIZE before loading the model using the specified parameters.",
        "type": "comment"
    },
    "74": {
        "file_id": 2,
        "content": "        world_size=world_size,\n        model_parallel_size=world_size,\n        mode='inference',\n        skip_init=True,\n        use_gpu_initialization=True if (torch.cuda.is_available() and args.quant is None) else False,\n        device='cpu' if args.quant else 'cuda',\n        **vars(args)\n    ), overwrite_args={'model_parallel_size': world_size} if world_size != 1 else {})\n    model = model.eval()\n    from sat.mpu import get_model_parallel_world_size\n    assert world_size == get_model_parallel_world_size(), \"world size must equal to model parallel size for cli_demo!\"\n    language_processor_version = model_args.text_processor_version if 'text_processor_version' in model_args else args.version\n    print(\"[Language processor version]:\", language_processor_version)\n    tokenizer = llama2_tokenizer(args.local_tokenizer, signal_type=language_processor_version)\n    image_processor = get_image_processor(model_args.eva_args[\"image_size\"][0])\n    cross_image_processor = get_image_processor(model_args.cross_image_pix) if \"cross_image_pix\" in model_args else None",
        "type": "code",
        "location": "/basic_demo/cli_demo_sat.py:42-58"
    },
    "75": {
        "file_id": 2,
        "content": "Creates a model with specified world and model parallel sizes, sets to inference mode if it's not the first process, uses GPU if available and quantization is not set, otherwise uses CPU, asserts that world size equals model parallel size for correct operation, prints language processor version, initializes tokenizer based on local_tokenizer argument and signal type (language processor version), and initializes image processor and cross-image processor if necessary.",
        "type": "comment"
    },
    "76": {
        "file_id": 2,
        "content": "    if args.quant:\n        quantize(model, args.quant)\n        if torch.cuda.is_available():\n            model = model.cuda()\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n    text_processor_infer = llama2_text_processor_inference(tokenizer, args.max_length, model.image_length)\n    if args.chinese:\n        if rank == 0:\n            print('Ê¨¢Ëøé‰ΩøÁî® CogAgent-CLI ÔºåËæìÂÖ•ÂõæÂÉèURLÊàñÊú¨Âú∞Ë∑ØÂæÑËØªÂõæÔºåÁªßÁª≠ËæìÂÖ•ÂÜÖÂÆπÂØπËØùÔºåclear ÈáçÊñ∞ÂºÄÂßãÔºåstop ÁªàÊ≠¢Á®ãÂ∫è')\n    else:\n        if rank == 0:\n            print('Welcome to CogAgent-CLI. Enter an image URL or local file path to load an image. Continue inputting text to engage in a conversation. Type \"clear\" to start over, or \"stop\" to end the program.')\n    with torch.no_grad():\n        while True:\n            history = None\n            cache_image = None\n            if args.chinese:\n                if rank == 0:\n                    image_path = [input(\"ËØ∑ËæìÂÖ•ÂõæÂÉèË∑ØÂæÑÊàñURLÔºö \")]\n                else:\n                    image_path = [None]\n            else:\n                if rank == 0:\n                    image_path = [input(\"Please enter the image path or URL: \")]",
        "type": "code",
        "location": "/basic_demo/cli_demo_sat.py:60-87"
    },
    "77": {
        "file_id": 2,
        "content": "Checking if quantization is needed, moving model to GPU if available, adding autoregressive mixin, initializing text processor, printing welcome message based on language, and starting an infinite loop for text input and image processing.",
        "type": "comment"
    },
    "78": {
        "file_id": 2,
        "content": "                else:\n                    image_path = [None]\n            if world_size > 1:\n                torch.distributed.broadcast_object_list(image_path, 0)\n            image_path = image_path[0]\n            assert image_path is not None\n            if image_path == 'stop':\n                break\n            if args.chinese:\n                if rank == 0:\n                    query = [input(\"Áî®Êà∑Ôºö\")]\n                else:\n                    query = [None]\n            else:\n                if rank == 0:\n                    query = [input(\"User: \")]\n                else:\n                    query = [None]\n            if world_size > 1:\n                torch.distributed.broadcast_object_list(query, 0)\n            query = query[0]\n            assert query is not None\n            while True:\n                if query == \"clear\":\n                    break\n                if query == \"stop\":\n                    sys.exit(0)\n                try:\n                    response, history, cache_image = chat(\n                        image_path,",
        "type": "code",
        "location": "/basic_demo/cli_demo_sat.py:88-120"
    },
    "79": {
        "file_id": 2,
        "content": "Code snippet is part of a distributed multi-process chat application. It checks user input, broadcasts it among processes, and handles various commands like \"stop\" or \"clear\".",
        "type": "comment"
    },
    "80": {
        "file_id": 2,
        "content": "                        model,\n                        text_processor_infer,\n                        image_processor,\n                        query,\n                        history=history,\n                        cross_img_processor=cross_image_processor,\n                        image=cache_image,\n                        max_length=args.max_length,\n                        top_p=args.top_p,\n                        temperature=args.temperature,\n                        top_k=args.top_k,\n                        invalid_slices=text_processor_infer.invalid_slices,\n                        args=args\n                        )\n                except Exception as e:\n                    print(e)\n                    break\n                if rank == 0 and not args.stream_chat:\n                    if args.chinese:\n                        print(\"Ê®°ÂûãÔºö\"+response)\n                    else:\n                        print(\"Model: \"+response)\n                image_path = None\n                if args.chinese:\n                    if rank == 0:",
        "type": "code",
        "location": "/basic_demo/cli_demo_sat.py:121-145"
    },
    "81": {
        "file_id": 2,
        "content": "Code creates a TextCompletion instance with given arguments. It handles exceptions and prints them if any occur. If `args.stream_chat` is False, it prints the model's response in the specified language (Chinese or English). Sets `image_path` to None.",
        "type": "comment"
    },
    "82": {
        "file_id": 2,
        "content": "                        query = [input(\"Áî®Êà∑Ôºö\")]\n                    else:\n                        query = [None]\n                else:\n                    if rank == 0:\n                        query = [input(\"User: \")]\n                    else:\n                        query = [None]\n                if world_size > 1:\n                    torch.distributed.broadcast_object_list(query, 0)\n                query = query[0]\n                assert query is not None\nif __name__ == \"__main__\":\n    main()",
        "type": "code",
        "location": "/basic_demo/cli_demo_sat.py:146-161"
    },
    "83": {
        "file_id": 2,
        "content": "This code takes user input and assigns it to the 'query' variable. If the program is running on multiple GPUs, it broadcasts the query across all GPUs.",
        "type": "comment"
    },
    "84": {
        "file_id": 3,
        "content": "/basic_demo/web_demo.py",
        "type": "filepath"
    },
    "85": {
        "file_id": 3,
        "content": "The code creates a web demo with Gradio, using CogVLM and CogAgent models for image uploads, text prompts, agent functions, image processing, chatbot interface, response generation, UI, command line arguments, input elements, function configurations, argument parsing, and model parameters.",
        "type": "summary"
    },
    "86": {
        "file_id": 3,
        "content": "\"\"\"\nThis script is a simple web demo of the CogVLM and CogAgent models, designed for easy and quick demonstrations.\nFor a more sophisticated user interface, users are encouraged to refer to the 'composite_demo',\nwhich is built with a more aesthetically pleasing Streamlit framework.\nUsage:\n- Use the interface to upload images and enter text prompts to interact with the models.\nRequirements:\n- Gradio (only 3.x,4.x is not support) and other necessary Python dependencies must be installed.\n- Proper model checkpoints should be accessible as specified in the script.\nNote: This demo is ideal for a quick showcase of the CogVLM and CogAgent models. For a more comprehensive and interactive\nexperience, refer to the 'composite_demo'.\n\"\"\"\nimport gradio as gr\nimport os, sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom PIL import Image\nimport torch\nimport time\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.mpu import get_model_parallel_world_size\nfrom sat.model import AutoModel",
        "type": "code",
        "location": "/basic_demo/web_demo.py:1-25"
    },
    "87": {
        "file_id": 3,
        "content": "This code is a simple web demo of the CogVLM and CogAgent models. It requires Gradio (version 3.x or 4.x) and other necessary Python dependencies. The script includes functionality to upload images and enter text prompts for interaction with the models. It's suitable for quick showcases, but a more comprehensive experience can be found in 'composite_demo'.",
        "type": "comment"
    },
    "88": {
        "file_id": 3,
        "content": "from utils.utils import chat, llama2_tokenizer, llama2_text_processor_inference, get_image_processor, parse_response\nfrom utils.models import CogAgentModel, CogVLMModel\nDESCRIPTION = '''<h1 style='text-align: center'> <a href=\"https://github.com/THUDM/CogVLM\">CogVLM / CogAgent</a> </h1>'''\nNOTES = '<h3> This app is adapted from <a href=\"https://github.com/THUDM/CogVLM\">https://github.com/THUDM/CogVLM</a>. It would be recommended to check out the repo if you want to see the detail of our model, CogVLM & CogAgent. </h3>'\nMAINTENANCE_NOTICE1 = 'Hint 1: If the app report \"Something went wrong, connection error out\", please turn off your proxy and retry.<br>Hint 2: If you upload a large size of image like 10MB, it may take some time to upload and process. Please be patient and wait.'\nAGENT_NOTICE = 'Hint 1: To use <strong>Agent</strong> function, please use the <a href=\"https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761\">prompts for agents</a>.'\nGROUNDING_NOTICE = 'Hint 2: To ",
        "type": "code",
        "location": "/basic_demo/web_demo.py:28-42"
    },
    "89": {
        "file_id": 3,
        "content": "These lines import necessary modules, define variables for the app's description and maintenance notice, and specify hints for using the agent function and grounding features.",
        "type": "comment"
    },
    "90": {
        "file_id": 3,
        "content": "use <strong>Grounding</strong> function, please use the <a href=\"https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L344\">prompts for grounding</a>.'\ndefault_chatbox = [(\"\", \"Hi, What do you want to know about this image?\")]\nmodel = image_processor = text_processor_infer = None\nis_grounding = False\ndef process_image_without_resize(image_prompt):\n    image = Image.open(image_prompt)\n    # print(f\"height:{image.height}, width:{image.width}\")\n    timestamp = int(time.time())\n    file_ext = os.path.splitext(image_prompt)[1]\n    filename_grounding = f\"examples/{timestamp}_grounding{file_ext}\"\n    return image, filename_grounding\nfrom sat.quantization.kernels import quantize\ndef load_model(args): \n    model, model_args = AutoModel.from_pretrained(\n        args.from_pretrained,\n        args=argparse.Namespace(\n        deepspeed=None,\n        local_rank=0,\n        rank=0,\n        world_size=world_size,\n        model_parallel_size=world_size,\n        mode='inference',\n        fp16=args.fp16,\n        bf16=args.bf16,",
        "type": "code",
        "location": "/basic_demo/web_demo.py:42-75"
    },
    "91": {
        "file_id": 3,
        "content": "Code snippet initializes a function to process an image without resizing, loads the model with given arguments, and defines a helper function for quantization.",
        "type": "comment"
    },
    "92": {
        "file_id": 3,
        "content": "        skip_init=True,\n        use_gpu_initialization=True if (torch.cuda.is_available() and args.quant is None) else False,\n        device='cpu' if args.quant else 'cuda'),\n        overwrite_args={'model_parallel_size': world_size} if world_size != 1 else {}\n    )\n    model = model.eval()\n    assert world_size == get_model_parallel_world_size(), \"world size must equal to model parallel size for cli_demo!\"\n    language_processor_version = model_args.text_processor_version if 'text_processor_version' in model_args else args.version\n    tokenizer = llama2_tokenizer(args.local_tokenizer, signal_type=language_processor_version)\n    image_processor = get_image_processor(model_args.eva_args[\"image_size\"][0])\n    cross_image_processor = get_image_processor(model_args.cross_image_pix) if \"cross_image_pix\" in model_args else None\n    if args.quant:\n        quantize(model, args.quant)\n        if torch.cuda.is_available():\n            model = model.cuda()\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())",
        "type": "code",
        "location": "/basic_demo/web_demo.py:76-93"
    },
    "93": {
        "file_id": 3,
        "content": "Creating model with specified options and device. Ensuring world size matches model parallel size. Setting up tokenizer and image processors. Quantizing model if needed, moving to GPU if available. Adding autoregressive mixin.",
        "type": "comment"
    },
    "94": {
        "file_id": 3,
        "content": "    text_processor_infer = llama2_text_processor_inference(tokenizer, args.max_length, model.image_length)\n    return model, image_processor, cross_image_processor, text_processor_infer\ndef post(\n        input_text,\n        temperature,\n        top_p,\n        top_k,\n        image_prompt,\n        result_previous,\n        hidden_image,\n        state\n        ):\n    result_text = [(ele[0], ele[1]) for ele in result_previous]\n    for i in range(len(result_text)-1, -1, -1):\n        if result_text[i][0] == \"\" or result_text[i][0] == None:\n            del result_text[i]\n    print(f\"history {result_text}\")\n    global model, image_processor, cross_image_processor, text_processor_infer, is_grounding\n    try:\n        with torch.no_grad():\n            pil_img, image_path_grounding = process_image_without_resize(image_prompt)\n            response, _, cache_image = chat(\n                    image_path=\"\", \n                    model=model, \n                    text_processor=text_processor_infer,\n                    img_processor=image_processor,",
        "type": "code",
        "location": "/basic_demo/web_demo.py:95-125"
    },
    "95": {
        "file_id": 3,
        "content": "This code defines a function that takes input text, temperature, top_p, top_k, image prompt, previous result history, hidden image, and state as parameters. It cleans up the previous result history by removing any empty or null elements. Then, it processes an image based on the given image prompt and calls another function 'chat' to generate a response using the provided model, text processor, and image processor.",
        "type": "comment"
    },
    "96": {
        "file_id": 3,
        "content": "                    query=input_text, \n                    history=result_text, \n                    cross_img_processor=cross_image_processor,\n                    image=pil_img, \n                    max_length=2048, \n                    top_p=top_p, \n                    temperature=temperature,\n                    top_k=top_k,\n                    invalid_slices=text_processor_infer.invalid_slices if hasattr(text_processor_infer, \"invalid_slices\") else [],\n                    no_prompt=False,\n                    args=state['args']\n            )\n    except Exception as e:\n        print(\"error message\", e)\n        result_text.append((input_text, 'Timeout! Please wait a few minutes and retry.'))\n        return \"\", result_text, hidden_image\n    answer = response\n    if is_grounding:\n        parse_response(pil_img, answer, image_path_grounding)\n        new_answer = answer.replace(input_text, \"\")\n        result_text.append((input_text, new_answer))\n        result_text.append((None, (image_path_grounding,)))\n    else:",
        "type": "code",
        "location": "/basic_demo/web_demo.py:126-149"
    },
    "97": {
        "file_id": 3,
        "content": "Function call to text generation model with input, history, processor, image, and additional parameters. Catch any exceptions and handle them by appending a timeout message to the result text. If grounding is enabled, parse the response and append it to the result text with the corresponding image path.",
        "type": "comment"
    },
    "98": {
        "file_id": 3,
        "content": "        result_text.append((input_text, answer))\n    print(result_text)\n    print('finished')\n    return \"\", result_text, hidden_image\ndef clear_fn(value):\n    return \"\", default_chatbox, None\ndef clear_fn2(value):\n    return default_chatbox\ndef main(args):\n    global model, image_processor, cross_image_processor, text_processor_infer, is_grounding\n    model, image_processor, cross_image_processor, text_processor_infer = load_model(args)\n    is_grounding = 'grounding' in args.from_pretrained\n    gr.close_all()\n    with gr.Blocks(css='style.css') as demo:\n        state = gr.State({'args': args})\n        gr.Markdown(DESCRIPTION)\n        gr.Markdown(NOTES)\n        with gr.Row():\n            with gr.Column(scale=5):\n                with gr.Group():\n                    gr.Markdown(AGENT_NOTICE)\n                    gr.Markdown(GROUNDING_NOTICE)\n                    input_text = gr.Textbox(label='Input Text', placeholder='Please enter text prompt below and press ENTER.')\n                    with gr.Row():\n                        run_button = gr.Button('Generate')",
        "type": "code",
        "location": "/basic_demo/web_demo.py:150-185"
    },
    "99": {
        "file_id": 3,
        "content": "This code defines a function named `main`, which loads a model and related processors, initializes the UI components, and handles user input to generate responses based on the text prompt. The UI includes textboxes for input, buttons to trigger generation, and displays the generated results. There are also two helper functions defined (`clear_fn` and `clear_fn2`) that appear to clear the chatbox and other UI elements.",
        "type": "comment"
    }
}
{
    "summary": "The code creates a FastAPI app for chat functionality, with endpoints for message interactions, model listings, and advanced model-based conversation generation. It also handles GPU memory management, CORS, and checks device presence before starting the server using Uvicorn.",
    "details": [
        {
            "comment": "Importing various libraries and setting up configurations for the model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":0-31",
            "content": "import os\nimport gc\nimport time\nimport base64\nfrom contextlib import asynccontextmanager\nfrom typing import List, Literal, Union, Tuple, Optional\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom loguru import logger\nfrom pydantic import BaseModel, Field\nfrom sse_starlette.sse import EventSourceResponse\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer, PreTrainedModel, PreTrainedTokenizer, \\\n    TextIteratorStreamer\nfrom PIL import Image\nfrom io import BytesIO\nMODEL_PATH = os.environ.get('MODEL_PATH', 'THUDM/cogvlm-chat-hf')\nTOKENIZER_PATH = os.environ.get(\"TOKENIZER_PATH\", 'lmsys/vicuna-7b-v1.5')\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nif os.environ.get('QUANT_ENABLED'):\n    QUANT_ENABLED = True\nelse:\n    with torch.cuda.device(DEVICE):\n        __, total_bytes = torch.cuda.mem_get_info()\n        total_gb = total_bytes / (1 << 30)\n        if total_gb < 40:\n            QUANT_ENABLED = True\n        else:\n            QUANT_ENABLED = False"
        },
        {
            "comment": "This code defines a class `ModelCard` using Pydantic, which is used to represent metadata about a machine learning model. It includes fields like model ID, owner, and creation time. The `lifespan` function ensures that GPU memory is cleared after the app's lifecycle ends in environments with GPUs available. The FastAPI app also uses a CORSMiddleware for handling Cross-Origin Resource Sharing, allowing requests from any origin (\"*\"), with credentials (\"true\"), for all methods (\"*\") and headers (\"*\").",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":33-67",
            "content": "@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"\n    An asynchronous context manager for managing the lifecycle of the FastAPI app.\n    It ensures that GPU memory is cleared after the app's lifecycle ends, which is essential for efficient resource management in GPU environments.\n    \"\"\"\n    yield\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\napp = FastAPI(lifespan=lifespan)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\nclass ModelCard(BaseModel):\n    \"\"\"\n    A Pydantic model representing a model card, which provides metadata about a machine learning model.\n    It includes fields like model ID, owner, and creation time.\n    \"\"\"\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"owner\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: Optional[list] = None"
        },
        {
            "comment": "This code defines various classes and types for handling chat messages, model lists, image URLs, text content, and more. These are used to interact with OpenAI's API for generating chat responses. The `ChatMessageInput` represents a message input, while the `ChatCompletionRequest` is used to specify parameters for generating chat responses using the specified model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":70-117",
            "content": "class ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\nclass ImageUrl(BaseModel):\n    url: str\nclass TextContent(BaseModel):\n    type: Literal[\"text\"]\n    text: str\nclass ImageUrlContent(BaseModel):\n    type: Literal[\"image_url\"]\n    image_url: ImageUrl\nContentItem = Union[TextContent, ImageUrlContent]\nclass ChatMessageInput(BaseModel):\n    role: Literal[\"user\", \"assistant\", \"system\"]\n    content: Union[str, List[ContentItem]]\n    name: Optional[str] = None\nclass ChatMessageResponse(BaseModel):\n    role: Literal[\"assistant\"]\n    content: str = None\n    name: Optional[str] = None\nclass DeltaMessage(BaseModel):\n    role: Optional[Literal[\"user\", \"assistant\", \"system\"]] = None\n    content: Optional[str] = None\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[ChatMessageInput]\n    temperature: Optional[float] = 0.8\n    top_p: Optional[float] = 0.8\n    max_tokens: Optional[int] = None\n    stream: Optional[bool] = False\n    # Additional parameters\n    repetition_penalty: Optional[float] = 1.0"
        },
        {
            "comment": "Class definitions for ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice, UsageInfo and ChatCompletionResponse.\nAn API endpoint to list available models with model cards information.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":120-151",
            "content": "class ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessageResponse\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\nclass UsageInfo(BaseModel):\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    completion_tokens: Optional[int] = 0\nclass ChatCompletionResponse(BaseModel):\n    model: str\n    object: Literal[\"chat.completion\", \"chat.completion.chunk\"]\n    choices: List[Union[ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice]]\n    created: Optional[int] = Field(default_factory=lambda: int(time.time()))\n    usage: Optional[UsageInfo] = None\n@app.get(\"/v1/models\", response_model=ModelList)\nasync def list_models():\n    \"\"\"\n    An endpoint to list available models. It returns a list of model cards.\n    This is useful for clients to query and understand what models are available for use.\n    \"\"\"\n    model_card = ModelCard(id=\"cogvlm-chat-17b\")  # can be replaced by your model id like cogagent-chat-18b\n    return ModelList(data=[model_card])"
        },
        {
            "comment": "This code defines a POST route for creating chat completions. It checks the validity of the input, then generates a response based on the provided parameters and model. If the stream parameter is set, it returns an EventSourceResponse to stream the generation progress. Otherwise, it calls generate_cogvlm function to generate the response. The usage information is recorded and the generated message is logged for debugging purposes.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":154-184",
            "content": "@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def create_chat_completion(request: ChatCompletionRequest):\n    global model, tokenizer\n    if len(request.messages) < 1 or request.messages[-1].role == \"assistant\":\n        raise HTTPException(status_code=400, detail=\"Invalid request\")\n    gen_params = dict(\n        messages=request.messages,\n        temperature=request.temperature,\n        top_p=request.top_p,\n        max_tokens=request.max_tokens or 1024,\n        echo=False,\n        stream=request.stream,\n    )\n    if request.stream:\n        generate = predict(request.model, gen_params)\n        return EventSourceResponse(generate, media_type=\"text/event-stream\")\n    response = generate_cogvlm(model, tokenizer, gen_params)\n    usage = UsageInfo()\n    message = ChatMessageResponse(\n        role=\"assistant\",\n        content=response[\"text\"],\n    )\n    logger.debug(f\"==== message ====\\n{message}\")\n    choice_data = ChatCompletionResponseChoice(\n        index=0,\n        message=message,"
        },
        {
            "comment": "This code is handling streaming predictions for real-time, continuous interactions with the model. It generates responses based on input stream and continuously yields these responses as JSON objects.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":185-210",
            "content": "    )\n    task_usage = UsageInfo.model_validate(response[\"usage\"])\n    for usage_key, usage_value in task_usage.model_dump().items():\n        setattr(usage, usage_key, getattr(usage, usage_key) + usage_value)\n    return ChatCompletionResponse(model=request.model, choices=[choice_data], object=\"chat.completion\", usage=usage)\nasync def predict(model_id: str, params: dict):\n    \"\"\"\n    Handle streaming predictions. It continuously generates responses for a given input stream.\n    This is particularly useful for real-time, continuous interactions with the model.\n    \"\"\"\n    global model, tokenizer\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0,\n        delta=DeltaMessage(role=\"assistant\"),\n        finish_reason=None\n    )\n    chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n    yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\n    previous_text = \"\"\n    for new_response in generate_stream_cogvlm(model, tokenizer, params):\n        decoded_unicode = new_response[\"text\"]"
        },
        {
            "comment": "This code creates a response using the CogVLM model and processes chat history, handling assistant role and chunking response.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":211-234",
            "content": "        delta_text = decoded_unicode[len(previous_text):]\n        previous_text = decoded_unicode\n        delta = DeltaMessage(\n            content=delta_text,\n            role=\"assistant\",\n        )\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=0,\n            delta=delta,\n        )\n        chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n        yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0,\n        delta=DeltaMessage(),\n    )\n    chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object=\"chat.completion.chunk\")\n    yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\ndef generate_cogvlm(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, params: dict):\n    \"\"\"\n    Generates a response using the CogVLM model. It processes the chat history and image data, if any,\n    and then invokes the model to generate a response."
        },
        {
            "comment": "Processing messages to extract text and images for ChatGPT-like API.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":235-264",
            "content": "    \"\"\"\n    for response in generate_stream_cogvlm(model, tokenizer, params):\n        pass\n    return response\ndef process_history_and_images(messages: List[ChatMessageInput]) -> Tuple[\n    Optional[str], Optional[List[Tuple[str, str]]], Optional[List[Image.Image]]]:\n    \"\"\"\n    Process history messages to extract text, identify the last user query,\n    and convert base64 encoded image URLs to PIL images.\n    Args:\n        messages(List[ChatMessageInput]): List of ChatMessageInput objects.\n    return: A tuple of three elements:\n             - The last user query as a string.\n             - Text history formatted as a list of tuples for the model.\n             - List of PIL Image objects extracted from the messages.\n    \"\"\"\n    formatted_history = []\n    image_list = []\n    last_user_query = ''\n    for i, message in enumerate(messages):\n        role = message.role\n        content = message.content\n        if isinstance(content, list):  # text\n            text_content = ' '.join(item.text for item in content if isinstance(item, TextContent))"
        },
        {
            "comment": "This code handles various types of content and extracts necessary information based on the role (user or assistant) and the position within a message history. For user messages, it checks if it's the last message in the history and assigns the text to 'last_user_query'. For assistant responses, it ensures that the previous question has been answered before generating a new response.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":265-286",
            "content": "        else:\n            text_content = content\n        if isinstance(content, list):  # image\n            for item in content:\n                if isinstance(item, ImageUrlContent):\n                    image_url = item.image_url.url\n                    if image_url.startswith(\"data:image/jpeg;base64,\"):\n                        base64_encoded_image = image_url.split(\"data:image/jpeg;base64,\")[1]\n                        image_data = base64.b64decode(base64_encoded_image)\n                        image = Image.open(BytesIO(image_data)).convert('RGB')\n                        image_list.append(image)\n        if role == 'user':\n            if i == len(messages) - 1:  # \u6700\u540e\u4e00\u6761\u7528\u6237\u6d88\u606f\n                last_user_query = text_content\n            else:\n                formatted_history.append((text_content, ''))\n        elif role == 'assistant':\n            if formatted_history:\n                if formatted_history[-1][1] != '':\n                    assert False, f\"the last query is answered. answer again. {formatted_history[-1][0]}, {formatted_history[-1][1]}, {text_content}\""
        },
        {
            "comment": "The code is part of a function that generates a stream of responses using the CogVLM model. It takes in messages, temperature, repetition penalty, top p, and maximum number of tokens as input parameters. It processes history and images from the messages and passes them to the model for conversation generation.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":287-311",
            "content": "                formatted_history[-1] = (formatted_history[-1][0], text_content)\n            else:\n                assert False, f\"assistant reply before user\"\n        else:\n            assert False, f\"unrecognized role: {role}\"\n    return last_user_query, formatted_history, image_list\n@torch.inference_mode()\ndef generate_stream_cogvlm(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, params: dict):\n    \"\"\"\n    Generates a stream of responses using the CogVLM model in inference mode.\n    It's optimized to handle continuous input-output interactions with the model in a streaming manner.\n    \"\"\"\n    messages = params[\"messages\"]\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    max_new_tokens = int(params.get(\"max_tokens\", 256))\n    query, history, image_list = process_history_and_images(messages)\n    logger.debug(f\"==== request ====\\n{query}\")\n    input_by_model = model.build_conversation_input_ids(tokenizer, query=query, history=history,"
        },
        {
            "comment": "This code prepares input data for a language model and sets up the streamer for generating text. The input includes 'input_ids', 'token_type_ids', and 'attention_mask' for the text, and optionally 'cross_images'. It also defines the maximum number of tokens to generate ('max_new_tokens'), whether to sample or generate deterministically based on temperature, and sets up the TextIteratorStreamer for generating text.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":312-334",
            "content": "                                                        images=[image_list[-1]])\n    inputs = {\n        'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n        'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n        'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n        'images': [[input_by_model['images'][0].to(DEVICE).to(torch_type)]],\n    }\n    if 'cross_images' in input_by_model and input_by_model['cross_images']:\n        inputs['cross_images'] = [[input_by_model['cross_images'][0].to(DEVICE).to(torch_type)]]\n    input_echo_len = len(inputs[\"input_ids\"][0])\n    streamer = TextIteratorStreamer(\n        tokenizer=tokenizer,\n        timeout=60.0,\n        skip_prompt=True,\n        skip_special_tokens=True\n)\n    gen_kwargs = {\n        \"repetition_penalty\": repetition_penalty,\n        \"max_new_tokens\": max_new_tokens,\n        \"do_sample\": True if temperature > 1e-5 else False,\n        \"top_p\": top_p if temperature > 1e-5 else 0,\n        'streamer': streamer,"
        },
        {
            "comment": "In this code, we have a function that generates text using an AI model. It checks the temperature value and updates it if necessary. The generated text is accumulated and returned in chunks along with usage information (prompt_tokens, completion_tokens, total_tokens). After generating the text, it cleans up memory by calling gc.collect() and torch.cuda.empty_cache(). If the CUDA capability of the GPU is version 8 or higher, it initializes the tokenizer from a specific pretrained model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":335-372",
            "content": "    }\n    if temperature > 1e-5:\n        gen_kwargs[\"temperature\"] = temperature\n    total_len = 0\n    generated_text = \"\"\n    with torch.no_grad():\n        model.generate(**inputs, **gen_kwargs)\n        for next_text in streamer:\n            generated_text += next_text\n            yield {\n                \"text\": generated_text,\n                \"usage\": {\n                    \"prompt_tokens\": input_echo_len,\n                    \"completion_tokens\": total_len - input_echo_len,\n                    \"total_tokens\": total_len,\n                },\n            }\n    ret = {\n        \"text\": generated_text,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": total_len - input_echo_len,\n            \"total_tokens\": total_len,\n        },\n    }\n    yield ret\ngc.collect()\ntorch.cuda.empty_cache()\nif __name__ == \"__main__\":\n    tokenizer = LlamaTokenizer.from_pretrained(\n        TOKENIZER_PATH,\n        trust_remote_code=True)\n    if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:"
        },
        {
            "comment": "Checking if CUDA device is present, sets torch type to bfloat16 or float16 depending on quantization enabled. Instantiates AutoModelForCausalLM model based on settings and moves it to the specified device (either CPU or CUDA). Finally, starts a server for the app using Uvicorn.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/openai_demo/openai_api.py\":373-399",
            "content": "        torch_type = torch.bfloat16\n    else:\n        torch_type = torch.float16\n    print(\"========Use torch type as:{} with device:{}========\\n\\n\".format(torch_type, DEVICE))\n    if 'cuda' in DEVICE:\n        if QUANT_ENABLED:\n            model = AutoModelForCausalLM.from_pretrained(\n                MODEL_PATH,\n                load_in_4bit=True,\n                trust_remote_code=True,\n                torch_dtype=torch_type,\n                low_cpu_mem_usage=True\n            ).eval()\n        else:\n            model = AutoModelForCausalLM.from_pretrained(\n                MODEL_PATH,\n                load_in_4bit=False,\n                trust_remote_code=True,\n                torch_dtype=torch_type,\n                low_cpu_mem_usage=True\n            ).to(DEVICE).eval()\n    else:\n        model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, trust_remote_code=True).float().to(DEVICE).eval()\n    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)"
        }
    ]
}
{
    "summary": "The code prepares data for fine-tuning, initializes models and mixins, converts data to tensors, trains the model, applies decoding strategies, calculates accuracy metrics, and saves the merged model.",
    "details": [
        {
            "comment": "The code is initializing a function to disable untrainable parameters in a model. It does this by specifying a list of layers that should be enabled for training and then iterating over the named parameters, setting the flag if the layer name matches any of the specified layers. If the parameter has its flag set to True, it will be trainable.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":0-25",
            "content": "import os\nimport torch\nimport argparse\nfrom functools import partial\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom sat import mpu, get_args, get_tokenizer\nfrom sat.training.deepspeed_training import training_main\nfrom sat.helpers import print_rank0\nfrom utils.models import FineTuneTrainCogAgentModel\nfrom utils.utils import llama2_text_processor, llama2_text_processor_inference, get_image_processor\ndef disable_untrainable_params(self):\n    total_trainable = 0\n    # enable = ['vit']\n    enable = [\"encoder\", \"cross_attention\", \"linear_proj\", 'mlp.vision', 'rotary.vision', 'eoi', 'boi', 'vit']\n    if self.args.use_ptuning:\n        enable.extend(['ptuning'])\n    if self.args.use_lora or self.args.use_qlora:\n        enable.extend(['matrix_A', 'matrix_B'])\n    for n, p in self.named_parameters():\n        flag = False\n        for e in enable:\n            if type(e) is tuple:\n                if e[0].lower() in n.lower() and e[1].lower() in n.lower() and 55 > int(n[:n.find('.mlp')].split('.')[-1]) > 45:"
        },
        {
            "comment": "The code is iterating through the parameters and checking if they belong to specific modules, such as 'encoder' or 'vit'. If a parameter belongs to one of these modules, it will set its learning rate scale to 0.1. It also counts the total number of trainable parameters and prints the count at the end. The function `data_collator` is responsible for converting lists or numpy arrays to tensors and concatenating tensors for specific attributes.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":26-54",
            "content": "                    flag = True\n                    break\n            else:\n                if e.lower() in n.lower():\n                    flag = True\n                    break\n        if not flag:\n            p.requires_grad_(False)\n        else:\n            total_trainable += p.numel()\n            if 'encoder' in n or 'vit' in n:\n                p.lr_scale = 0.1\n            print_rank0(n)\n    print_rank0(\"***** Total trainable parameters: \"+str(total_trainable)+\" *****\")\nFineTuneTrainCogAgentModel.disable_untrainable_params = disable_untrainable_params\ndef data_collator(examples, cross_image_processor=None):\n    def to_tensor(value):\n        \"\"\"Converts lists or numpy arrays to tensors.\"\"\"\n        if isinstance(value, list):\n            return torch.tensor(value)\n        elif isinstance(value, np.ndarray):\n            return torch.from_numpy(value)\n        return value\n    def concatenate_tensors(attribute, key):\n        \"\"\"Concatenates tensors for a specific attribute and key.\"\"\"\n        if attribute is None:"
        },
        {
            "comment": "This code converts all lists and numpy arrays in examples to tensors, then extracts and concatenates attributes from examples.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":55-75",
            "content": "            return torch.cat([ex[key] for ex in examples if isinstance(ex[key], torch.Tensor)])\n        else:\n            return torch.cat([ex[attribute][key] for ex in examples if isinstance(ex[attribute][key], torch.Tensor)])\n    # Convert all lists and numpy arrays in examples to tensors\n    for example in examples:\n        for key, value in example.items():\n            example[key] = to_tensor(value)\n    # Extract and concatenate attributes from examples\n    img_args = {}\n    for attribute in ['vision', 'cross']:\n        if attribute == 'cross' and cross_image_processor is None:\n            continue\n        if attribute in examples[-1]:  # Using the last example as reference\n            for key in examples[-1][attribute]:\n                tensor_key = f\"{attribute}_{key}\"\n                tensors_to_concatenate = [ex[attribute][key] for ex in examples if isinstance(ex[attribute][key], torch.Tensor)]\n                if tensors_to_concatenate:\n                    img_args[tensor_key] = concatenate_tensors(attribute, key)"
        },
        {
            "comment": "This code is part of a function that finetunes a COG Agent model. It first checks if a tensor key exists in the last example, and if not, assigns it from there. Then, it removes 'vision' and 'cross' keys from all examples. Next, it creates `model_args` by concatenating tensors and copying other attributes from the last example for all keys in that example. Finally, it merges `img_args` into `model_args`. The code also includes a separate function (`broadcast_auto`) that seems to handle broadcasting data based on its type.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":76-108",
            "content": "                else:\n                    img_args[tensor_key] = examples[-1][attribute][key]\n    # Remove 'vision' and 'cross' keys from examples\n    for example in examples:\n        example.pop('vision', None)\n        example.pop('cross', None)\n    # Create model_args by concatenating tensors and copying other attributes\n    model_args = {key: concatenate_tensors(None, key) \n                  if isinstance(examples[-1][key], torch.Tensor) else examples[-1][key] \n                  for key in examples[-1]\n                  }\n    # Merge img_args into model_args\n    model_args.update(img_args)\n    return model_args\nfrom collections import defaultdict\ndef broadcast_auto(data_dict):\n    type2list = defaultdict(list)\n    other = []\n    for k in data_dict:\n        if type(data_dict[k]) is torch.Tensor:\n            type2list[data_dict[k].dtype].append(k)\n        else:\n            other.append(k)\n    new_data = {}\n    for k in type2list:\n        new_data.update(mpu.broadcast_data(type2list[k], data_dict, k))\n    for k in other:"
        },
        {
            "comment": "109-139: Function to get batch data, handles broadcasting and potential type conversions based on args.\nget_batch: Loads data, broadcasts it if data_iterator is not None, checks data type for possible fp16 or bf16 conversion, returns data batch.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":109-139",
            "content": "        new_data[k] = data_dict[k]\n    return new_data\ndef get_batch(data_iterator, args, timers):\n    # Broadcast data.\n    timers('data loader').start()\n    if data_iterator is not None:\n        data = next(data_iterator)\n    else:\n        data = None\n    timers('data loader').stop()\n    data_b = broadcast_auto(data)\n    for k in data_b:\n        if type(data_b[k]) is torch.Tensor and data_b[k].dtype is not torch.int32 and data_b[k].dtype is not torch.long:\n            if args.fp16:\n                data_b[k] = data_b[k].half()\n            elif args.bf16:\n                data_b[k] = data_b[k].bfloat16()\n    return data_b\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.generation.autoregressive_sampling import filling_sequence\nfrom sat.generation.sampling_strategies import BaseStrategy, BeamSearchStrategy\ndef chat(model, tokenizer, tokens,\n         max_length: int = 1800, num_beams=5, top_p=0.95, top_k=0, temperature=0.8, **kwargs):\n    inputs = tokens.to(model.parameters().__next__().device)[0]"
        },
        {
            "comment": "The code is concatenating input tokens with a padding token (-1) to fill up the maximum sequence length. It then applies a BaseStrategy or BeamSearchStrategy (depending on parameters) for decoding the generated text. The get_func is used to retrieve masks and position IDs for the given text prompt. Finally, forward_step_eval defines a compute_metrics function that decodes the predicted sequences and compares them with the ground truth labels.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":140-164",
            "content": "    seq = torch.cat(\n        [inputs, torch.tensor([-1] * (max_length - len(inputs)), device=inputs.device)], dim=0\n    )\n    strategy = BaseStrategy(temperature=temperature, top_p=0.4, top_k=1, end_tokens=[tokenizer.eos_token_id])\n    # strategy = BeamSearchStrategy(temperature=temperature, top_p=top_p, top_k=top_k, end_tokens=[tokenizer.eos_token_id],\n    #                               num_beams=num_beams, consider_end=True)\n    get_func = llama2_text_processor_inference.get_func(None, None, image_rope_mask=kwargs['image_rope_mask'])\n    output = filling_sequence(\n        model, seq,\n        batch_size=1,\n        strategy=strategy,\n        get_masks_and_position_ids=get_func,\n        **kwargs\n    )[0]  # drop memory\n    return output\ndef forward_step_eval(data_iterator, model, args, timers):\n    def compute_metrics(eval_preds):\n        preds, labels, device = eval_preds\n        preds = preds.unsqueeze(0)\n        if isinstance(preds, tuple):\n            preds = preds[0]\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)"
        },
        {
            "comment": "This code calculates accuracy metrics for a model's predictions and labels. It replaces -100 in the labels with the pad token id, decodes the labels and predictions using the tokenizer, then compares them to calculate the accuracy and accuracy without considering case. Finally, it returns the calculated metrics as a dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":165-194",
            "content": "        if args.ignore_pad_token_for_loss:\n            # Replace -100 in the labels as we can't decode them.\n            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        score_dict = {\n            \"acc\": [],\n            \"acc_w/o_case\": [],\n        }\n        for pred, label in zip(decoded_preds, decoded_labels):\n            if args.rank == 0:\n                print('pred', pred, 'label', label, flush=True)\n            if pred == label:\n                score_dict['acc'].append(1.)\n            else:\n                score_dict['acc'].append(0.)\n            if pred.lower() == label.lower():\n                score_dict['acc_w/o_case'].append(1.)\n            else:\n                score_dict['acc_w/o_case'].append(0.)\n        for k, v in score_dict.items():\n            score_dict[k] = float(np.mean(v))\n        return score_dict\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)"
        },
        {
            "comment": "The code is preparing the data and fine-tuning a model for a specific context length, then generating outputs using an autoregressive mixin. It also computes metrics for evaluation and returns results.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":195-216",
            "content": "    timers('batch generator').stop()\n    context_len = int(data_b['context_length'][0])\n    tokens = data_b['input_ids'][:, :context_len]\n    data_b['vision_expert_mask'] = data_b['vision_expert_mask'][:, :context_len]\n    data_b['image_embed_mask'] = data_b['image_embed_mask'][:, :context_len]\n    data_b['image_rope_mask'] = data_b['image_rope_mask'][:, :context_len]\n    data_b.pop('input_ids')\n    data_b.pop('attention_mask')\n    data_b.pop('position_ids')\n    labels = data_b.pop('labels')\n    qid = data_b.pop('question_id')\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n    outputs = chat(model, tokenizer, tokens, **data_b)[0][context_len:]\n    # print(outputs)\n    model.del_mixin('auto-regressive')\n    return torch.tensor(0, device=outputs.device), {k: torch.tensor(v, device=outputs.device) for k, v in\n                                                    compute_metrics(\n                                                        (outputs.cpu(), labels.cpu(), outputs.device)).items()}"
        },
        {
            "comment": "This code defines two functions. The first function, `forward_step`, performs a forward step in a neural network model. It takes input data from a data iterator, processes it, and calculates the loss between predicted logits and actual labels. The second function, `create_dataset_function`, creates a dataset by combining image and text processors with specified arguments and paths.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":219-243",
            "content": "from torch.nn import CrossEntropyLoss\ndef forward_step(data_iterator, model, args, timers):\n    \"\"\"Forward step.\"\"\"\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)\n    labels = data_b.pop('labels')\n    timers('batch generator').stop()\n    logits = model(**data_b)[0]\n    lm_logits = logits.to(torch.float32)\n    # Shift so that tokens < n predict n\n    shift_labels = labels[..., 1:].contiguous()\n    shift_logits = lm_logits[..., -1-shift_labels.size(-1):-1, :].contiguous()\n    # Flatten the tokens\n    loss_fct = CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    loss = loss.to(torch.float32)\n    return loss, {'loss': loss}\nfrom utils.utils import ItemDataset\ndef create_dataset_function(image_processor, text_processor, cross_image_processor, path, args):\n    dataset = ItemDataset(image_processor, text_processor, args, path, cross_image_processor=cross_image_processor)"
        },
        {
            "comment": "This code defines a function that returns a dataset and uses the argparse module to parse command-line arguments for fine-tuning a CogAgent model. It also imports classes LoraMixin and PTuningV2Mixin from sat.model.finetune, and sets default values for various model-specific arguments. If args.use_qlora is True, the device will be set to 'cpu'.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":244-262",
            "content": "    return dataset\nfrom sat.model.finetune.lora2 import LoraMixin\nfrom sat.model.finetune.prompt_tuning import PTuningV2Mixin\nif __name__ == '__main__':\n    py_parser = argparse.ArgumentParser(add_help=False)\n    py_parser.add_argument('--max_length', type=int)\n    py_parser.add_argument('--ignore_pad_token_for_loss', action='store_false')\n    py_parser.add_argument(\"--version\", type=str, default=\"chat\", choices=[\"chat\", \"vqa\"], help='version to interact with')\n    py_parser.add_argument(\"--from_pretrained\", type=str, default=\"cogagent-chat\", help='pretrained ckpt')\n    py_parser.add_argument(\"--local_tokenizer\", type=str, default=\"lmsys/vicuna-7b-v1.5\", help='tokenizer path')\n    py_parser.add_argument(\"--vit_checkpoint_activations\", action='store_true')\n    py_parser = FineTuneTrainCogAgentModel.add_model_specific_args(py_parser)\n    known, args_list = py_parser.parse_known_args()\n    args = get_args(args_list)\n    args = argparse.Namespace(**vars(args), **vars(known))\n    if args.use_qlora:\n        args.device = 'cpu'"
        },
        {
            "comment": "This code initializes a fine-tuning model for the CogAgent using the specified pretrained model and arguments. It adds mixins such as PTuning, Lora, or QLora based on the provided options. The model is then sent to GPU if available. Finally, it creates a tokenizer instance from the specified local tokenizer and signal type.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":264-277",
            "content": "    model, args = FineTuneTrainCogAgentModel.from_pretrained(args.from_pretrained, args, overwrite_args={'model_parallel_size': args.model_parallel_size} if args.model_parallel_size != 1 else {})\n    if args.use_ptuning: # TODO: wait for SAT updating\n        model.add_mixin(\"ptuning\", PTuningV2Mixin(args.num_layers, args.hidden_size // args.num_attention_heads, args.num_attention_heads, args.pre_seq_len))\n    if args.use_lora:\n        model.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range), reinit=True)\n        model.get_mixin(\"eva\").vit_model.add_mixin(\"lora\", LoraMixin(args.eva_args['num_layers'], args.lora_rank, layer_range=args.layer_range), reinit=True)\n    elif args.use_qlora:\n        model.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range, qlora=True), reinit=True)\n    if args.use_qlora and torch.cuda.is_available():\n        model = model.to('cuda')\n    from utils.utils import llama2_tokenizer\n    tokenizer = llama2_tokenizer(args.local_tokenizer, signal_type=args.version)"
        },
        {
            "comment": "Initializing image, cross-image, and text processors; creating model using training main function with provided arguments; merging Lora if enabled; saving merged model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_demo.py\":278-289",
            "content": "    image_processor = get_image_processor(args.eva_args[\"image_size\"][0])\n    cross_image_processor = get_image_processor(args.cross_image_pix)\n    text_processor = llama2_text_processor(tokenizer, args.max_length, args.image_length)\n    model = training_main(args, model_cls=model, forward_step_function=forward_step, create_dataset_function=partial(create_dataset_function, image_processor, text_processor, cross_image_processor), collate_fn=partial(data_collator, cross_image_processor=cross_image_processor), forward_step_eval=forward_step_eval)\n    if args.use_lora:\n        model.get_mixin(\"lora\").merge_lora()\n        model.get_mixin(\"eva\").vit_model.get_mixin(\"lora\").merge_lora()\n        args.use_lora = False\n        args.save = \"checkpoints/merged_lora_cogagent\"\n        from sat.training.model_io import save_checkpoint\n        save_checkpoint(1, model, None, None, args)"
        }
    ]
}
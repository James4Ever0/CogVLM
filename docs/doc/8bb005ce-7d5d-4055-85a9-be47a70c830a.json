{
    "summary": "The transformer model's attention layer features classes for identity mapping, self-attention, memory-efficient calculations, and includes forward propagation with drop path, attention mechanism, residual connection, and MLP for hidden state enhancement. The EVA2CLIPModel class inherits from BaseModel, initializes properties and mixins, and uses add_model_specific_args to configure argument groups for its parameters.",
    "details": [
        {
            "comment": "This code defines two classes, `IdentityMixin` and `XAttn`, which inherit from the base mixin class `BaseMixin`. The `IdentityMixin` class performs identity mapping by returning all elements except the first one from its input. The `XAttn` class implements a self-attention function with optional masking, attention dropout, and logging of attention weights. It also applies a scaling factor to the attention scores for better numerical stability.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_model.py\":0-25",
            "content": "import torch\nfrom sat.model.base_model import BaseModel\nfrom sat.model.mixins import BaseMixin\nfrom sat.model.official.vit_model import ViTProperty, ImagePatchEmbeddingMixin, InterpolatedPositionEmbeddingMixin, gelu\nfrom sat import mpu\nclass IdentityMixin(BaseMixin):\n    def __init__(self):\n        super().__init__()\n    def final_forward(self, logits, **kwargs):\n        return logits[:, 1:]\nimport xformers.ops as xops\nclass XAttn(BaseMixin):\n    def __init__(self, head_dim):\n        super().__init__()\n        self.scale = head_dim ** -0.5\n    def attention_fn(self, query_layer, key_layer, value_layer, attention_mask,\n                       attention_dropout=None, log_attention_weights=None, scaling_attention_score=True, **kwargs):\n        dropout_p = 0. # xformers does not support dropout for eva hidden size\n        query_layer = query_layer.permute(0, 2, 1, 3)   # B, num_heads, N, C -> B, N, num_heads, C\n        key_layer = key_layer.permute(0, 2, 1, 3)\n        value_layer = value_layer.permute(0, 2, 1, 3)"
        },
        {
            "comment": "This code defines two functions. The first function, `memory_efficient_attention`, performs memory-efficient attention calculation by using a combination of masks and dropout. The second function, `attention_forward`, is responsible for forward propagation in the attention layer of a transformer model. It applies the defined attention mechanism on input hidden states based on the provided layer ID and mask.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_model.py\":27-51",
            "content": "        out = xops.memory_efficient_attention(\n            query_layer, key_layer, value_layer,\n            p=dropout_p,\n            scale=self.scale,\n            )\n        return out\n    def attention_forward(self, hidden_states, mask, **kw_args):\n        self = self.transformer.layers[kw_args['layer_id']].attention\n        attention_fn = self.hooks['attention_fn']\n        mixed_raw_layer = self.query_key_value(hidden_states)\n        B, N, C = hidden_states.shape\n        mixed_raw_layer = mixed_raw_layer.reshape(B, N, 3, self.num_attention_heads_per_partition, -1).permute(2, 0, 3, 1, 4)   # 3, B, num_heads, N, C\n        query_layer, key_layer, value_layer = mixed_raw_layer[0], mixed_raw_layer[1], mixed_raw_layer[2]\n        dropout_fn = self.attention_dropout if self.training else None\n        context_layer = attention_fn(query_layer, key_layer, value_layer, mask, dropout_fn, **kw_args)\n        context_layer = context_layer.view(B, N, -1)\n        output = self.dense(context_layer)\n        if self.training:"
        },
        {
            "comment": "Layer forward function for a transformer layer, applies self-attention, and optional drop path during training.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_model.py\":52-78",
            "content": "            output = self.output_dropout(output)\n        return output\nclass NewLayerForward(BaseMixin):\n    def __init__(self):\n        super().__init__()\n    def layer_forward(self, hidden_states, mask, *args, **kw_args):\n        '''\n            hidden_states: [batch, seq_len, hidden_size]\n            mask: [(1, 1), seq_len, seq_len]\n        '''\n        self = self.transformer.layers[kw_args['layer_id']]\n        attention_input = hidden_states\n        # Self attention.\n        attention_output = self.input_layernorm(self.attention(attention_input, mask, **kw_args))\n        # DropPath for attention\n        if self.training and self.drop_path > 0.:\n            if mpu.get_cuda_rng_tracker is not None:\n                # drop_path must use model parallel rng tracker\n                # the tracker is initialized as seed of `seed + model_parallel_rank`\n                # deepspeed act-ckpt record the model parallel tracker states\n                with mpu.get_cuda_rng_tracker().fork():\n                    # drop_path percentage 0, others 1/(1-p)"
        },
        {
            "comment": "Applies attention mechanism, adds residual connection and MLP to the hidden states.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_model.py\":79-98",
            "content": "                    random_tensor = (1-self.drop_path\n                                    + torch.rand((attention_output.shape[0],), dtype=attention_output.dtype, device=attention_output.device)).floor_() / (1-self.drop_path)\n                    attention_output = random_tensor.view(-1, 1, 1) * attention_output\n        # Residual connection.\n        hidden_states = attention_input + attention_output\n        mlp_input = hidden_states\n        # MLP.\n        mlp_output = self.post_attention_layernorm(self.mlp(mlp_input, **kw_args))\n        # DropPath for mlp\n        if self.training and self.drop_path > 0.:\n            if mpu.get_cuda_rng_tracker is not None:\n                with mpu.get_cuda_rng_tracker().fork():\n                    random_tensor = (1-self.drop_path\n                                    + torch.rand((mlp_output.shape[0],), dtype=mlp_output.dtype, device=mlp_output.device)).floor_() / (1-self.drop_path)\n                    mlp_output = random_tensor.view(-1, 1, 1) * mlp_output\n        # Second residual connection."
        },
        {
            "comment": "This code defines a class \"EVA2CLIPModel\" that inherits from \"BaseModel\". It initializes properties and mixins, and adds them to the model instance. The function \"add_model_specific_args\" is used to add model-specific arguments to a parser object.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_model.py\":99-118",
            "content": "        output = mlp_input + mlp_output\n        return output\nclass EVA2CLIPModel(BaseModel):\n    def __init__(self, args, transformer=None, parallel_output=True, **kwargs):\n        property = ViTProperty(args.image_size, args.patch_size, args.pre_len, args.post_len)\n        args.max_sequence_length = property.pre_len + property.num_patches + property.post_len\n        if 'activation_func' not in kwargs:\n            kwargs['activation_func'] = gelu\n        super().__init__(args, transformer=transformer, parallel_output=parallel_output, **kwargs)\n        self.transformer.property = property\n        self.add_mixin(\"patch_embedding\", ImagePatchEmbeddingMixin(args.in_channels, args.hidden_size, property))\n        self.add_mixin(\"pos_embedding\", InterpolatedPositionEmbeddingMixin())\n        self.add_mixin(\"final\", IdentityMixin())\n        self.add_mixin(\"newpost\", NewLayerForward())\n        self.add_mixin(\"xattn\", XAttn(args.hidden_size // args.num_attention_heads))\n    @classmethod\n    def add_model_specific_args(cls, parser):"
        },
        {
            "comment": "Configuring argument groups for EVA2CLIP model parameters.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_model.py\":119-125",
            "content": "        group = parser.add_argument_group('EVA2CLIP', 'EVA2CLIP Configurations')\n        group.add_argument('--image-size', nargs='+', type=int, default=[224, 224])\n        group.add_argument('--pre-len', type=int, default=1) # [cls] by default\n        group.add_argument('--post-len', type=int, default=0) # empty by default, but sometimes with special tokens, such as [det] in yolos.\n        group.add_argument('--in-channels', type=int, default=3)\n        group.add_argument('--patch-size', type=int, default=16)\n        return parser"
        }
    ]
}
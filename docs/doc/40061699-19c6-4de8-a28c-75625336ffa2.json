{
    "summary": "The code initializes layers and mixins, creates a tensor from a mask, separates hidden states, applies gate projections, and combines them back together. The module dictionary includes vision query key value, dense layers, and attention forward function with multi-head attention, rotary embeddings, and visual expert masking for model parallelism in transformer layers.",
    "details": [
        {
            "comment": "This code is initializing an instance of the LlamaVisionExpertFCMixin class, which inherits from BaseMixin. It takes in parameters like in_features, hidden_features, num_layers, num_vision_layers, vision_layer_range, params_dtype and device for initialization. It also initializes a ModuleList of ColumnParallelLinear objects for the gate projection layer.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":0-24",
            "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sat.transformer_defaults import attention_fn_default\nfrom sat.model.base_model import BaseMixin, non_conflict\nfrom sat.mpu.layers import ColumnParallelLinear, RowParallelLinear\nfrom sat.mpu.utils import split_tensor_along_last_dim\nfrom sat import mpu\nclass LlamaVisionExpertFCMixin(BaseMixin):\n    def __init__(self, in_features, hidden_features, num_layers=32, num_vision_layers=0, vision_layer_range=None,\n                 params_dtype=torch.float, device=torch.device('cpu')):\n        super().__init__()\n        self.num_layers = num_layers\n        self.num_vision_layers = num_vision_layers\n        if vision_layer_range is None:\n            vision_layer_range = [i for i in range(min(num_vision_layers, num_layers))]\n        self.vision_layer_range = vision_layer_range\n        self.gate_proj = nn.ModuleList([ColumnParallelLinear(\n            in_features,\n            hidden_features,\n            gather_output=False,\n            init_method=None,"
        },
        {
            "comment": "Creating a list of vision expert dense layers and their corresponding projection layers for the VisionTransformer model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":25-57",
            "content": "            bias=False,\n            params_dtype=params_dtype,\n            module=self,\n            name=\"dense_h_to_4h_gate\",\n            skip_init=True,\n            device=device\n        ) for i in range(num_layers)])\n        # Trainable vision expert parameters\n        vision_dense_h_to_4h_list = []\n        vision_dense_4h_to_h_list = []\n        gate_proj_list = []\n        for i in vision_layer_range:\n            vision_dense_h_to_4h = ColumnParallelLinear(\n                in_features,\n                hidden_features,\n                gather_output=False,\n                init_method=None,\n                bias=False,\n                params_dtype=params_dtype,\n                module=self,\n                name=\"vision_dense_h_to_4h\",\n                skip_init=True,\n                device=device\n            )\n            # Project back to h.\n            vision_dense_4h_to_h = RowParallelLinear(\n                hidden_features,\n                in_features,\n                input_is_parallel=True,\n                init_method=None,"
        },
        {
            "comment": "Initializing multiple parallel linear layers for vision feature extraction and storing them in separate lists.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":58-87",
            "content": "                bias=False,\n                params_dtype=params_dtype,\n                module=self,\n                name=\"vision_dense_4h_to_h\",\n                skip_init=True,\n                device=device\n            )\n            gate_proj = ColumnParallelLinear(\n                in_features,\n                hidden_features,\n                gather_output=False,\n                init_method=None,\n                bias=False,\n                params_dtype=params_dtype,\n                module=self,\n                name=\"vision_gate_proj\",\n                skip_init=True,\n                device=device\n            )\n            vision_dense_h_to_4h_list.append(vision_dense_h_to_4h)\n            vision_dense_4h_to_h_list.append(vision_dense_4h_to_h)\n            gate_proj_list.append(gate_proj)\n        self.vision_dense_h_to_4h_list = nn.ModuleDict([\n            (str(layer_id), vision_dense_h_to_4h)\n            for layer_id, vision_dense_h_to_4h in zip(vision_layer_range, vision_dense_h_to_4h_list)\n        ])\n        self.vision_dense_4h_to_h_list = nn.ModuleDict(["
        },
        {
            "comment": "Code block is defining a mixin class for transformer layers with vision expert functionality. The class initializes and stores vision dense layers and gate projection layers for each vision layer in the transformer. The `mlp_forward` method allows selective usage of the vision expert layers based on a given 'layer_id' and a 'vision_expert_mask'.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":88-109",
            "content": "            (str(layer_id), vision_dense_4h_to_h)\n            for layer_id, vision_dense_4h_to_h in zip(vision_layer_range, vision_dense_4h_to_h_list)\n        ])\n        self.vision_gate_proj = nn.ModuleDict([\n            (str(layer_id), gate_proj)\n            for layer_id, gate_proj in zip(vision_layer_range, gate_proj_list)\n        ])\n    def mlp_forward(self, hidden_states, **kw_args):\n        mixin_self = self\n        self = self.transformer.layers[kw_args['layer_id']].mlp\n        if \"vision_expert_mask\" in kw_args:\n            vision_expert_mask = kw_args['vision_expert_mask']\n        else:\n            vision_expert_mask = None\n        layer_id_key = str(int(kw_args['layer_id']))\n        if kw_args['layer_id'] in mixin_self.vision_layer_range and (vision_expert_mask is not None) and vision_expert_mask.any():\n            vision_dense_h_to_4h = mixin_self.vision_dense_h_to_4h_list[layer_id_key]\n            vision_dense_4h_to_h = mixin_self.vision_dense_4h_to_h_list[layer_id_key]\n            vision_gate_proj = mixin_self.vision_gate_proj[layer_id_key]"
        },
        {
            "comment": "This code creates a new tensor of the same shape as 'hidden_states' and assigns values based on a mask. It separates the hidden states into language and vision components, applies gate projections and intermediate calculations, and combines them back together in 'output'.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":110-123",
            "content": "            output = torch.empty(hidden_states.shape, dtype=hidden_states.dtype, device=hidden_states.device)\n            language_hidden_state = hidden_states[~vision_expert_mask.bool()]\n            language_intermediate_parallel = self.activation_func(mixin_self.gate_proj[kw_args['layer_id']](language_hidden_state)) * self.dense_h_to_4h(language_hidden_state)\n            output[~vision_expert_mask.bool()] = self.dense_4h_to_h(language_intermediate_parallel)  # language_output\n            vision_hidden_state = hidden_states[vision_expert_mask.bool()]\n            vision_intermediate_parallel = vision_dense_h_to_4h(vision_hidden_state)\n            gate_output = vision_gate_proj(vision_hidden_state)\n            vision_intermediate_parallel *= self.activation_func(gate_output)\n            output[vision_expert_mask.bool()] = vision_dense_4h_to_h(vision_intermediate_parallel)  # vision_output\n        else:\n            intermediate_parallel = self.activation_func(mixin_self.gate_proj[kw_args['layer_id']](hidden_states)) * self.dense_h_to_4h(hidden_states)"
        },
        {
            "comment": "Code: This code defines a class named \"LlamaVisionExpertAttnMixin\". It includes parameters such as hidden_size, num_heads, num_layers, num_vision_layers, use_vision_expert, vision_layer_range, params_dtype, and device. The class inherits from BaseMixin.\n\nStorage location: \"CogVLM/utils/models/mixin.py\":124-144",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":124-144",
            "content": "            output = self.dense_4h_to_h(intermediate_parallel)\n        return output.contiguous()\n    def copy_param(self):\n        with torch.no_grad():\n            for i in self.vision_layer_range:\n                self.vision_gate_proj[str(i)].weight.data.copy_(self.gate_proj[i].weight.data)\n                self.vision_dense_4h_to_h_list[str(i)].weight.data.copy_(self.transformer.layers[i].mlp.dense_4h_to_h.weight.data)\n                self.vision_dense_h_to_4h_list[str(i)].weight.data.copy_(self.transformer.layers[i].mlp.dense_h_to_4h.weight.data)\nfrom sat.mpu import get_model_parallel_world_size\nfrom sat.mpu.utils import divide\nfrom sat.model.position_embedding.triton_rotary_embeddings import FastRotaryEmbedding\nclass LlamaVisionExpertAttnMixin(BaseMixin):\n    def __init__(self, hidden_size, num_heads, num_layers=28, num_vision_layers=0, use_vision_expert=True, vision_layer_range=None,\n                 params_dtype=torch.float, device=torch.device('cpu')):\n        super().__init__()\n        world_size = get_model_parallel_world_size()"
        },
        {
            "comment": "Initializing model parameters based on input arguments.\n\nThis code initializes various attributes of the model based on input arguments, such as hidden size, number of attention heads, world size, etc. It also creates a FastRotaryEmbedding object and handles the initialization of vision expert parameters if necessary.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":145-168",
            "content": "        self.hidden_size = hidden_size\n        self.num_attention_heads = num_heads\n        self.hidden_size_per_attention_head = divide(hidden_size, num_heads)\n        self.num_attention_heads_per_partition = divide(num_heads, world_size)\n        self.inner_hidden_size = num_heads * self.hidden_size_per_attention_head\n        self.rotary_emb = FastRotaryEmbedding(\n             hidden_size // num_heads, pos_idx_in_fp32=False\n         )\n        self.num_vision_layers = num_vision_layers\n        self.num_layers = num_layers\n        if vision_layer_range is None:\n            vision_layer_range = [i for i in range(min(num_vision_layers, num_layers))]\n        self.vision_layer_range = vision_layer_range\n        self.use_vision_expert = use_vision_expert\n        # Trainable vision expert parameters\n        if self.use_vision_expert:\n            vision_query_key_value_list = []\n            vision_dense_list = []\n            for i in vision_layer_range:\n                vision_query_key_value = ColumnParallelLinear("
        },
        {
            "comment": "Creates two Linear layers for query, key, and value projections, and appends them to lists.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":169-197",
            "content": "                    hidden_size,\n                    3 * hidden_size,\n                    stride=3,\n                    gather_output=False,\n                    init_method=None,\n                    bias=False,\n                    params_dtype=params_dtype,\n                    module=self,\n                    name=\"vision_query_key_value\",\n                    skip_init=True,\n                    device=device\n                )\n                vision_dense = RowParallelLinear(\n                    self.inner_hidden_size,\n                    hidden_size,\n                    input_is_parallel=True,\n                    init_method=None,\n                    bias=False,\n                    params_dtype=params_dtype,\n                    module=self,\n                    name=\"vision_dense\",\n                    skip_init=True,\n                    device=device,\n                    final_bias=False\n                )\n                vision_query_key_value_list.append(vision_query_key_value)\n                vision_dense_list.append(vision_dense)"
        },
        {
            "comment": "Creates a module dictionary for vision query key value and dense layers.\nDefines the attention forward function, allowing for custom attention layers and handling of vision expert masks if provided.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":199-220",
            "content": "            self.vision_query_key_value_list = nn.ModuleDict([\n                (str(layer_id), vision_query_key_value)\n                for layer_id, vision_query_key_value in zip(vision_layer_range, vision_query_key_value_list)\n            ])\n            self.vision_dense_list = nn.ModuleDict([\n                (str(layer_id), vision_dense)\n                for layer_id, vision_dense in zip(vision_layer_range, vision_dense_list)\n            ])\n    def attention_forward(self, hidden_states, mask, **kw_args):\n        mixin_self = self\n        self = self.transformer.layers[kw_args['layer_id']].attention\n        attention_fn = attention_fn_default\n        if 'attention_fn' in self.hooks:\n            attention_fn = self.hooks['attention_fn']\n        if \"vision_expert_mask\" in kw_args:\n            vision_expert_mask = kw_args['vision_expert_mask']\n        else:\n            vision_expert_mask = None\n        layer_id_key = str(int(kw_args['layer_id']))\n        if mixin_self.use_vision_expert and kw_args['layer_id'] in mixin_self.vision_layer_range and ("
        },
        {
            "comment": "If vision_expert_mask is not None, the code creates a new tensor mixed_raw_layer with the same shape as hidden_states. It separates language and vision hidden_states based on the vision_expert_mask, and assigns each to the corresponding region in mixed_raw_layer using the query_key_value function from the mixin module. If vision_expert_mask is None, it simply calls self.query_key_value with the original hidden_states.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":221-237",
            "content": "                vision_expert_mask is not None) and vision_expert_mask.any():\n            shape = list(hidden_states.shape)\n            parallel_size = mpu.get_model_parallel_world_size()\n            shape[-1] = shape[-1] * 3 // parallel_size\n            vision_query_key_value = mixin_self.vision_query_key_value_list[layer_id_key]\n            mixed_raw_layer = torch.empty(shape, dtype=hidden_states.dtype, device=hidden_states.device)\n            language_hidden_states = hidden_states[~vision_expert_mask.bool()]\n            vision_hidden_states = hidden_states[vision_expert_mask.bool()]\n            mixed_raw_layer[~vision_expert_mask.bool()] = self.query_key_value(\n                language_hidden_states)  # language_mixed_raw_layer\n            mixed_raw_layer[vision_expert_mask.bool()] = vision_query_key_value(\n                vision_hidden_states)  # vision_mixed_raw_layer\n        else:\n            mixed_raw_layer = self.query_key_value(hidden_states)\n        (mixed_query_layer,\n            mixed_key_layer,"
        },
        {
            "comment": "This code is performing multi-head attention with rotary embeddings and optional visual expert masking. It splits the mixed raw layer, applies dropout if training, transposes query, key, and value layers, applies rotary embedding to query and key layers, performs attention operation, and rearranges the resulting context layer.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":238-255",
            "content": "            mixed_value_layer) = split_tensor_along_last_dim(mixed_raw_layer, 3)\n        dropout_fn = self.attention_dropout if self.training else None\n        query_layer = self._transpose_for_scores(mixed_query_layer)\n        key_layer = self._transpose_for_scores(mixed_key_layer)\n        value_layer = self._transpose_for_scores(mixed_value_layer)\n        query_layer, key_layer = mixin_self.rotary_emb(query_layer,key_layer, kw_args['position_ids'], max_seqlen=kw_args['position_ids'].max()+1, layer_id=kw_args['layer_id'])\n        context_layer = attention_fn(query_layer, key_layer, value_layer, mask, dropout_fn, **kw_args)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        if mixin_self.use_vision_expert and kw_args['layer_id'] in mixin_self.vision_layer_range and (\n                vision_expert_mask is not None) and vision_expert_mask.any():"
        },
        {
            "comment": "This code applies model parallelism to a transformer model. It splits the layers of the transformer and performs parallel computations on different parts of the input, using a mask to separate the data for different experts (vision and language). The `copy_param` method copies the weights from the original transformer's layers to the split layers.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/mixin.py\":256-273",
            "content": "            vision_dense = mixin_self.vision_dense_list[layer_id_key]\n            parallel_size = mpu.get_model_parallel_world_size()\n            target_shape = context_layer.shape[:-1] + (context_layer.shape[-1] * parallel_size,)\n            output = torch.empty(target_shape, dtype=hidden_states.dtype, device=hidden_states.device)\n            output[~vision_expert_mask.bool()] = self.dense(context_layer[~vision_expert_mask.bool()])  # language\n            output[vision_expert_mask.bool()] = vision_dense(context_layer[vision_expert_mask.bool()])  # vision\n        else:\n            output = self.dense(context_layer)\n        if self.training:\n            output = self.output_dropout(output)\n        return output.contiguous()\n    def copy_param(self):\n        with torch.no_grad():\n            for i in self.vision_layer_range:\n                self.vision_query_key_value_list[str(i)].weight.data.copy_(self.transformer.layers[i].attention.query_key_value.weight.data)\n                self.vision_dense_list[str(i)].weight.data.copy_(self.transformer.layers[i].attention.dense.weight.data)"
        }
    ]
}
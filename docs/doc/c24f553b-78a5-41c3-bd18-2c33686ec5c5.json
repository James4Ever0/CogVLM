{
    "summary": "The code defines a `GLU` class for CogVLM language model, sets up URLs, initializes an `EVA2CLIPModel`, performs word embedding forward pass with vision inputs, and introduces a `FineTuneTrainCogVLMModel` class that fine-tunes the model by adding mixins.",
    "details": [
        {
            "comment": "This code defines a class `GLU` and imports various modules for creating a large language model. It also sets up URLs for different models in the CogVLM library.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogvlm_model.py\":0-24",
            "content": "from sat.model.official.llama_model import LLaMAModel\nimport json\nimport torch\nfrom sat.model.base_model import BaseMixin\nimport torch.nn as nn\nfrom .mixin import LlamaVisionExpertFCMixin, LlamaVisionExpertAttnMixin\nfrom sat.resources.urls import MODEL_URLS\nMODEL_URLS[\"cogvlm-base-224\"] = \"r2://cogvlm-base-224.zip\"\nMODEL_URLS[\"cogvlm-base-490\"] = \"r2://cogvlm-base-490.zip\"\nMODEL_URLS[\"cogvlm-chat-v1.1\"] = \"r2://cogvlm-chat-v1.1.zip\"\nMODEL_URLS[\"cogvlm-grounding-base\"] = \"r2://cogvlm-grounding-base.zip\"\nMODEL_URLS[\"cogvlm-grounding-generalist-v1.1\"] = \"r2://cogvlm-grounding-generalist-v1.1.zip\"\nclass GLU(nn.Module):\n    def __init__(self, args, in_features):\n        super().__init__()\n        self.linear_proj = nn.Linear(in_features, args.hidden_size, bias=False)\n        self.norm1 = nn.LayerNorm(args.hidden_size)\n        self.act1 = nn.GELU()\n        self.act2 = nn.functional.silu\n        self.dense_h_to_4h = nn.Linear(args.hidden_size, args.inner_hidden_size, bias=False)\n        self.gate_proj = nn.Linear(args.hidden_size, args.inner_hidden_size, bias=False)"
        },
        {
            "comment": "This code is defining a class for the CogVLM model and its forward function, as well as a function to override certain arguments when running in inference mode.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogvlm_model.py\":25-55",
            "content": "        self.dense_4h_to_h = nn.Linear(args.inner_hidden_size, args.hidden_size, bias=False)\n    def forward(self, x):\n        x = self.linear_proj(x)\n        x = self.act1(self.norm1(x))\n        x = self.act2(self.gate_proj(x)) * self.dense_h_to_4h(x)\n        x = self.dense_4h_to_h(x)\n        return x\nfrom .eva_clip_model import EVA2CLIPModel\nimport argparse\nfrom copy import deepcopy\ndef override_dist_dtype_device_args(args, b={}):\n    if args.mode == 'inference':\n        minimal_args = argparse.Namespace(\n            world_size=args.world_size,\n            rank=args.rank,\n            local_rank=args.local_rank,\n            skip_init=args.skip_init,\n            use_gpu_initialization=args.use_gpu_initialization,\n            deepspeed=args.deepspeed,\n            bf16=args.bf16,\n            fp16=args.fp16,\n            mode=args.mode,\n            device=args.device\n        )\n    else:\n        minimal_args = argparse.Namespace(\n                world_size=args.world_size,\n                rank=args.rank,\n                local_rank=args.local_rank,"
        },
        {
            "comment": "Functionality:\n- Initializing an instance of `EVA2CLIPModel` with given arguments.\n- Overriding the dist, dtype, and device args from `args` and `eva_args`.\n- Setting the `vit_model` attribute of the `ImageMixin` class to the initialized model.\n- Setting the `in_features` attribute to 1792.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogvlm_model.py\":56-77",
            "content": "                skip_init=args.skip_init,\n                use_gpu_initialization=args.use_gpu_initialization,\n                deepspeed=args.deepspeed,\n                bf16=args.bf16,\n                fp16=args.fp16,\n                mode=args.mode,\n                checkpoint_activations=args.checkpoint_activations if not hasattr(args, 'vit_checkpoint_activations') else args.vit_checkpoint_activations,\n                checkpoint_num_layers=args.checkpoint_num_layers,\n                device=args.device,\n                hidden_dropout=0.,\n                attention_dropout=0.,\n            )\n    if hasattr(args, 'model_parallel_size'):\n        b['model_parallel_size'] = args.model_parallel_size\n    return argparse.Namespace(**deepcopy(b), **vars(minimal_args))\nclass ImageMixin(BaseMixin):\n    def __init__(self, args):\n        super().__init__()\n        vit_args = override_dist_dtype_device_args(args, args.eva_args)\n        self.vit_model = EVA2CLIPModel(EVA2CLIPModel.get_args(**vars(vit_args)))\n        self.in_features = 1792"
        },
        {
            "comment": "This code defines a class method that performs word embedding forward pass. It first checks if input has only one token or no vision inputs, in which case it returns the output from the transformer's word embeddings. If there are vision inputs, it passes them to the Vision Transformer model (vit_model), applies linear projection, and then masks and combines vision and word embeddings.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogvlm_model.py\":78-95",
            "content": "        self.linear_proj = GLU(args, self.in_features)\n        self.image_length = args.image_length\n        self.boi = nn.Parameter(torch.zeros(1, 1, args.hidden_size))\n        self.eoi = nn.Parameter(torch.zeros(1, 1, args.hidden_size))\n    def word_embedding_forward(self, input_ids, output_cross_layer, **kw_args):\n        vision_inputs = {}\n        for k in kw_args:\n            if k.startswith('vision_') and k != 'vision_expert_mask':\n                vision_inputs[k[7:]] = kw_args[k]\n        if input_ids.shape[1] == 1 or not vision_inputs:\n            return self.transformer.word_embeddings(input_ids)\n        image_emb = self.vit_model(**vision_inputs)[0]\n        image_emb = self.linear_proj(image_emb)\n        image_embed_mask = kw_args['image_embed_mask']\n        word_embedding = self.transformer.word_embeddings(input_ids).clone()\n        word_embedding[image_embed_mask.bool()] = torch.cat([self.boi.repeat(len(image_emb), 1, 1), image_emb, self.eoi.repeat(len(image_emb), 1, 1)], dim=1).reshape(-1, image_emb.shape[-1])"
        },
        {
            "comment": "Function: CogVLMModel\n- Initializes the CogVLM model with specified arguments, transformer, and parallel_output.\n- Sets image_length, adds ImageMixin, removes mlp mixin, adds LlamaVisionExpertFCMixin, removes rotary mixin, adds LlamaVisionExpertAttnMixin.\n- Adds model-specific arguments for CogVLM to the parser.\n- Defines forward function for model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogvlm_model.py\":96-116",
            "content": "        return word_embedding.contiguous()\nclass CogVLMModel(LLaMAModel):\n    def __init__(self, args, transformer=None, parallel_output=True, **kwargs):\n        super().__init__(args, transformer=transformer, parallel_output=parallel_output, **kwargs)\n        self.image_length = args.image_length\n        self.add_mixin(\"eva\", ImageMixin(args))\n        self.del_mixin(\"mlp\")\n        self.add_mixin(\"mlp\", LlamaVisionExpertFCMixin(args.hidden_size, args.inner_hidden_size, args.num_layers, 32))\n        self.del_mixin(\"rotary\")\n        self.add_mixin(\"rotary\", LlamaVisionExpertAttnMixin(args.hidden_size, args.num_attention_heads, args.num_layers, 32))\n    @classmethod\n    def add_model_specific_args(cls, parser):\n        group = parser.add_argument_group('CogVLM', 'CogVLM Configurations')\n        group.add_argument('--image_length', type=int, default=256)\n        group.add_argument('--eva_args', type=json.loads, default={})\n        return super().add_model_specific_args(parser)\n    def forward(self, input_ids, vision_expert_mask, image_embed_mask, **kwargs):"
        },
        {
            "comment": "This code defines a class `FineTuneTrainCogVLMModel` that inherits from `CogVLMModel`. The constructor initializes the object and adds some model-specific arguments for fine-tuning. The function `add_model_specific_args` is used to add these specific arguments to the parser.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogvlm_model.py\":117-134",
            "content": "        if input_ids.shape[1] > 1:\n            return super().forward(input_ids=input_ids, vision_expert_mask=vision_expert_mask, image_embed_mask=image_embed_mask, **kwargs)\n        return super().forward(input_ids=input_ids, **kwargs)\nclass FineTuneTrainCogVLMModel(CogVLMModel):\n    def __init__(self, args, transformer=None, parallel_output=True, **kw_args):\n        super().__init__(args, transformer=transformer, parallel_output=parallel_output, **kw_args)\n        self.args = args\n        # If you want to use model parallel with a mp_size=1 checkpoint, and meanwhile you also want to use lora,\n        # you have to add_mixin after loading model checkpoint.\n    @classmethod\n    def add_model_specific_args(cls, parser):\n        group = parser.add_argument_group('CogVLM-finetune', 'CogVLM finetune Configurations')\n        group.add_argument('--pre_seq_len', type=int, default=8)\n        group.add_argument('--lora_rank', type=int, default=10)\n        group.add_argument('--use_ptuning', action=\"store_true\")"
        },
        {
            "comment": "This code is adding model-specific arguments and mixins to the FineTuneTestCogVLMModel class. It adds options for using PTuningV2Mixin, LoraMixin, and EvaMixin based on the provided arguments. The code allows for fine-tuning of the model with these mixins if specified.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogvlm_model.py\":135-150",
            "content": "        group.add_argument('--use_lora', action=\"store_true\")\n        group.add_argument('--use_qlora', action=\"store_true\")\n        group.add_argument('--layer_range', nargs='+', type=int, default=None)\n        return super().add_model_specific_args(parser)\nfrom sat.model.finetune import PTuningV2Mixin\nfrom sat.model.finetune.lora2 import LoraMixin\nclass FineTuneTestCogVLMModel(CogVLMModel):\n    def __init__(self, args, transformer=None, parallel_output=True, **kw_args):\n        super().__init__(args, transformer=transformer, parallel_output=parallel_output, **kw_args)\n        if args.use_ptuning:\n            self.add_mixin(\"ptuning\", PTuningV2Mixin(args.num_layers, args.hidden_size // args.num_attention_heads, args.num_attention_heads, args.pre_seq_len))\n        if args.use_lora:\n            self.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range), reinit=True)\n            self.get_mixin(\"eva\").vit_model.add_mixin(\"lora\", LoraMixin(args.eva_args['num_layers'], args.lora_rank, layer_range=args.layer_range), reinit=True)"
        },
        {
            "comment": "This code is adding arguments for CogVLM finetuning. If --use_qlora argument is provided, a LoraMixin with qlora set to True is added to the model. It also adds default values for pre_seq_len, lora_rank, use_ptuning, use_lora, use_qlora, and layer_range.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogvlm_model.py\":151-164",
            "content": "        elif args.use_qlora:\n            self.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range, qlora=True), reinit=True)\n        self.args = args\n    @classmethod\n    def add_model_specific_args(cls, parser):\n        group = parser.add_argument_group('CogVLM-finetune', 'CogVLM finetune Configurations')\n        group.add_argument('--pre_seq_len', type=int, default=8)\n        group.add_argument('--lora_rank', type=int, default=10)\n        group.add_argument('--use_ptuning', action=\"store_true\")\n        group.add_argument('--use_lora', action=\"store_true\")\n        group.add_argument('--use_qlora', action=\"store_true\")\n        group.add_argument('--layer_range', nargs='+', type=int, default=None)\n        return super().add_model_specific_args(parser)"
        }
    ]
}
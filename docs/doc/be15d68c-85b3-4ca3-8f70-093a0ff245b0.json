{
    "summary": "The code imports modules for training CogVLM models, processes data and tensor elements, generates chat responses using a given model, adjusts parameters, utilizes language model, performs evaluation, manages command line arguments, handles LORA/QLORA application, moves model to GPU if available, initializes processors, and saves merged model.",
    "details": [
        {
            "comment": "This code is importing necessary modules, defining a function to disable untrainable parameters in a model, and setting up enable flags for specific types of parameter tuning. It appears to be part of a larger fine-tuning process for a CogVLM model, potentially using DeepSpeed training.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":0-28",
            "content": "import os\nimport torch\nimport argparse\nfrom functools import partial\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom sat import mpu, get_args, get_tokenizer\nfrom sat.training.deepspeed_training import training_main\nfrom sat.helpers import print_rank0\nfrom utils.models import FineTuneTrainCogVLMModel\nfrom utils.utils import llama2_text_processor, llama2_text_processor_inference, get_image_processor\ndef disable_untrainable_params(self):\n    total_trainable = 0\n    enable = [('mlp', 'vit')]\n    if self.args.use_ptuning:\n        enable.extend(['ptuning'])\n    if self.args.use_lora or self.args.use_qlora:\n        enable.extend(['matrix_A', 'matrix_B'])\n    for n, p in self.named_parameters():\n        flag = False\n        for e in enable:\n            if type(e) is tuple:\n                if e[0].lower() in n.lower() and e[1].lower() in n.lower() and 55 > int(n[:n.find('.mlp')].split('.')[-1]) > 45:\n                    flag = True\n                    break\n            else:\n                if e.lower() in n.lower():"
        },
        {
            "comment": "This code is defining a function `data_collator` that processes examples and creates a dictionary of tensors for training. It checks if the list is empty, converts lists to tensors, converts numpy arrays to tensors, and concatenates images into one tensor.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":29-54",
            "content": "                    flag = True\n                    break\n        if not flag:\n            p.requires_grad_(False)\n        else:\n            total_trainable += p.numel()\n            print_rank0(n)\n    print_rank0(\"***** Total trainable parameters: \"+str(total_trainable)+\" *****\")\nFineTuneTrainCogVLMModel.disable_untrainable_params = disable_untrainable_params\ndef data_collator(examples):\n    examples = [ex for ex in examples if len(ex) > 0] # drop {}\n    for example in examples:\n        for k in example:\n            if isinstance(example[k], list):\n                example[k] = torch.tensor(example[k])\n            elif isinstance(example[k], np.ndarray):\n                example[k] = torch.from_numpy(example[k])\n    img_args = {}\n    tmp_example = examples[0]\n    for k in tmp_example['vision']:\n        if type(tmp_example['vision'][k]) is torch.Tensor:\n            img_args['vision_'+k] = torch.cat([example['vision'][k] for example in examples])\n        else:\n            img_args['vision_'+k] = example['vision'][k]"
        },
        {
            "comment": "This code is processing a dictionary of data, separating tensor and non-tensor elements, then broadcasting the tensor data. It returns a new dictionary with the processed data. The function \"get_batch\" retrieves data from an iterator and applies this processing.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":55-90",
            "content": "    for example in examples:\n        example.pop('vision')\n        if 'cross' in example:\n            example.pop('cross')\n    model_args = {}\n    tmp_example = examples[0]\n    for k in tmp_example:\n        if type(tmp_example[k]) is torch.Tensor:\n            model_args[k] = torch.cat([example[k] for example in examples])\n        else:\n            model_args[k] = tmp_example[k]\n    model_args.update(img_args)\n    return model_args\nfrom collections import defaultdict\ndef broadcast_auto(data_dict):\n    type2list = defaultdict(list)\n    other = []\n    for k in data_dict:\n        if type(data_dict[k]) is torch.Tensor:\n            type2list[data_dict[k].dtype].append(k)\n        else:\n            other.append(k)\n    new_data = {}\n    for k in type2list:\n        new_data.update(mpu.broadcast_data(type2list[k], data_dict, k))\n    for k in other:\n        new_data[k] = data_dict[k]\n    return new_data\ndef get_batch(data_iterator, args, timers):\n    # Broadcast data.\n    timers('data loader').start()\n    if data_iterator is not None:"
        },
        {
            "comment": "Function: chat\nDescription: Generates a response from the given model, tokenizer, tokens.\nInputs: model (transformer model), tokenizer (tokenizing function), tokens (input tokens), max_length (maximum length of output), num_beams (number of beams for beam search), top_p (sampling probability), top_k (top k values for sampling).\nOutputs: Response generated by the model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":91-118",
            "content": "        data = next(data_iterator)\n    else:\n        data = None\n    timers('data loader').stop()\n    data_b = broadcast_auto(data)\n    for k in data_b:\n        if type(data_b[k]) is torch.Tensor and data_b[k].dtype is not torch.int32 and data_b[k].dtype is not torch.long:\n            if args.fp16:\n                data_b[k] = data_b[k].half()\n            elif args.bf16:\n                data_b[k] = data_b[k].bfloat16()\n    return data_b\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.generation.autoregressive_sampling import filling_sequence\nfrom sat.generation.sampling_strategies import BaseStrategy, BeamSearchStrategy\ndef chat(model, tokenizer, tokens,\n         max_length: int = 1800, num_beams=5, top_p=0.95, top_k=0, temperature=0.8, **kwargs):\n    inputs = tokens.to(model.parameters().__next__().device)[0]\n    seq = torch.cat(\n        [inputs, torch.tensor([-1] * (max_length - len(inputs)), device=inputs.device)], dim=0\n    )\n    str"
        },
        {
            "comment": "Strategy is set based on input parameters.\nFunction \"filling_sequence\" called with model, sequence, batch size, strategy, and additional kwargs.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":118-141",
            "content": "ategy = BaseStrategy(temperature=temperature, top_p=0.4, top_k=1, end_tokens=[tokenizer.eos_token_id])\n    # strategy = BeamSearchStrategy(temperature=temperature, top_p=top_p, top_k=top_k, end_tokens=[tokenizer.eos_token_id],\n    #                               num_beams=num_beams, consider_end=True)\n    get_func = llama2_text_processor_inference.get_func(None, None, image_rope_mask=kwargs['image_rope_mask'])\n    output = filling_sequence(\n        model, seq,\n        batch_size=1,\n        strategy=strategy,\n        get_masks_and_position_ids=get_func,\n        **kwargs\n    )[0]  # drop memory\n    return output\ndef forward_step_eval(data_iterator, model, args, timers):\n    def compute_metrics(eval_preds):\n        preds, labels, device = eval_preds\n        preds = preds.unsqueeze(0)\n        if isinstance(preds, tuple):\n            preds = preds[0]\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n        if args.ignore_pad_token_for_loss:\n            # Replace -100 in the labels as we can't decode them."
        },
        {
            "comment": "This code calculates accuracy and accuracy without case sensitivity for the predictions and labels. It first converts the labels into padded token IDs or keeps them as the pad_token if they are -100, then decodes the label and prediction batches. The code then iterates over the pairs of decoded predictions and labels, checking if they match or not for both case-sensitive and case-insensitive scenarios. It stores the counts in a dictionary and calculates the average to get the accuracy scores. After that, it returns the score_dict containing the averaged accuracy and accuracy without case sensitivity. The code also handles a batch generator and gets a batch of data.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":142-172",
            "content": "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        score_dict = {\n            \"acc\": [],\n            \"acc_w/o_case\": [],\n        }\n        for pred, label in zip(decoded_preds, decoded_labels):\n            if args.rank == 0:\n                print('pred', pred, 'label', label, flush=True)\n            if pred == label:\n                score_dict['acc'].append(1.)\n            else:\n                score_dict['acc'].append(0.)\n            if pred.lower() == label.lower():\n                score_dict['acc_w/o_case'].append(1.)\n            else:\n                score_dict['acc_w/o_case'].append(0.)\n        for k, v in score_dict.items():\n            score_dict[k] = float(np.mean(v))\n        return score_dict\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)\n    timers('batch generator').stop()\n    context_len = int(data_b['context_length'][0])"
        },
        {
            "comment": "This code chunk is finetuning a model for a specific task using the CogVLM model. It extracts relevant data from a dataset, applies necessary transformations to adapt it to the model's requirements, and passes it through the model to generate predictions. The function returns a tensor of 0 and a dictionary containing computed metrics for evaluation. The code also includes calls to a `chat` function, CachedAutoregressiveMixin(), and compute_metrics().",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":173-195",
            "content": "    tokens = data_b['input_ids'][:, :context_len]\n    data_b['vision_expert_mask'] = data_b['vision_expert_mask'][:, :context_len]\n    data_b['image_embed_mask'] = data_b['image_embed_mask'][:, :context_len]\n    data_b['image_rope_mask'] = data_b['image_rope_mask'][:, :context_len]\n    data_b.pop('input_ids')\n    data_b.pop('attention_mask')\n    data_b.pop('position_ids')\n    labels = data_b.pop('labels')\n    qid = data_b.pop('question_id')\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n    outputs = chat(model, tokenizer, tokens, **data_b)[0][context_len:]\n    # print(outputs)\n    model.del_mixin('auto-regressive')\n    return torch.tensor(0, device=outputs.device), {k: torch.tensor(v, device=outputs.device) for k, v in\n                                                    compute_metrics(\n                                                        (outputs.cpu(), labels.cpu(), outputs.device)).items()}\nfrom torch.nn import CrossEntropyLoss\ndef forward_step(data_iterator, model, args, timers):"
        },
        {
            "comment": "Code is performing forward pass for a language model fine-tuning demonstration. It gets the batch data, passes it through the model to get logits, shifts logits and labels accordingly, calculates loss using CrossEntropyLoss, returns loss and other information. The function create_dataset_function creates a dataset function by combining image and text processors and arguments. Additionally, there's an if condition for checking if the script is being run as main.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":196-225",
            "content": "    \"\"\"Forward step.\"\"\"\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)\n    labels = data_b.pop('labels')\n    timers('batch generator').stop()\n    logits = model(**data_b)[0]\n    lm_logits = logits.to(torch.float32)\n    # Shift so that tokens < n predict n\n    shift_labels = labels[..., 1:].contiguous()\n    shift_logits = lm_logits[..., -1-shift_labels.size(-1):-1, :].contiguous()\n    # Flatten the tokens\n    loss_fct = CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    loss = loss.to(torch.float32)\n    return loss, {'loss': loss}\nfrom utils.utils import ItemDataset\ndef create_dataset_function(image_processor, text_processor, path, args):\n    dataset = ItemDataset(image_processor, text_processor, args, path)\n    return dataset\nfrom sat.model.finetune.lora2 import LoraMixin\nfrom sat.model.finetune.prompt_tuning import PTuningV2Mixin\nif __name__ == '__main__':\n    py_parser = argparse.ArgumentParser(add_help=False)"
        },
        {
            "comment": "This code is parsing command line arguments for training a CogVLM model. The arguments specify the maximum sequence length, whether to ignore padding tokens in loss calculation, the version of the model to interact with, pre-trained checkpoint, tokenizer path, activations from vision transformer (ViT) checkpoints, and device settings. The code also sets up the model on the appropriate device based on whether the user is using QLORA or P-tuning.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":226-240",
            "content": "    py_parser.add_argument('--max_length', type=int)\n    py_parser.add_argument('--ignore_pad_token_for_loss', action='store_false')\n    py_parser.add_argument(\"--version\", type=str, default=\"chat_old\", help='version to interact with')\n    py_parser.add_argument(\"--from_pretrained\", type=str, default=\"cogvlm-chat\", help='pretrained ckpt')\n    py_parser.add_argument(\"--local_tokenizer\", type=str, default=\"lmsys/vicuna-7b-v1.5\", help='tokenizer path')\n    py_parser.add_argument(\"--vit_checkpoint_activations\", action='store_true')\n    py_parser = FineTuneTrainCogVLMModel.add_model_specific_args(py_parser)\n    known, args_list = py_parser.parse_known_args()\n    args = get_args(args_list)\n    args = argparse.Namespace(**vars(args), **vars(known))\n    if args.use_qlora:\n        args.device = 'cpu'\n    model, args = FineTuneTrainCogVLMModel.from_pretrained(args.from_pretrained, args, overwrite_args={'model_parallel_size': args.model_parallel_size} if args.model_parallel_size != 1 else {})\n    if args.use_ptuning:"
        },
        {
            "comment": "The code adds mixins to the model, applies LORA or QLORA if specified, moves the model to GPU if available, loads a tokenizer, and initializes text and image processors for training.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":241-255",
            "content": "        model.add_mixin(\"ptuning\", PTuningV2Mixin(args.num_layers, args.hidden_size // args.num_attention_heads, args.num_attention_heads, args.pre_seq_len))\n    if args.use_lora:\n        model.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range), reinit=True)\n        model.get_mixin(\"eva\").vit_model.add_mixin(\"lora\", LoraMixin(args.eva_args['num_layers'], args.lora_rank, layer_range=args.layer_range), reinit=True)\n    elif args.use_qlora:\n        model.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range, qlora=True), reinit=True)\n    if args.use_qlora and torch.cuda.is_available():\n        model = model.to('cuda')\n    from utils.utils import llama2_tokenizer\n    tokenizer = llama2_tokenizer(args.local_tokenizer, signal_type=args.version)\n    image_processor = get_image_processor(args.eva_args[\"image_size\"][0])\n    text_processor = llama2_text_processor(tokenizer, args.max_length, args.image_length)\n    model = training_ma"
        },
        {
            "comment": "Applies LORA and saves merged model",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogvlm_demo.py\":255-262",
            "content": "in(args, model_cls=model, forward_step_function=forward_step, create_dataset_function=partial(create_dataset_function, image_processor, text_processor), collate_fn=data_collator, forward_step_eval=forward_step_eval)\n    if args.use_lora:\n        model.get_mixin(\"lora\").merge_lora()\n        model.get_mixin(\"eva\").vit_model.get_mixin(\"lora\").merge_lora()\n        args.use_lora = False\n        args.save = \"checkpoints/merged_lora_cogvlm{}\".format(args.eva_args[\"image_size\"][0])\n        from sat.training.model_io import save_checkpoint\n        save_checkpoint(1, model, None, None, args)"
        }
    ]
}
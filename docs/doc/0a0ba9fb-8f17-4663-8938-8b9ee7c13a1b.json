{
    "summary": "The script fine-tunes the \"cogvlm-base-490\" model using 8 GPUs, a local tokenizer and saves checkpoints every 200 iterations. It utilizes NCCL distributed backend, cosine learning rate decay, warmup rate of 0.02, Deepspeed for evaluation, and strict mode for results.",
    "details": [
        {
            "comment": "This script sets up the environment for fine-tuning a specific model, \"cogvlm-base-490\". It uses 8 GPUs per worker, and sets the maximum length for input to 1288. The model is loaded from a merged Lora checkpoint. It also uses a local tokenizer (lmsys/vicuna-7b-v1.5) and specifies that this is the base version of the model. The script also defines some environment variables and file paths for the training process, as well as the paths to train and test data.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm.sh\":0-35",
            "content": "#! /bin/bash\n# export PATH=/usr/local/cuda/bin:$PATH\n# export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\nNUM_GPUS_PER_WORKER=8\nMP_SIZE=1\nscript_path=$(realpath $0)\nscript_dir=$(dirname $script_path)\nmain_dir=$(dirname $script_dir)\nMODEL_TYPE=\"cogvlm-base-490\"\nVERSION=\"base\"\nMODEL_ARGS=\"--from_pretrained ./checkpoints/merged_lora_490 \\\n    --max_length 1288 \\\n    --lora_rank 10 \\\n    --use_lora \\\n    --local_tokenizer lmsys/vicuna-7b-v1.5 \\\n    --version $VERSION\"\n# Tips: If training models of resolution 244, you can set --max_length smaller \nOPTIONS_SAT=\"SAT_HOME=~/.sat_models\"\nOPTIONS_NCCL=\"NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 LOCAL_WORLD_SIZE=$NUM_GPUS_PER_WORKER\"\nHOST_FILE_PATH=\"hostfile\"\ntrain_data=\"./archive_split/train\"\ntest_data=\"./archive_split/test\"\ngpt_options=\" \\\n       --experiment-name finetune-$MODEL_TYPE \\\n       --model-parallel-size ${MP_SIZE} \\\n       --mode finetune \\\n       --train-iters 0 \\\n       --resume-dataloader \\\n       $MODEL_ARGS \\\n       --train-data ${train_data} \\"
        },
        {
            "comment": "The code is running a finetuning demo for CogVLM. It uses NCCL distributed backend, cosine learning rate decay, warmup rate of 0.02, and saves checkpoints every 200 iterations. It evaluates the model every 200 iterations, saves results in \"checkpoints\", uses strict evaluation, sets an eval batch size of 1, splits data with 1.0, uses a specific Deepspeed configuration file, skips initialization, and sets a seed value of 2023. Finally, it runs the command to evaluate the model using Deepspeed and other options.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm.sh\":36-58",
            "content": "       --test-data ${test_data} \\\n       --distributed-backend nccl \\\n       --lr-decay-style cosine \\\n       --warmup .02 \\\n       --checkpoint-activations \\\n       --save-interval 200 \\\n       --eval-interval 200 \\\n       --save \"./checkpoints\" \\\n       --strict-eval \\\n       --eval-batch-size 1 \\\n       --split 1. \\\n       --deepspeed_config test_config_bf16.json \\\n       --skip-init \\\n       --seed 2023\n\"\nrun_cmd=\"${OPTIONS_NCCL} ${OPTIONS_SAT} deepspeed --master_port 16666 --hostfile ${HOST_FILE_PATH} evaluate_cogvlm_demo.py ${gpt_options}\"\necho ${run_cmd}\neval ${run_cmd}\nset +x"
        }
    ]
}
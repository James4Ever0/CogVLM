{
    "summary": "The code initializes a GLU module for CogAgent, uses Vision Transformer (VIT) model and introduces ExternalVisionModel instance. It also defines FineTuneTrainCogAgentModel class for model fine-tuning with optional PTuningV2Mixin and LoraMixin mixins, and includes specific configuration options using a parser.",
    "details": [
        {
            "comment": "This code defines a GLU (Gated Linear Units) module for the CogAgent model. It initializes layers such as linear projections, layer norm, activation functions, and dense layers for the GLU operation. The model URLs are also defined for CogAgent chat and VQA models.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":0-26",
            "content": "from sat.model.official.llama_model import LLaMAModel\nimport json\nimport torch\nfrom functools import partial\nfrom sat.model.base_model import BaseMixin\nimport torch.nn as nn\nimport numpy as np\nfrom sat.resources.urls import MODEL_URLS\nfrom .eva_clip_L_hf import Eva2LargeEncoder\nfrom .mixin import LlamaVisionExpertFCMixin, LlamaVisionExpertAttnMixin\nMODEL_URLS[\"cogagent-chat\"] = \"r2://cogagent-chat.zip\"\nMODEL_URLS[\"cogagent-vqa\"] = \"r2://cogagent-vqa.zip\"\nclass GLU(nn.Module):\n    def __init__(self, args, in_features):\n        super().__init__()\n        self.linear_proj = nn.Linear(in_features, args.hidden_size, bias=False)\n        self.norm1 = nn.LayerNorm(args.hidden_size)\n        self.act1 = nn.GELU()\n        self.act2 = nn.functional.silu\n        self.dense_h_to_4h = nn.Linear(args.hidden_size, args.inner_hidden_size, bias=False)\n        self.gate_proj = nn.Linear(args.hidden_size, args.inner_hidden_size, bias=False)\n        self.dense_4h_to_h = nn.Linear(args.inner_hidden_size, args.hidden_size, bias=False)"
        },
        {
            "comment": "The code defines a forward function for a model and imports the EVA2CLIPModel class. It also includes a function called override_dist_dtype_device_args that takes in arguments and returns minimal arguments based on the mode (inference or training) of the model. The function is used to simplify the argument parsing process.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":28-58",
            "content": "    def forward(self, x):\n        x = self.linear_proj(x)\n        x = self.act1(self.norm1(x))\n        x = self.act2(self.gate_proj(x)) * self.dense_h_to_4h(x)\n        x = self.dense_4h_to_h(x)\n        return x\nfrom .eva_clip_model import EVA2CLIPModel\nimport argparse\nfrom copy import deepcopy\ndef override_dist_dtype_device_args(args, b={}):\n    if args.mode == 'inference':\n        minimal_args = argparse.Namespace(\n            world_size=args.world_size,\n            rank=args.rank,\n            local_rank=args.local_rank,\n            skip_init=args.skip_init,\n            use_gpu_initialization=args.use_gpu_initialization,\n            deepspeed=args.deepspeed,\n            bf16=args.bf16,\n            fp16=args.fp16,\n            mode=args.mode,\n            device=args.device\n        )\n    else:\n        minimal_args = argparse.Namespace(\n                world_size=args.world_size,\n                rank=args.rank,\n                local_rank=args.local_rank,\n                skip_init=args.skip_init,\n                use_gpu_initialization=args.use_gpu_initialization,"
        },
        {
            "comment": "This code initializes a Vision Transformer (VIT) model and adds a linear projection layer on top. It takes arguments for various options like deepspeed, bf16, fp16, mode, checkpoint activations, checkpoint num layers, device, model parallel size, and more. The initialized VIT model is then used to create an ExternalVisionModel instance.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":59-81",
            "content": "                deepspeed=args.deepspeed,\n                bf16=args.bf16,\n                fp16=args.fp16,\n                mode=args.mode,\n                checkpoint_activations=args.checkpoint_activations if not hasattr(args, 'vit_checkpoint_activations') else args.vit_checkpoint_activations,\n                checkpoint_num_layers=args.checkpoint_num_layers,\n                device=args.device,\n                hidden_dropout=0.,\n                attention_dropout=0.,\n            )\n    if hasattr(args, 'model_parallel_size'):\n        b['model_parallel_size'] = args.model_parallel_size\n    return argparse.Namespace(**deepcopy(b), **vars(minimal_args))\nclass ExternalVisionModel(BaseMixin):\n    '''A combination of vit and a linear projection'''\n    def __init__(self, args, vitclass):\n        '''\n            args: the args to initialize the vit model\n            vitclass: the class of VIT model, must be a subclass of BaseModel\n            project_dim: the dimension of the projection layer\n            default_load: the default load path for the vit model"
        },
        {
            "comment": "This code initializes a model with a VIT (Vision Transformer) class and adds position embedding. The model parallel size is set, and an optional positional embedding layer can be added for the ViT input. It also includes parameters to specify the number of images being used in cross-modal reasoning. The forward function returns the output from the VIT model with position embedding and self.pos_embed added.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":82-107",
            "content": "            model_parallel_size: the model parallel size for the vit model\n        '''\n        super().__init__()\n        self.vit = vitclass()\n        # self.ppx = nn.Embedding(80, 1024)\n        # self.ppy = nn.Embedding(80, 1024)\n        # nn.init.uniform_(self.ppx.weight.data)\n        # nn.init.uniform_(self.ppy.weight.data)\n        # self.pos_embed = nn.Parameter(\n        #     torch.from_numpy(get_2d_sincos_pos_embed(1024, 80)).float()\n        # )\n        cross_image_length = (args.cross_image_pix//14)**2\n        self.pos_embed = nn.Parameter(\n            torch.zeros(cross_image_length, 1024)\n        )\n    def forward(self, *args, **kw_args):\n        enc = self.vit(*args, **kw_args)\n        # i = torch.arange(80, device=enc.device)\n        # j = torch.arange(80, device=enc.device)\n        # posx = self.ppx(i).unsqueeze(0).repeat(80, 1, 1)\n        # posy = self.ppy(j).unsqueeze(1).repeat(1, 80, 1)\n        # pos = (posx + posy).view(-1, 1024).unsqueeze(0)\n        # return enc + pos + self.pos_embed.unsqueeze(0)"
        },
        {
            "comment": "This code defines a class named \"ImageMixin\" which inherits from \"BaseMixin\". This class initializes an instance of the EVA2CLIPModel and sets up several parameters for processing image features. The code also defines methods to handle image embeddings, but the provided snippet doesn't show this part.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":108-134",
            "content": "        return enc + self.pos_embed.unsqueeze(0)\nclass ImageMixin(BaseMixin):\n    def __init__(self, args):\n        super().__init__()\n        vit_args = override_dist_dtype_device_args(args, args.eva_args)\n        self.vit_model = EVA2CLIPModel(EVA2CLIPModel.get_args(**vars(vit_args)))\n        self.in_features = 1792\n        self.linear_proj = GLU(args, self.in_features)\n        self.image_length = args.image_length\n        self.boi = nn.Parameter(torch.zeros(1, 1, args.hidden_size))\n        self.eoi = nn.Parameter(torch.zeros(1, 1, args.hidden_size))\n        # self.ppx = nn.Embedding(16,1792)\n        # self.ppy = nn.Embedding(16,1792)\n        # self.pos_embed = nn.Parameter(\n        #     torch.from_numpy(get_2d_sincos_pos_embed(1792, 16)).float()\n        # )\n        self.pos_embed = nn.Parameter(\n            torch.zeros(self.image_length, 1792)\n        )\n    def word_embedding_forward(self, input_ids, output_cross_layer, **kw_args):\n        vision_inputs = {}\n        for k in kw_args:\n            if k.startswith('vision_') and k != 'vision_expert_mask':"
        },
        {
            "comment": "The code takes a combination of input text and image, performs necessary computations for image embeddings, and combines the image embeddings with the word embeddings to create a final embedding. The code is part of the COG-VLM model, which uses both text and image inputs.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":135-154",
            "content": "                vision_inputs[k[7:]] = kw_args[k]\n        if input_ids.shape[1] == 1 or not vision_inputs:\n            return self.transformer.word_embeddings(input_ids)\n        image_emb = self.vit_model(**vision_inputs)[0]\n        # i = torch.arange(16, device=image_emb.device)\n        # j = torch.arange(16, device=image_emb.device)\n        # posx = self.ppx(i).unsqueeze(0).repeat(16, 1, 1)\n        # posy = self.ppy(j).unsqueeze(1).repeat(1, 16, 1)\n        # pos = (posx + posy).view(256, -1).unsqueeze(0)\n        # image_emb = image_emb + pos + self.pos_embed.unsqueeze(0)\n        image_emb = image_emb + self.pos_embed.unsqueeze(0)\n        image_emb = self.linear_proj(image_emb)\n        image_embed_mask = kw_args['image_embed_mask']\n        word_embedding = self.transformer.word_embeddings(input_ids).clone()\n        word_embedding[image_embed_mask.bool()] = torch.cat([self.boi.repeat(len(image_emb), 1, 1), image_emb, self.eoi.repeat(len(image_emb), 1, 1)], dim=1).reshape(-1, image_emb.shape[-1])\n        return word_embedding.contiguous()"
        },
        {
            "comment": "This code defines a CogAgentModel class that inherits from LLaMAModel and adds specific configurations for the CogAgent model. It initializes the model with image-related arguments, adds ImageMixin and MlpExpertFC/AttnExpertFC Mixins, and includes an ExternalVisionModel as part of the model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":156-174",
            "content": "class CogAgentModel(LLaMAModel):\n    def __init__(self, args, transformer=None, parallel_output=True, **kwargs):\n        super().__init__(args, transformer=transformer, parallel_output=parallel_output, **kwargs)\n        self.image_length = args.image_length\n        self.cross_image_pix = args.cross_image_pix\n        self.add_mixin(\"eva\", ImageMixin(args))\n        self.del_mixin(\"mlp\")\n        self.add_mixin(\"mlp\", LlamaVisionExpertFCMixin(args.hidden_size, args.inner_hidden_size, args.num_layers, 32))\n        self.del_mixin(\"rotary\")\n        self.add_mixin(\"rotary\", LlamaVisionExpertAttnMixin(args.hidden_size, args.num_attention_heads, args.num_layers, 32))\n        cross_model = ExternalVisionModel(args, vitclass=partial(Eva2LargeEncoder, image_size=self.cross_image_pix))\n        # if args.mode != 'inference':\n        # cross_model.vit.model.set_grad_checkpointing(True)\n        self.add_mixin(\"encoder\", cross_model)\n    @classmethod\n    def add_model_specific_args(cls, parser):\n        group = parser.add_argument_group('CogAgent', 'CogAgent Configurations')"
        },
        {
            "comment": "This code adds arguments for model parameters and defines the forward method to handle cross-attention inputs.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":175-194",
            "content": "        group.add_argument('--image_length', type=int, default=256)\n        group.add_argument('--cross_image_pix', type=int, default=1120) # Standard CogAgent use 1120; if you want to adjust this param, finetune the model first.\n        group.add_argument('--eva_args', type=json.loads, default={})\n        return super().add_model_specific_args(parser)\n    def forward(self, input_ids, vision_expert_mask, image_embed_mask, **kwargs):\n        cross_inputs = {}\n        for k in kwargs:\n            if k.startswith('cross_'):\n                cross_inputs[k[6:]] = kwargs[k]\n        if kwargs.get(\"mems_cross\") is not None:\n            kwargs['encoder_outputs'] = kwargs[\"mems_cross\"][0]\n        else:\n            outputs = self.get_mixin('encoder')(**cross_inputs)\n            kwargs['encoder_outputs'] = outputs\n        kwargs['cross_attention_mask'] = cross_inputs['attention_mask'] \n        if input_ids.shape[1] > 1:\n            return super().forward(input_ids=input_ids, vision_expert_mask=vision_expert_mask, image_embed_mask=image_embed_mask, **kwargs)"
        },
        {
            "comment": "The code is defining a class `FineTuneTrainCogAgentModel` which inherits from `CogAgentModel`. It has an initializer that initializes the superclass, sets `args`, and provides optional arguments for model parallelism and LoRA. The class also includes `add_model_specific_args` to add specific configuration options for the model fine-tuning process.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":195-213",
            "content": "        return super().forward(input_ids=input_ids, **kwargs)\nclass FineTuneTrainCogAgentModel(CogAgentModel):\n    def __init__(self, args, transformer=None, parallel_output=True, **kw_args):\n        super().__init__(args, transformer=transformer, parallel_output=parallel_output, **kw_args)\n        self.args = args\n        # If you want to use model parallel with a mp_size=1 checkpoint, and meanwhile you also want to use lora,\n        # you have to add_mixin after loading model checkpoint.\n    @classmethod\n    def add_model_specific_args(cls, parser):\n        group = parser.add_argument_group('CogAgent-finetune', 'CogAgent finetune Configurations')\n        group.add_argument('--pre_seq_len', type=int, default=8)\n        group.add_argument('--lora_rank', type=int, default=10)\n        group.add_argument('--use_ptuning', action=\"store_true\")\n        group.add_argument('--use_lora', action=\"store_true\")\n        group.add_argument('--use_qlora', action=\"store_true\")\n        group.add_argument('--layer_range', nargs='+', type=int, default=None)"
        },
        {
            "comment": "This code is initializing a FineTuneTestCogAgentModel with optional PTuningV2Mixin and LoraMixin mixins. If args.use_ptuning is True, it adds the PTuningV2Mixin. If args.use_lora or args.use_qlora is True, it adds the LoraMixin to the model and its get_mixin('eva') respectively. The reinit parameter ensures the model mixins are properly initialized.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":214-228",
            "content": "        return super().add_model_specific_args(parser)\nfrom sat.model.finetune import PTuningV2Mixin\nfrom sat.model.finetune.lora2 import LoraMixin\nclass FineTuneTestCogAgentModel(CogAgentModel):\n    def __init__(self, args, transformer=None, parallel_output=True, **kw_args):\n        super().__init__(args, transformer=transformer, parallel_output=parallel_output, **kw_args)\n        if args.use_ptuning:\n            self.add_mixin(\"ptuning\", PTuningV2Mixin(args.num_layers, args.hidden_size // args.num_attention_heads, args.num_attention_heads, args.pre_seq_len))\n        if args.use_lora:\n            self.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range), reinit=True)\n            self.get_mixin(\"eva\").vit_model.add_mixin(\"lora\", LoraMixin(args.eva_args['num_layers'], args.lora_rank, layer_range=args.layer_range), reinit=True)\n        elif args.use_qlora:\n            self.add_mixin(\"lora\", LoraMixin(args.num_layers, args.lora_rank, layer_range=args.layer_range, qlora=True), reinit=True)"
        },
        {
            "comment": "This code adds model-specific arguments to a parser for a CogAgent finetune configuration. It includes options for pre_seq_len, lora_rank, use of PTuning, LORA, QLORA, and layer range.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/cogagent_model.py\":229-240",
            "content": "        self.args = args\n    @classmethod\n    def add_model_specific_args(cls, parser):\n        group = parser.add_argument_group('CogAgent-finetune', 'CogAgent finetune Configurations')\n        group.add_argument('--pre_seq_len', type=int, default=8)\n        group.add_argument('--lora_rank', type=int, default=10)\n        group.add_argument('--use_ptuning', action=\"store_true\")\n        group.add_argument('--use_lora', action=\"store_true\")\n        group.add_argument('--use_qlora', action=\"store_true\")\n        group.add_argument('--layer_range', nargs='+', type=int, default=None)\n        return super().add_model_specific_args(parser)"
        }
    ]
}
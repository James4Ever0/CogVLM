{
    "summary": "This code demonstrates a CLI using CogAgent and CogVLM, utilizing the Vicuna-7b-v1.5 tokenizer for chat interaction with users. It supports GPU with bfloat16 for performance, processes one image at a time, uses argparse for command line arguments, and includes a loop for user input.",
    "details": [
        {
            "comment": "This code is for a command-line interface (CLI) demo using CogAgent and CogVLM. It requires the Vicuna-7b-v1.5 tokenizer model and suggests using GPU with bfloat16 support for better performance. Only one picture can be processed at a time in a conversation, and it supports 4-bit quantization. The code uses argparse to handle command-line arguments.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/basic_demo/cli_demo_hf.py\":0-18",
            "content": "\"\"\"\nThis is a demo for using CogAgent and CogVLM in CLI\nMake sure you have installed vicuna-7b-v1.5 tokenizer model (https://huggingface.co/lmsys/vicuna-7b-v1.5), full checkpoint of vicuna-7b-v1.5 LLM is not required.\nIn this demo, We us chat template, you can use others to replace such as 'vqa'.\nStrongly suggest to use GPU with bfloat16 support, otherwise, it will be slow.\nMention that only one picture can be processed at one conversation, which means you can not replace or insert another picture during the conversation.\n\"\"\"\nimport argparse\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModelForCausalLM, LlamaTokenizer\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--quant\", choices=[4], type=int, default=None, help='quantization bits')\nparser.add_argument(\"--from_pretrained\", type=str, default=\"THUDM/cogagent-chat-hf\", help='pretrained ckpt')\nparser.add_argument(\"--local_tokenizer\", type=str, default=\"lmsys/vicuna-7b-v1.5\", help='tokenizer path')\nparser.add_argument(\"--fp16\", action=\"store_true\")"
        },
        {
            "comment": "This code is parsing arguments and setting up a Hugging Face AutoModelForCausalLM. It first checks if the --bf16 argument is set, then sets the torch type accordingly (either bfloat16 or float16). If the quant argument is also set, it loads a 4-bit quantized model. The model is then loaded and moved to the appropriate device (CPU or CUDA, if available). Finally, it sets up a text_only_template for chat interaction with the user.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/basic_demo/cli_demo_hf.py\":19-51",
            "content": "parser.add_argument(\"--bf16\", action=\"store_true\")\nargs = parser.parse_args()\nMODEL_PATH = args.from_pretrained\nTOKENIZER_PATH = args.local_tokenizer\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = LlamaTokenizer.from_pretrained(TOKENIZER_PATH)\nif args.bf16:\n    torch_type = torch.bfloat16\nelse:\n    torch_type = torch.float16\nprint(\"========Use torch type as:{} with device:{}========\\n\\n\".format(torch_type, DEVICE))\nif args.quant:\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_PATH,\n        torch_dtype=torch_type,\n        low_cpu_mem_usage=True,\n        load_in_4bit=True,\n        trust_remote_code=True\n    ).eval()\nelse:\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_PATH,\n        torch_dtype=torch_type,\n        low_cpu_mem_usage=True,\n        load_in_4bit=args.quant is not None,\n        trust_remote_code=True\n    ).to(DEVICE).eval()\ntext_only_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {} ASSISTANT:\""
        },
        {
            "comment": "This code is creating a loop where the user can input either an image path or a query. If an image path is not entered, it will default to plain text conversation. The loop continues until the user enters \"clear\". If no image is provided and it's the first query, it formats the query as per the 'text_only_template'. If there is already a history of queries, it appends all previous queries with their corresponding responses, then adds the current query. It then builds the conversation input for the model based on whether an image is present or not.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/basic_demo/cli_demo_hf.py\":53-82",
            "content": "while True:\n    image_path = input(\"image path >>>>> \")\n    if image_path == '':\n        print('You did not enter image path, the following will be a plain text conversation.')\n        image = None\n        text_only_first_query = True    \n    else:\n        image = Image.open(image_path).convert('RGB')\n    history = []\n    while True:\n        query = input(\"Human:\")\n        if query == \"clear\":\n            break\n        if image is None:\n            if text_only_first_query:\n                query = text_only_template.format(query)\n                text_only_first_query = False\n            else:\n                old_prompt = ''\n                for _, (old_query, response) in enumerate(history):\n                    old_prompt += old_query + \" \" + response + \"\\n\"\n                query = old_prompt + \"USER: {} ASSISTANT:\".format(query)\n        if image is None:\n            input_by_model = model.build_conversation_input_ids(tokenizer, query=query, history=history, template_version='base')\n        else:\n            in"
        },
        {
            "comment": "Building conversation input data and preparing for model generation.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/basic_demo/cli_demo_hf.py\":82-99",
            "content": "put_by_model = model.build_conversation_input_ids(tokenizer, query=query, history=history, images=[image])\n        inputs = {\n            'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n            'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n            'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n            'images': [[input_by_model['images'][0].to(DEVICE).to(torch_type)]] if image is not None else None,\n        }\n        if 'cross_images' in input_by_model and input_by_model['cross_images']:\n            inputs['cross_images'] = [[input_by_model['cross_images'][0].to(DEVICE).to(torch_type)]]\n        # add any transformers params here.\n        gen_kwargs = {\"max_length\": 2048,\n                      \"do_sample\": False} # \"temperature\": 0.9\n        with torch.no_grad():\n            outputs = model.generate(**inputs, **gen_kwargs)\n            outputs = outputs[:, inputs['input_ids'].shape[1]:]\n            response = tokenizer.decode(outputs[0])"
        },
        {
            "comment": "Split response by \" Industrially \" and print the first part, then store in history.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/basic_demo/cli_demo_hf.py\":100-102",
            "content": "            response = response.split(\"</s>\")[0]\n            print(\"\\nCog:\", response)\n        history.append((query, response))"
        }
    ]
}
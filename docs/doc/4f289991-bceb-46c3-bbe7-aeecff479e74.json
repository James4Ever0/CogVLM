{
    "summary": "This script configures DeepSpeed-accelerated training for a CogAgent chat model, setting environment variables, distributed backend, learning rate decay style, and checkpoint options. It trains on specified data locations, enables evaluation, uses a random seed, and executes the DeepSpeed command with given parameters.",
    "details": [
        {
            "comment": "This script is configuring environment variables for finetuning a CogAgent chat model, specifying model arguments, and defining options for SAT and NCCL. It also sets the experiment name, model-parallel size, training data location, and test data location.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent.sh\":0-33",
            "content": "#! /bin/bash\n# export PATH=/usr/local/cuda/bin:$PATH\n# export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\nNUM_GPUS_PER_WORKER=8\nMP_SIZE=1\nscript_path=$(realpath $0)\nscript_dir=$(dirname $script_path)\nmain_dir=$(dirname $script_dir)\nMODEL_TYPE=\"cogagent-chat\"\nVERSION=\"chat\"\n# Tips: max_length should be longer than 256, to accomodate low-resolution image tokens\nMODEL_ARGS=\"--from_pretrained ./checkpoints/ft_cogagent_model \\\n    --max_length 400 \\\n    --local_tokenizer lmsys/vicuna-7b-v1.5 \\\n    --version $VERSION\"\nOPTIONS_SAT=\"SAT_HOME=~/.sat_models\"\nOPTIONS_NCCL=\"NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 LOCAL_WORLD_SIZE=$NUM_GPUS_PER_WORKER\"\nHOST_FILE_PATH=\"hostfile\"\ntrain_data=\"./archive_split/train\"\ntest_data=\"./archive_split/test\"\ngpt_options=\" \\\n       --experiment-name finetune-$MODEL_TYPE \\\n       --model-parallel-size ${MP_SIZE} \\\n       --mode finetune \\\n       --train-iters 0 \\\n       --resume-dataloader \\\n       $MODEL_ARGS \\\n       --train-data ${train_data} \\\n       --test-data ${test_data} \\"
        },
        {
            "comment": "This code is configuring and executing a DeepSpeed-accelerated training job for a GPT-like model. It sets distributed backend as NCCL, learning rate decay style as cosine, warmup rate to 0.2, enables checkpoint activations, saves checkpoints every 200 steps, evaluates after every 200 steps, and uses the \"checkpoints\" directory for saving checkpoints. The code also performs strict evaluation, sets evaluation batch size to 1, specifies a dataset split of 1, uses the deepspeed_config file \"test_config_bf16.json\", skips initialization, and sets a random seed of 2023. It then executes the DeepSpeed command with the specified options.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent.sh\":34-55",
            "content": "       --distributed-backend nccl \\\n       --lr-decay-style cosine \\\n       --warmup .02 \\\n       --checkpoint-activations \\\n       --save-interval 200 \\\n       --eval-interval 200 \\\n       --save \"./checkpoints\" \\\n       --strict-eval \\\n       --eval-batch-size 1 \\\n       --split 1. \\\n       --deepspeed_config test_config_bf16.json \\\n       --skip-init \\\n       --seed 2023\n\"\nrun_cmd=\"${OPTIONS_NCCL} ${OPTIONS_SAT} deepspeed --master_port 16666 --hostfile ${HOST_FILE_PATH} evaluate_cogagent_demo.py ${gpt_options}\"\necho ${run_cmd}\neval ${run_cmd}\nset +x"
        }
    ]
}
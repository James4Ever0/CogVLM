{
    "summary": "The code creates a function to convert lists and numpy arrays into tensors, generates chat conversation outputs using an autoregressive model with optional beam search strategy, and utilizes a transformer model for text generation. It defines a dataset function, sets up argument parsing for interacting with a specific model version and pretrained checkpoint, initializes a fine-tuned CogAgent model, prepares tokenizer and image processors, and calls a training function.",
    "details": [
        {
            "comment": "Creating a data collator function for converting lists and numpy arrays to tensors, concatenating tensors for specific attributes.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent_demo.py\":0-28",
            "content": "import os\nimport torch\nimport argparse\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom sat import mpu, get_args, get_tokenizer\nfrom sat.training.deepspeed_training import training_main\nfrom sat.helpers import print_rank0\nfrom collections import defaultdict\nfrom functools import partial\nfrom utils.models import FineTuneTestCogAgentModel\nfrom utils.utils import llama2_text_processor, llama2_text_processor_inference, get_image_processor\ndef data_collator(examples, cross_image_processor=None):\n    def to_tensor(value):\n        \"\"\"Converts lists or numpy arrays to tensors.\"\"\"\n        if isinstance(value, list):\n            return torch.tensor(value)\n        elif isinstance(value, np.ndarray):\n            return torch.from_numpy(value)\n        return value\n    def concatenate_tensors(attribute, key):\n        \"\"\"Concatenates tensors for a specific attribute and key.\"\"\"\n        if attribute is None:\n            return torch.cat([ex[key] for ex in examples if isinstance(ex[key], torch.Tensor)])"
        },
        {
            "comment": "This code is checking if the attribute 'cross' exists in each example and if the cross_image_processor is not None. If either of these conditions are not met, it skips this attribute. Otherwise, it iterates over each example and checks if the current attribute exists within that example. It then concatenates any tensors found into a single tensor and stores it in img_args dictionary under the key \"{attribute}_{key}\". If no tensors are found for an attribute, it uses the last example as reference to populate the img_args dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent_demo.py\":29-50",
            "content": "        else:\n            return torch.cat([ex[attribute][key] for ex in examples if isinstance(ex[attribute][key], torch.Tensor)])\n    # Convert all lists and numpy arrays in examples to tensors\n    for example in examples:\n        for key, value in example.items():\n            example[key] = to_tensor(value)\n    # Extract and concatenate attributes from examples\n    img_args = {}\n    for attribute in ['vision', 'cross']:\n        if attribute == 'cross' and cross_image_processor is None:\n            continue\n        if attribute in examples[-1]:  # Using the last example as reference\n            for key in examples[-1][attribute]:\n                tensor_key = f\"{attribute}_{key}\"\n                tensors_to_concatenate = [ex[attribute][key] for ex in examples if isinstance(ex[attribute][key], torch.Tensor)]\n                if tensors_to_concatenate:\n                    img_args[tensor_key] = concatenate_tensors(attribute, key)\n                else:\n                    img_args[tensor_key] = examples[-1][attribute][key]"
        },
        {
            "comment": "This function is removing 'vision' and 'cross' keys from examples and creating model_args by concatenating tensors and copying other attributes. Then, it merges img_args into model_args and returns the result. The second function classifies keys based on their data type, broadcasts tensor data, and collects in a new dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent_demo.py\":52-81",
            "content": "    # Remove 'vision' and 'cross' keys from examples\n    for example in examples:\n        example.pop('vision', None)\n        example.pop('cross', None)\n    # Create model_args by concatenating tensors and copying other attributes\n    model_args = {key: concatenate_tensors(None, key) \n                  if isinstance(examples[-1][key], torch.Tensor) else examples[-1][key] \n                  for key in examples[-1]\n                  }\n    # Merge img_args into model_args\n    model_args.update(img_args)\n    return model_args\ndef broadcast_auto(data_dict):\n    # Classify keys based on their data type\n    tensor_keys_by_dtype = defaultdict(list)\n    non_tensor_keys = []\n    for key, value in data_dict.items():\n        if isinstance(value, torch.Tensor):\n            tensor_keys_by_dtype[value.dtype].append(key)\n        else:\n            non_tensor_keys.append(key)\n    # Broadcast tensor data and collect in a new dictionary\n    broadcasted_data = {}\n    for dtype, keys in tensor_keys_by_dtype.items():\n        broadcasted_data.update(mpu.broadcast_data(keys, data_dict, dtype))"
        },
        {
            "comment": "This function takes in a model, tokenizer, and tokens, and returns the generated output for a chat conversation. It uses an autoregressive model with optional beam search strategy to generate responses based on input tokens. The maximum length of generated outputs is set to 1800, and parameters such as number of beams, top_p, top_k, and temperature can be customized.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent_demo.py\":83-115",
            "content": "    # Add non-tensor data to the new dictionary\n    for key in non_tensor_keys:\n        broadcasted_data[key] = data_dict[key]\n    return broadcasted_data\ndef get_batch(data_iterator, args, timers):\n    # Broadcast data.\n    timers('data loader').start()\n    if data_iterator is not None:\n        data = next(data_iterator)\n    else:\n        data = None\n    timers('data loader').stop()\n    data_b = broadcast_auto(data)\n    for k in data_b:\n        if type(data_b[k]) is torch.Tensor and data_b[k].dtype is not torch.int32 and data_b[k].dtype is not torch.long:\n            if args.fp16:\n                data_b[k] = data_b[k].half()\n            elif args.bf16:\n                data_b[k] = data_b[k].bfloat16()\n    return data_b\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.generation.autoregressive_sampling import filling_sequence\nfrom sat.generation.sampling_strategies import BaseStrategy, BeamSearchStrategy\ndef chat(model, tokenizer, tokens,\n         max_length: int = 1800, num_beams=5, top_p=0.95, top_k=0, temperature=0.8, **kwargs):"
        },
        {
            "comment": "This code is using a transformer model to generate text based on a given input. It uses a specific strategy for generating the text and applies a text processor function to the input before passing it to the model. The generated text is then returned as output.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent_demo.py\":116-139",
            "content": "    inputs = tokens.to(model.parameters().__next__().device)[0]\n    seq = torch.cat(\n        [inputs, torch.tensor([-1] * (max_length - len(inputs)), device=inputs.device)], dim=0\n    )\n    strategy = BaseStrategy(temperature=temperature, top_p=0.4, top_k=1, end_tokens=[tokenizer.eos_token_id])\n    # strategy = BeamSearchStrategy(temperature=temperature, top_p=top_p, top_k=top_k, end_tokens=[tokenizer.eos_token_id],\n    #                               num_beams=num_beams, consider_end=True)\n    get_func = llama2_text_processor_inference.get_func(None, None, image_rope_mask=kwargs['image_rope_mask'])\n    output = filling_sequence(\n        model, seq,\n        batch_size=1,\n        strategy=strategy,\n        get_masks_and_position_ids=get_func,\n        **kwargs\n    )[0]  # drop memory\n    return output\ndef forward_step_eval(data_iterator, model, args, timers):\n    def compute_metrics(eval_preds):\n        preds, labels, device = eval_preds\n        preds = preds.unsqueeze(0)\n        if isinstance(preds, tuple):"
        },
        {
            "comment": "This code calculates the accuracy of predicted and actual outputs for a text generation model. It compares the predicted tokens with their corresponding labels (ground truth) and stores the accuracy results in a dictionary called 'score_dict'. The script also handles case sensitivity by providing separate accuracy scores with and without considering it.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent_demo.py\":140-166",
            "content": "            preds = preds[0]\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n        if args.ignore_pad_token_for_loss:\n            # Replace -100 in the labels as we can't decode them.\n            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        score_dict = {\n            \"acc\": [],\n            \"acc_w/o_case\": [],\n        }\n        for pred, label in zip(decoded_preds, decoded_labels):\n            if args.rank == 0:\n                print('pred', pred, 'label', label, flush=True)\n            if pred == label:\n                score_dict['acc'].append(1.)\n            else:\n                score_dict['acc'].append(0.)\n            if pred.lower() == label.lower():\n                score_dict['acc_w/o_case'].append(1.)\n            else:\n                score_dict['acc_w/o_case'].append(0.)\n        for k, v in score_dict.items():\n            score_dict[k] = float(np.mean(v))\n        return score_dict"
        },
        {
            "comment": "This code is getting a batch of data and extracting the necessary components for evaluation. It also sets up the model to behave as an autoregressive sequence generator, uses it to generate outputs for each token in the extracted components, removes the mixin, and converts the results into tensors for further processing or returning.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent_demo.py\":168-192",
            "content": "    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)\n    timers('batch generator').stop()\n    context_len = int(data_b['context_length'][0])\n    tokens = data_b['input_ids'][:, :context_len]\n    data_b['vision_expert_mask'] = data_b['vision_expert_mask'][:, :context_len]\n    data_b['image_embed_mask'] = data_b['image_embed_mask'][:, :context_len]\n    data_b['image_rope_mask'] = data_b['image_rope_mask'][:, :context_len]\n    data_b.pop('input_ids')\n    data_b.pop('attention_mask')\n    data_b.pop('position_ids')\n    labels = data_b.pop('labels')\n    qid = data_b.pop('question_id')\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n    outputs = chat(model, tokenizer, tokens, **data_b)[0][context_len:]\n    # print(outputs)\n    model.del_mixin('auto-regressive')\n    return torch.tensor(0, device=outputs.device), {k: torch.tensor(v, device=outputs.device) for k, v in\n                                                    compute_metrics("
        },
        {
            "comment": "The code defines a function \"forward_step\" that performs a forward pass in a model, computes the loss between predicted logits and labels, and returns the loss as output. It uses CrossEntropyLoss for computing loss, ignores padding tokens (-100), and converts the tensors to float32 type. The \"create_dataset_function\" function creates a dataset function using image and text processors, cross_image_processor, path, and args.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent_demo.py\":193-219",
            "content": "                                                        (outputs.cpu(), labels.cpu(), outputs.device)).items()}\nfrom torch.nn import CrossEntropyLoss\ndef forward_step(data_iterator, model, args, timers):\n    \"\"\"Forward step.\"\"\"\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)\n    labels = data_b.pop('labels')\n    timers('batch generator').stop()\n    logits = model(**data_b)[0]\n    lm_logits = logits.to(torch.float32)\n    # Shift so that tokens < n predict n\n    shift_labels = labels[..., 1:].contiguous()\n    shift_logits = lm_logits[..., -1-shift_labels.size(-1):-1, :].contiguous()\n    # Flatten the tokens\n    loss_fct = CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    loss = loss.to(torch.float32)\n    return loss, {'loss': loss}\nfrom utils.utils import ItemDataset\ndef create_dataset_function(image_processor, text_processor, cross_image_processor, path, args):"
        },
        {
            "comment": "This code defines a dataset function, parses command-line arguments, and returns the dataset. It also sets up argument parsing for interacting with a specific version of the model and using a specific pretrained checkpoint and tokenizer.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent_demo.py\":220-236",
            "content": "    dataset = ItemDataset(image_processor, text_processor, args, path, cross_image_processor=cross_image_processor)\n    return dataset\nif __name__ == '__main__':\n    py_parser = argparse.ArgumentParser(add_help=False)\n    py_parser.add_argument('--max_length', type=int)\n    py_parser.add_argument('--ignore_pad_token_for_loss', action='store_false')\n    py_parser.add_argument(\"--version\", type=str, default=\"chat\", help='version to interact with')\n    py_parser.add_argument(\"--from_pretrained\", type=str, default=\"cogagent-chat\", help='pretrained ckpt')\n    py_parser.add_argument(\"--local_tokenizer\", type=str, default=\"lmsys/vicuna-7b-v1.5\", help='tokenizer path')\n    py_parser.add_argument(\"--vit_checkpoint_activations\", action='store_true')\n    py_parser = FineTuneTestCogAgentModel.add_model_specific_args(py_parser)\n    known, args_list = py_parser.parse_known_args()\n    args = get_args(args_list)\n    args = argparse.Namespace(**vars(args), **vars(known))\n    if args.use_qlora:\n        args.device = 'cpu'"
        },
        {
            "comment": "The code is initializing a fine-tuned CogAgent model, preparing the tokenizer and image processors, and calling a training function with specified parameters.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogagent_demo.py\":238-247",
            "content": "    model, args = FineTuneTestCogAgentModel.from_pretrained(args.from_pretrained, args, overwrite_args={'model_parallel_size': args.model_parallel_size} if args.model_parallel_size != 1 else {})\n    if args.use_qlora and torch.cuda.is_available():\n        model = model.to('cuda')\n    from utils.utils import llama2_tokenizer\n    tokenizer = llama2_tokenizer(args.local_tokenizer, signal_type=args.version)\n    image_processor = get_image_processor(args.eva_args[\"image_size\"][0])\n    cross_image_processor = get_image_processor(args.cross_image_pix)\n    text_processor = llama2_text_processor(tokenizer, args.max_length, args.image_length)\n    training_main(args, model_cls=model, forward_step_function=forward_step, create_dataset_function=partial(create_dataset_function, image_processor, text_processor, cross_image_processor), collate_fn=partial(data_collator, cross_image_processor=cross_image_processor), forward_step_eval=forward_step_eval)"
        }
    ]
}
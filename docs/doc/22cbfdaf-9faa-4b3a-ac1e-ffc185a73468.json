{
    "summary": "CogVLM-SFT-311K is a bilingual visual instruction dataset for CogVLM v1.0 training, containing 22,464 images with descriptions and 56,673 images with multi-turn conversations, licensed under Attribution-NonCommercial 4.0 International, and following a JSON structure format.",
    "details": [
        {
            "comment": "This code describes the construction of CogVLM-SFT-311K dataset, a bilingual visual instruction data used in initial training of CogVLM v1.0. It includes 3500 high-quality samples from MiniGPT-4, integration with Llava-Instruct-150K and Chinese translation, followed by noise correction and retranslation to English. The dataset is licensed under Attribution-NonCommercial 4.0 International due to non-commercial agreements.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/dataset.md\":0-10",
            "content": "# CogVLM-SFT-311K: Bilingual Visual Instruction Data in CogVLM SFT\nCogVLM-SFT-311K is the primary aligned corpus used in the initial training of CogVLM v1.0. The process of constructing this dataset is as follows:\n1. Approximately 3500 high-quality data samples were selected from the open source [MiniGPT-4](https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align), known as minigpt4-3500.\n2. Minigpt4-3500 was integrated with [Llava-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) and translated into Chinese through a language model.\n3. We discovered significant noise in the detailed description part of minigpt4-3500 and Llava-instruct. Thus, we corrected these Chinese corpora and retranslated them into English.\n## License\n+ Due to non-commercial agreements, we did not use these data in the bilingual version of CogVLM or any other models involving commercialization.\n+ The dataset license adheres to: <br> Attribution-NonCommercial 4.0 International. It should abide by the policy of OpenAI: https://openai.com/policies/terms-of-use"
        },
        {
            "comment": "Dataset address provided for CogVLM-SFT-311K.\nDataset contains three folders with mixed data from minigpt4-3500 and llava datasets.\nData is distributed in images and labels folders.\nIncludes 22,464 images and descriptions and 56,673 images with multi-turn conversations.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/dataset.md\":11-39",
            "content": "This will not allow you to use these data for any **commercial activitiesI**.\n## Dataset Address\n+ [CogVLM-SFT-311K](https://huggingface.co/datasets/THUDM/CogVLM-SFT-311K)\n## Dataset Information\nThe dataset contains three folders corresponding to the mixed part of minigpt4-3500 and llava, the llava solo conversation, and the multi-turn conversation datasets. Their layout is as follows:\n```\n.CogVLM-SFT-311K\n\u251c\u2500\u2500 llava_details-minigpt4_3500_formate\n\u251c\u2500\u2500 llava_instruction_multi_conversations_formate\n\u2514\u2500\u2500 llava_instruction_single_conversation_formate\n```\nIn our open-source data, the datasets are distributed as follows:\n```\n.llava_details-minigpt4_3500_formate\n\u251c\u2500\u2500 images\n\u2502   \u2514\u2500\u2500 00000001.jpg\n\u2514\u2500\u2500 labels\n    \u2514\u2500\u2500 00000001.json\n```\nImages are stored in the images folder, while labels containing the corresponding image description or dialogue are stored in the labels folder.\n## Dataset Quantity\n+ llava_details-minigpt4_3500_formate: 22,464 images and descriptions\n+ llava_instruction_multi_conversations_formate: 56,673 images and multi-turn conversations"
        },
        {
            "comment": "Code is defining a dataset format for single-turn conversations and image descriptions. It includes a caption format for image description and a conversation format for image dialogue, both in JSON structure.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/dataset.md\":40-65",
            "content": "+ llava_instruction_single_conversation_formate: 76,634 images and single-turn conversations\nDataset Format\nCaption format for image description\n```\n{\n  \"captions\": [\n    {\n      \"role\": \"caption\",\n      \"content\": \"The photograph features a beach scene with a group of people surfing in the ocean. There are ten individuals spaced out in the water, sitting or lying on various sizes of body boards. The surfers line up in a row, close to the sandy shoreline. Some of the body boards are visible floating on the surface of the water, with the surfers riding on them or nearby.\"\n    }\n  ]\n}\n```\nConversation format for image dialogue\n```\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What can be inferred about the zebras' behavior and surroundings?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Based on the image, we can infer that the two zebras are likely seeking relief from the sun's heat, as they are standing side by side under the branches of a thorny tree. This shade-providing "
        },
        {
            "comment": "The code seems to be describing a scene where zebras are seeking shelter under a tree in a green field to adapt to their environment and survive.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/dataset.md\":65-74",
            "content": "tree offers some respite from the sun, possibly during the hottest part of the day. The zebras are in a green field with grass, providing them with an ideal environment to graze and eat while staying near their source of shelter. This shows that the zebras' behavior is influenced by the conditions and available resources in their surroundings. It also highlights that these animals adopt strategies to adapt to the fluctuating conditions of their environment, such as cooperation and seeking shelter, to survive and thrive in their natural habitat.\"\n    }\n  ]\n}\n```\n## References\nThis project utilizes data and concepts based on the following research papers:\n- Zhu, D., Chen, J., Shen, X., Li, X., & Elhoseiny, M. (2023). MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. arXiv preprint arXiv:2304.10592.\n- Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Visual Instruction Tuning. arXiv:2304.08485. "
        }
    ]
}
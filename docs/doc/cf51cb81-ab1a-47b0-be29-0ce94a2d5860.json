{
    "summary": "The script sets environment variables, specifies model details, and runs a language model finetuning process using Deepspeed, NCCL, and CUDA with distributed backend, cosine learning rate decay, checkpoint activations, and saves/evaluates every 200 steps.",
    "details": [
        {
            "comment": "This script is setting environment variables for CUDA and LD_LIBRARY_PATH, defining the number of GPUs per worker and MPI size. It also specifies the model type, version, arguments, options for SAT and NCCL, path to host file, and data locations for training and validation.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_lora.sh\":0-35",
            "content": "#! /bin/bash\n# export PATH=/usr/local/cuda/bin:$PATH\n# export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\nNUM_GPUS_PER_WORKER=8\nMP_SIZE=1\nscript_path=$(realpath $0)\nscript_dir=$(dirname $script_path)\nmain_dir=$(dirname $script_dir)\nMODEL_TYPE=\"cogagent-chat\"\nVERSION=\"chat\"\nMODEL_ARGS=\"--from_pretrained $MODEL_TYPE \\\n    --max_length 400 \\\n    --lora_rank 50 \\\n    --use_lora \\\n    --local_tokenizer lmsys/vicuna-7b-v1.5 \\\n    --version $VERSION\"\n# TIPS: max_length include low-resolution image sequence (which has 256 tokens) \nOPTIONS_SAT=\"SAT_HOME=~/.sat_models\"\nOPTIONS_NCCL=\"NCCL_DEBUG=info NCCL_IB_DISABLE=0 NCCL_NET_GDR_LEVEL=2 LOCAL_WORLD_SIZE=$NUM_GPUS_PER_WORKER\"\nHOST_FILE_PATH=\"hostfile\"\ntrain_data=\"./archive_split/train\"\nvalid_data=\"./archive_split/valid\"\ngpt_options=\" \\\n       --experiment-name finetune-$MODEL_TYPE \\\n       --model-parallel-size ${MP_SIZE} \\\n       --mode finetune \\\n       --train-iters 2000 \\\n       --resume-dataloader \\\n       $MODEL_ARGS \\\n       --train-data ${train_data} \\\n       --valid-data ${valid_data} \\"
        },
        {
            "comment": "This script is finetuning a language model using Deepspeed, a distributed training framework. It uses NCCL for distributed backend, cosine learning rate decay style, 0.2 warmup ratio, checkpoint activations and VIT checkpoint activations, saves checkpoints every 200 steps, evaluates the model every 200 steps, and runs on a machine with hostfile located at ${HOST_FILE_PATH}. The master port is 16666. It uses a test configuration file named \"test_config_bf16.json\", skips initialization, sets a seed value of 2023, and executes the finetune_cogagent_demo.py script with provided GPT options.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/finetune_cogagent_lora.sh\":36-58",
            "content": "       --distributed-backend nccl \\\n       --lr-decay-style cosine \\\n       --warmup .02 \\\n       --checkpoint-activations \\\n       --vit_checkpoint_activations \\\n       --save-interval 200 \\\n       --eval-interval 200 \\\n       --save \"./checkpoints\" \\\n       --eval-iters 10 \\\n       --eval-batch-size 1 \\\n       --split 1. \\\n       --deepspeed_config test_config_bf16.json \\\n       --skip-init \\\n       --seed 2023\n\"\nrun_cmd=\"${OPTIONS_NCCL} ${OPTIONS_SAT} deepspeed --master_port 16666 --hostfile ${HOST_FILE_PATH} finetune_cogagent_demo.py ${gpt_options}\"\necho ${run_cmd}\neval ${run_cmd}\nset +x"
        }
    ]
}
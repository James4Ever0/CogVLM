{
    "summary": "The code sets up libraries, configurations, and manages interaction with large language models. It includes functions for efficient user input handling, threading for generation, streaming generation setup, and starts a thread to generate tokens using the model, yielding the generated token stream.",
    "details": [
        {
            "comment": "The code is importing necessary libraries and setting up configurations for the chatbot system. It checks if the GPU supports bfloat16, and if not, it warns and uses fp16 instead. The models' info includes paths and devices for tokenizer, agent_chat model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/composite_demo/client.py\":0-29",
            "content": "from __future__ import annotations\nfrom threading import Thread\nimport streamlit as st\nimport torch\nimport warnings\nimport os\nfrom typing import Any, Protocol\nfrom collections.abc import Iterable\nfrom huggingface_hub.inference._text_generation import TextGenerationStreamResponse, Token\nfrom transformers import AutoTokenizer, TextIteratorStreamer, AutoModelForCausalLM\nfrom conversation import Conversation\n# Check if GPU supports bfloat16\nif torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n    torch_type = torch.bfloat16\nelse:\n    torch_type = torch.float16\n    warnings.warn(\"Your GPU does not support bfloat16 type, use fp16 instead\")\n# if you use all of Our model, include cogagent-chat cogvlm-chat cogvlm-grounding and put it in different devices, you can do like this.\nmodels_info = {\n    'tokenizer': {\n        'path': os.environ.get('TOKENIZER_PATH', 'lmsys/vicuna-7b-v1.5'),\n    },\n    'agent_chat': {\n        'path': os.environ.get('MODEL_PATH_AGENT_CHAT', 'THDUM/cogagent-chat-hf'),\n        'device': ['cuda:0']"
        },
        {
            "comment": "The code defines a dictionary `models_info` containing information about various language models, such as their path and the device on which they should be run. It also provides a function `get_client()` to create an HFClient object with the specified model information, and another function `process_history()` to process conversation history by extracting the query and history pairs.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/composite_demo/client.py\":30-66",
            "content": "    },\n    'vlm_chat': {\n        'path': os.environ.get('MODEL_PATH_VLM_CHAT', 'THDUM/cogvlm-chat-hf'),\n        'device': ['cuda:3']\n    },\n    'vlm_grounding': {\n        'path': os.environ.get('MODEL_PATH_VLM_GROUNDING','THDUM/cogvlm-grounding-generalist-hf'),\n        'device': ['cuda:6']\n    }\n}\n# if you just use one model, use like this\n# models_info = {\n#     'tokenizer': {\n#         'path': os.environ.get('TOKENIZER_PATH', 'lmsys/vicuna-7b-v1.5'),\n#     },\n#     'agent_chat': {\n#         'path': os.environ.get('MODEL_PATH_AGENT_CHAT', 'THUDM/cogagent-chat-hf'),\n#         'device': ['cuda:0']\n#     },\n@st.cache_resource\ndef get_client() -> Client:\n    client = HFClient(models_info)\n    return client\ndef process_history(history: list[Conversation]):\n    \"\"\"\n        Process the input history to extract the query and the history pairs.\n        Args:\n            History(list[Conversation]): A list of Conversation objects representing all conversations.\n        Returns:\n            query(str): The current user input string."
        },
        {
            "comment": "This code defines a function and a class. The function, `generate_stream`, iterates through the history of conversations, extracting user text and assistant responses into a list called `history_pairs`. It also saves the last user image and the most recent query in separate variables. Finally, it returns these values.\n\nThe class, `Client`, is defined as a Protocol and does not contain any methods or attributes.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/composite_demo/client.py\":67-97",
            "content": "            history_pairs(list[(str,str)]): A list of (user, assistant) pairs.\n            last_user_image(Image): The last user image. Only the latest image.\n    \"\"\"\n    history_pairs = []\n    query = \"\"\n    last_user_image = None\n    user_text = None\n    for i, conversation in enumerate(history):\n        if conversation.role == conversation.role.USER:\n            user_text = conversation.content\n            if conversation.image:\n                last_user_image = conversation.image\n            if i == len(history) - 1:\n                query = conversation.content\n        else:\n            if user_text is not None:\n                history_pairs.append((user_text, conversation.content))\n                user_text = None\n    return query, history_pairs, last_user_image\nclass Client(Protocol):\n    def generate_stream(self,\n                        history: list[Conversation],\n                        grounding: bool = False,\n                        model_use: str = 'agent_chat',\n                        **parameters: Any"
        },
        {
            "comment": "HFClient manages interaction with various large language models for text generation tasks, supports multiple models and specified tasks (chatting or grounding), loads each model based on provided information, assigns to CUDA device, handles tokenizer used across all models.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/composite_demo/client.py\":98-119",
            "content": "                        ) -> Iterable[TextGenerationStreamResponse]:\n        ...\nclass HFClient(Client):\n    \"\"\"\n        The HFClient class manages the interaction with various large language models\n        for text generation tasks. It supports handling multiple models, each designated\n        for a specific task like chatting or grounding.\n        Args:\n            models_info (dict): A dictionary containing the configuration for each model.\n                The dictionary format is:\n                    - 'tokenizer': Path and settings for the tokenizer.\n                    - 'agent_chat': Path and settings for the CogAgent-chat-18B model.\n                    - 'vlm_chat': Path and settings for the CogVLM-chat-17B model.\n                    - 'vlm_grounding': Path and settings for the CogVLM-grounding-17B model.\n        The class loads each model based on the provided information and assigns it to the\n        specified CUDA device. It also handles the tokenizer used across all models.\n        \"\"\"\n    def __init__(self, models_info):"
        },
        {
            "comment": "Creates a dictionary of models and initializes the tokenizer. Iterates over each model, appends to dictionary for specified model name based on device and selects the best GPU for a specific model name.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/composite_demo/client.py\":120-142",
            "content": "        self.models = {}\n        self.tokenizer = AutoTokenizer.from_pretrained(models_info['tokenizer']['path'], trust_remote_code=True)\n        for model_name, model_info in models_info.items():\n            if model_name != 'tokenizer':\n                self.models[model_name] = []\n                for device in model_info['device']:\n                    model = AutoModelForCausalLM.from_pretrained(\n                        model_info['path'],\n                        torch_dtype=torch_type,\n                        low_cpu_mem_usage=True,\n                        trust_remote_code=True,\n                    ).to(device).eval()\n                    self.models[model_name].append(model)\n    def select_best_gpu(self, model_name):\n        min_memory_used = None\n        selected_model = None\n        for model in self.models[model_name]:\n            device = next(model.parameters()).device\n            mem_used = torch.cuda.memory_allocated(device=device)\n            if min_memory_used is None or mem_used < min_memory_used:"
        },
        {
            "comment": "This function generates a stream of text responses based on input history and selected model. It can enable grounding if specified, and the behavior changes depending on selected model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/composite_demo/client.py\":143-166",
            "content": "                min_memory_used = mem_used\n                selected_model = model\n        return selected_model\n    def generate_stream(self,\n                        history: list,\n                        grounding: bool = False,\n                        model_use: str = 'agent_chat',\n                        **parameters: Any\n                        ) -> Iterable[TextGenerationStreamResponse]:\n        \"\"\"\n        Generates a stream of text responses based on the input history and selected model.\n        This method facilitates a chat-like interaction with the models. Depending on the\n        model selected and whether grounding is enabled, it alters the behavior of the text\n        generation process.\n        Args:\n            history (list[Conversation]): A list of Conversation objects representing the\n                dialogue history.\n            grounding (bool, optional): A flag to indicate whether grounding should be used\n                in the generation process. Defaults to False.\n            model_use (str, optional): The key name of the model to be used for the generation."
        },
        {
            "comment": "This code selects the appropriate model based on the input parameter `model_use`, processes the input history, and feeds it into the model to generate text. It uses threading for efficient generation process handling. The user input information is printed including query, history, model details, and device details.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/composite_demo/client.py\":167-193",
            "content": "                Defaults to 'agent_chat'.\n            **parameters (Any): Additional parameters that may be required for the generation\n                process.\n        Yields:\n            Iterable[TextGenerationStreamResponse]: A stream of text generation responses, each\n            encapsulating a generated piece of text.\n        The method selects the appropriate model based on `model_use`, processes the input\n        history, and feeds it into the model to generate text. It uses threading to handle\n        the generation process efficiently.\n        \"\"\"\n        query, history, image = process_history(history)\n        if grounding:\n            query += \"(with grounding)\"\n        model = self.select_best_gpu(model_use)\n        device = next(model.parameters()).device\n        # Print user input info\n        print(\"\\n== Input ==\\n\", query)\n        print(\"\\n==History==\\n\", history)\n        print(\"\\n== Model ==\\n\\n\", model.config.name_or_path)\n        print(\"\\n== Device ==\\n\\n\", device)\n        input_by_model = model.build_conversation_input_ids("
        },
        {
            "comment": "Code is creating a dictionary of inputs for model inference, handling cross_images if present and setting up the TextIteratorStreamer for streaming generation.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/composite_demo/client.py\":194-215",
            "content": "            self.tokenizer,\n            query=query,\n            history=history,\n            images=[image]\n        )\n        inputs = {\n            'input_ids': input_by_model['input_ids'].unsqueeze(0).to(device),\n            'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(device),\n            'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(device),\n            'images': [[input_by_model['images'][0].to(device).to(torch_type)]],\n        }\n        # CogVLM model do not have param 'cross_images', Only CogAgent have.\n        if 'cross_images' in input_by_model and input_by_model['cross_images']:\n            inputs['cross_images'] = [[input_by_model['cross_images'][0].to(device).to(torch_type)]]\n        # Use TextIteratorStreamer for streaming generation like huggingface.\n        streamer = TextIteratorStreamer(self.tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n        parameters['streamer'] = streamer\n        gen_kwargs = {**parameters, **inputs}"
        },
        {
            "comment": "Starts a thread to generate tokens using the model and yields the generated token stream.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/composite_demo/client.py\":216-227",
            "content": "        with torch.no_grad():\n            thread = Thread(target=model.generate, kwargs=gen_kwargs)\n            thread.start()\n            for next_text in streamer:\n                yield TextGenerationStreamResponse(\n                    token=Token(\n                        id=0,\n                        logprob=0,\n                        text=next_text,\n                        special=False,\n                    )\n                )"
        }
    ]
}
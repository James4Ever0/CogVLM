{
    "summary": "Both comments describe the initialization and definition of vision transformer models, with Comment A emphasizing CLIP Vision model's layer configuration and Comment B explaining Eva2LargeEncoder for EVAVisionTransformer model and position embedding.",
    "details": [
        {
            "comment": "This function, `broadcast`, performs broadcastable concatenation of tensors along a specified dimension. It takes a list of tensors and a dimension (default is -1) as input parameters. It checks if all tensors have the same number of dimensions and that the dimensions are valid for broadcasting. Then it expands the dimensions, creating new tensor shapes to accommodate the broadcasted operation. Finally, it expands each tensor along its corresponding dimension and returns a list of expanded tensors.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":0-19",
            "content": "from math import pi\nimport torch\nfrom torch import nn\nfrom einops import rearrange, repeat\nimport logging\ndef broadcat(tensors, dim = -1):\n    num_tensors = len(tensors)\n    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n    assert len(shape_lens) == 1, 'tensors must all have the same number of dimensions'\n    shape_len = list(shape_lens)[0]\n    dim = (dim + shape_len) if dim < 0 else dim\n    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), 'invalid dimensions for broadcastable concatentation'\n    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n    expanded_dims.insert(dim, (dim, dims[dim]))\n    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))"
        },
        {
            "comment": "This code defines a class `VisionRotaryEmbeddingFast` for creating rotary position embeddings. It takes in parameters such as the dimensionality (dim), patch sequence length (pt_seq_len), feature sequence length (ft_seq_len), custom frequencies, and frequency mode. It initializes the object by determining the frequencies based on the provided parameters using the `freqs` variable. The code also includes a helper function called `rotate_half` for rotating half of the tensor.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":20-53",
            "content": "    return torch.cat(tensors, dim = dim)\ndef rotate_half(x):\n    x = rearrange(x, '... (d r) -> ... d r', r = 2)\n    x1, x2 = x.unbind(dim = -1)\n    x = torch.stack((-x2, x1), dim = -1)\n    return rearrange(x, '... d r -> ... (d r)')\nclass VisionRotaryEmbeddingFast(nn.Module):\n    def __init__(\n        self,\n        dim,\n        pt_seq_len,\n        ft_seq_len=None,\n        custom_freqs = None,\n        freqs_for = 'lang',\n        theta = 10000,\n        max_freq = 10,\n        num_freqs = 1,\n        patch_dropout = 0.\n    ):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == 'lang':\n            freqs = 1. / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n        elif freqs_for == 'pixel':\n            freqs = torch.linspace(1., max_freq / 2, dim // 2) * pi\n        elif freqs_for == 'constant':\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f'unknown modality {freqs_for}')\n        if ft_seq_len is None: ft_seq_len = pt_seq_len"
        },
        {
            "comment": "Computing Fourier coefficients for time sequence.\nRegistering Fourier coefficients as buffers.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":54-79",
            "content": "        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n        freqs = torch.einsum('..., f -> ... f', t, freqs)\n        freqs = repeat(freqs, '... n -> ... (n r)', r = 2)\n        freqs = broadcat((freqs[:, None, :], freqs[None, :, :]), dim = -1)\n        freqs_cos = freqs.cos().view(-1, freqs.shape[-1])\n        freqs_sin = freqs.sin().view(-1, freqs.shape[-1])\n        self.patch_dropout = patch_dropout\n        self.register_buffer(\"freqs_cos\", freqs_cos)\n        self.register_buffer(\"freqs_sin\", freqs_sin)\n        logging.info(f'Shape of rope freq: {self.freqs_cos.shape}')\n    def forward(self, t, patch_indices_keep=None):\n        if patch_indices_keep is not None:\n            batch = t.size()[0]\n            batch_indices = torch.arange(batch)\n            batch_indices = batch_indices[..., None]\n            freqs_cos = repeat(self.freqs_cos, 'i j -> n i m j', n=t.shape[0], m=t.shape[1])\n            freqs_sin = repeat(self.freqs_sin, 'i j -> n i m j', n=t.shape[0], m=t.shape[1])\n            freqs_cos = freqs_cos[batch_indices, patch_indices_keep]"
        },
        {
            "comment": "Code at line 80-114:\nThe code calculates the output by multiplying input 't' with cosine values from self.freqs_cos and rotated half of 't' with sinusoidal values from self.freqs_sin, then summing them together. This is part of a model function that returns the transformed input.\n\nCode from 80 to 114:\nThis section defines a PatchDropout class which applies dropout to specific patches in an image and performs operations such as forward, __call__, and extra methods like _get_drop_patches. It takes patch size and drop probability as parameters for initialization and uses torch.nn.functional.dropout internally.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":80-114",
            "content": "            freqs_cos = rearrange(freqs_cos, 'n i m j -> n m i j')\n            freqs_sin = freqs_sin[batch_indices, patch_indices_keep]\n            freqs_sin = rearrange(freqs_sin, 'n i m j -> n m i j')\n            return  t * freqs_cos + rotate_half(t) * freqs_sin\n        return  t * self.freqs_cos + rotate_half(t) * self.freqs_sin\nimport torch.nn as nn\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nfrom functools import partial\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n# --------------------------------------------------------\n# Adapted from  https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\nimport math\nimport os\nfrom functools import partial\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport logging\ntry:\n    from timm.models.layers import drop_path, to_2tuple, trunc_normal_\nexcept:\n    from timm.layers import drop_path, to_2tuple, trunc_normal_\nclass PatchDropout(nn.Module):"
        },
        {
            "comment": "This code defines a class for masking a random set of patches in an input tensor during training. It takes two arguments, the probability of masking and whether to exclude the first token (CLS) or not. The forward method applies the mask based on these parameters.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":115-149",
            "content": "    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n    def __init__(self, prob, exclude_first_token=True):\n        super().__init__()\n        assert 0 <= prob < 1.\n        self.prob = prob\n        self.exclude_first_token = exclude_first_token  # exclude CLS token\n        logging.info(f\"os.getenv('RoPE')={os.getenv('RoPE')}\")\n    def forward(self, x):\n        if not self.training or self.prob == 0.:\n            return x\n        if self.exclude_first_token:\n            cls_tokens, x = x[:, :1], x[:, 1:]\n        else:\n            cls_tokens = torch.jit.annotate(torch.Tensor, x[:, :1])\n        batch = x.size()[0]\n        num_tokens = x.size()[1]\n        batch_indices = torch.arange(batch)\n        batch_indices = batch_indices[..., None]\n        keep_prob = 1 - self.prob\n        num_patches_keep = max(1, int(num_tokens * keep_prob))\n        rand = torch.randn(batch, num_tokens)\n        patch_indices_keep = rand.topk(num_patches_keep, dim=-1).indices\n        x = x[batch_indices, patch_indices_keep]\n        if self.exclude_first_token:"
        },
        {
            "comment": "Code is defining classes for DropPath and Mlp in a neural network model. DropPath implements drop paths (Stochastic Depth) to randomly zero out some channels of input during training. Mlp defines a fully connected layer with a middle layer that can be used as an additional module in the network. Both classes take in features and activation layers as parameters for their construction.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":150-188",
            "content": "            x = torch.cat((cls_tokens, x), dim=1)\n        if self.training and os.getenv('RoPE') == '1':\n            return x, patch_indices_keep\n        return x\nif os.getenv('ENV_TYPE') == 'deepspeed':\n    try:\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\nimport xformers.ops as xops\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n    def extra_repr(self) -> str:\n        return 'p={}'.format(self.drop_prob)\nclass Mlp(nn.Module):\n    def __init__(\n        self, \n        in_features, \n        hidden_features=None, \n        out_features=None, \n        act_layer=nn.GELU, \n        norm_layer=nn.LayerNorm, "
        },
        {
            "comment": "Class \"eva_clip_L_hf\" is a neural network layer that takes in input features, performs linear transformation (fc1), applies activation function (act), normalization (ffn_ln if subln=False) and another linear transformation (fc2). It also has a dropout layer.\n\nClass \"SwiGLU\" is a neural network layer that takes in input features, performs two linear transformations (w1 and w2) and applies activation functions.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":189-222",
            "content": "        drop=0.,\n        subln=False,\n        ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement \n        x = self.ffn_ln(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\nclass SwiGLU(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0., \n                norm_layer=nn.LayerNorm, subln=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.w1 = nn.Linear(in_features, hidden_features)"
        },
        {
            "comment": "This code defines a neural network model for the \"Efficient Vision Transformer\" architecture. It includes modules such as Linear layers (nn.Linear), activation functions (act_layer), normalization layers (norm_layer), and Dropout layers (nn.Dropout). The Attention class is used to compute attention scores using multi-head attention.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":223-253",
            "content": "        self.w2 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.w3 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):\n        x1 = self.w1(x)\n        x2 = self.w2(x)\n        hidden = self.act(x1) * x2\n        x = self.ffn_ln(hidden)\n        x = self.w3(x)\n        x = self.drop(x)\n        return x\nclass Attention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None, xattn=False, rope=None, subln=False, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n        self.subln = subln\n        if self.subln:"
        },
        {
            "comment": "Initialize query, key, and value projection layers for the MultiHeadAttention module. If qkv_bias is True, add separate bias parameters for each projection layer. If window_size is provided, initialize relative position bias table and set num_relative_distance and coords_h variables.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":254-275",
            "content": "            self.q_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=False)\n        else:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])"
        },
        {
            "comment": "This code calculates relative coordinates and creates an index for the relative position in a 2D grid. It then assigns labels to these positions based on their distances from the center of the grid and from the outer edges.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":276-289",
            "content": "            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(window_size[0] * window_size[1] + 1, ) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1"
        },
        {
            "comment": "This code defines a class for a self-attention mechanism. It initializes variables for the window size, relative position bias table, and index. If no window size is provided, it sets them to None. The class also includes an attention dropout layer and layers for linear transformations. The forward function performs linear transformations on input x, reshapes the output, and permutes the dimensions.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":291-314",
            "content": "            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.inner_attn_ln = norm_layer(all_head_dim) if subln else nn.Identity()\n        # self.proj = nn.Linear(all_head_dim, all_head_dim)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n        self.rope = rope\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        B, N, C = x.shape\n        if self.subln: \n            q = F.linear(input=x, weight=self.q_proj.weight, bias=self.q_bias)\n            k = F.linear(input=x, weight=self.k_proj.weight, bias=None)\n            v = F.linear(input=x, weight=self.v_proj.weight, bias=self.v_bias)\n            q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)     # B, num_heads, N, C"
        },
        {
            "comment": "This code is responsible for the transformation and computation of query (q), key (k), and value (v) tensors in a transformer model. The code handles these operations based on certain conditions: if self.rope or self.xattn are True, it performs additional operations such as applying the rope function or permuting the tensor shapes. The qkv_bias is used when both self.q_bias and self.v_bias exist, otherwise, a linear layer (F.linear) is used to compute qkv. Finally, the code reshapes and permutes the tensors according to their specific shapes.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":315-339",
            "content": "            k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)  \n            v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3) \n        else: \n            qkv_bias = None\n            if self.q_bias is not None:\n                qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)   # 3, B, num_heads, N, C\n            q, k, v = qkv[0], qkv[1], qkv[2]\n        if self.rope:\n            # slightly fast impl\n            q_t = q[:, :, 1:, :]\n            ro_q_t = self.rope(q_t)\n            q = torch.cat((q[:, :, :1, :], ro_q_t), -2).type_as(v)\n            k_t = k[:, :, 1:, :]\n            ro_k_t = self.rope(k_t)\n            k = torch.cat((k[:, :, :1, :], ro_k_t), -2).type_as(v)\n        if self.xattn:\n            q = q.permute(0, 2, 1, 3)   # B, num_heads, N, C -> B, N, num_heads, C\n            k = k.permute(0, 2, 1, 3)"
        },
        {
            "comment": "This code block is performing multi-head self-attention using either memory efficient method or matrix multiplication, depending on the flag. It also applies dropout and optionally adds relative position biases to the attention scores.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":340-363",
            "content": "            v = v.permute(0, 2, 1, 3)\n            x = xops.memory_efficient_attention(\n                q, k, v,\n                p=self.xattn_drop,\n                scale=self.scale,\n                )\n            x = x.reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        else:\n            q = q * self.scale\n            attn = (q @ k.transpose(-2, -1))\n            if self.relative_position_bias_table is not None:\n                relative_position_bias = \\\n                    self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                        self.window_size[0] * self.window_size[1] + 1,\n                        self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0).type_as(attn)\n            if rel_pos_bias is not None:"
        },
        {
            "comment": "This code defines a block for the Transformer architecture. It consists of a normalization layer, an attention mechanism with optional self-attention masking and dropout, a feedforward network with optional dropout, and possibly other operations depending on the parameters specified in the constructor.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":364-389",
            "content": "                attn = attn + rel_pos_bias.type_as(attn)\n            if attn_mask is not None:\n                attn_mask = attn_mask.bool()\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        return x\nclass Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None, xattn=False, rope=None, postnorm=False,\n                 subln=False, naiveswiglu=False):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,"
        },
        {
            "comment": "This code defines a class with various layers and parameters for a transformer model. It includes an attention dropout, projection dropout, window size, attention head dimensions, cross-attention, relative position embedding, normalization layer, and a drop path for stochastic depth. The MLP block is optional depending on the naiveswiglu variable. If init_values is not None and greater than 0, it initializes the gamma_1 parameter.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":390-414",
            "content": "            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim,\n            xattn=xattn, rope=rope, subln=subln, norm_layer=norm_layer)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        if naiveswiglu:\n            self.mlp = SwiGLU(\n                in_features=dim, \n                hidden_features=mlp_hidden_dim, \n                subln=subln,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.mlp = Mlp(\n                in_features=dim, \n                hidden_features=mlp_hidden_dim, \n                act_layer=act_layer,\n                subln=subln,\n                drop=drop\n            )\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)"
        },
        {
            "comment": "This code is defining a forward function for a transformer model that includes attention and feed-forward layers. It has the option to include layer normalization before or after the layers, and uses dropout for regularization. The gamma parameters are learnable scaling factors for the output of each layer when they're not set to None.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":415-434",
            "content": "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n        self.postnorm = postnorm\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        if self.gamma_1 is None:\n            if self.postnorm:\n                x = x + self.drop_path(self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            if self.postnorm:\n                x = x + self.drop_path(self.gamma_1 * self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.gamma_2 * self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))"
        },
        {
            "comment": "The code defines a class `eva_clip_L_hf` that performs some operation on input `x` using drop path and mlp layers, and returns the result. It also includes a nested class `PatchEmbed` that takes an image as input, performs convolutional projection to convert it into patches, and flattens and transposes the output. The code contains an assertion for checking if the input image size matches the expected model size.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":435-459",
            "content": "                x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)"
        },
        {
            "comment": "This code initializes a RelativePositionBias class that calculates pair-wise relative position indices for tokens within a specified window. The class takes two parameters: window_size and num_heads. It creates a relative_position_bias_table with the shape (num_relative_distance, num_heads) and fills it with zeros. The code then calculates the pair-wise relative position indices using meshgrid and meshgrid inverse operations. These calculated indices are used for biasing the attention scores in the transformer model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":460-480",
            "content": "        return x\nclass RelativePositionBias(nn.Module):\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0"
        },
        {
            "comment": "This code calculates the relative position indices and generates a position bias table for a vision transformer model. The position bias is used in the forward pass to account for relative positions between tokens within the input window.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":481-500",
            "content": "        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = \\\n            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n    def forward(self):\n        relative_position_bias = \\\n            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                self.window_size[0] * self.window_size[1] + 1,\n                self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\nclass EVAVisionTransformer(nn.Module):"
        },
        {
            "comment": "This code is initializing a Vision Transformer model with options for patch or hybrid CNN input stage. It takes in parameters such as image size, patch size, number of channels, number of classes, embedding dimension, depth, number of attention heads, mlp ratio, and more. The model uses PatchEmbed class from the same module to handle the input embeddings.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":501-515",
            "content": "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, patch_dropout=0.,\n                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False, rope=False,\n                 use_mean_pooling=True, init_scale=0.001, grad_checkpointing=False, xattn=False, postnorm=False,\n                 pt_hw_seq_len=16, intp_freq=False, naiveswiglu=False, subln=False):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)"
        },
        {
            "comment": "This code initializes the class attributes for a transformer model. It creates parameters for the classification token, position embedding (optional), and dropout probability. The code also handles optional arguments for relative position bias and rotation embeddings, as well as an attribute for naive_swiglu function.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":516-543",
            "content": "        num_patches = self.patch_embed.num_patches\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n        if rope:\n            half_head_dim = embed_dim // num_heads // 2\n            hw_seq_len = img_size // patch_size\n            self.rope = VisionRotaryEmbeddingFast(\n                dim=half_head_dim,\n                pt_seq_len=pt_hw_seq_len,\n                ft_seq_len=hw_seq_len if intp_freq else None,\n                # patch_dropout=patch_dropout\n            )\n        else: \n            self.rope = None\n        self.naiveswiglu = naiveswiglu"
        },
        {
            "comment": "Creates a list of Blocks for the transformer model, with dropout rate and stochastic depth decay rule. Adds norm layer and linear layer for output head. Initializes weights normally.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":545-561",
            "content": "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n                xattn=xattn, rope=self.rope, postnorm=postnorm, subln=subln, naiveswiglu=naiveswiglu)\n            for i in range(depth)])\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)"
        },
        {
            "comment": "This code initializes and configures the model. It applies weight initialization, sets patch dropout if necessary, and fixes the weight scale for certain layers.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":562-588",
            "content": "        # trunc_normal_(self.mask_token, std=.02)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=.02)\n            self.head.weight.data.mul_(init_scale)\n            self.head.bias.data.mul_(init_scale)\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0. else nn.Identity()\n        self.grad_checkpointing = grad_checkpointing\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            if self.naiveswiglu:\n                rescale(layer.mlp.w3.weight.data, layer_id + 1)\n            else:\n                rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n    def get_cast_dtype(self) -> torch.dtype:"
        },
        {
            "comment": "Method return_dtype: Returns the data type of the weight matrix in the first block's MLP.\n\nMethod init_weights: Initializes the weights for linear layers using truncated normal distribution with a standard deviation of 0.02, and initializes bias to 0.\n\nMethod get_num_layers: Returns the number of blocks in the model.\n\nMethod lock: Locks all parameters in the model by setting their requires_grad attribute to False. Optional argument unlocked_groups can be used for partial locking (not currently supported).\n\nMethod set_grad_checkpointing: Enables or disables gradient checkpointing, a technique to save memory during backpropagation.\n\nMethod no_weight_decay: Specifies which layers should not have weight decay applied to them ('pos_embed' and 'cls_token' in this case).\n\nMethod get_classifier: Returns the classifier head of the model.\n\nMethod reset_classifier: Resets the classifier head by setting its num_classes attribute and optionally specifying a global pooling method.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":589-620",
            "content": "        return self.blocks[0].mlp.fc2.weight.dtype\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n    def get_num_layers(self):\n        return len(self.blocks)\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, 'partial locking not currently supported for this model'\n        for param in self.parameters():\n            param.requires_grad = False\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n    def get_classifier(self):\n        return self.head\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes"
        },
        {
            "comment": "This code initializes a linear layer for the head and defines a forward_features method. The method applies patch embedding, concatenates class tokens, adds positional embeddings, applies position dropout, and optionally applies patch dropout based on an environment variable 'RoPE'. If RoPE is set to 1 and patch_dropout is not an identity function, it performs a patch dropout operation and updates the forward method of rope module accordingly. Otherwise, it directly applies patch dropout.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":621-641",
            "content": "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n    def forward_features(self, x, return_all_features=False):\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        if os.getenv('RoPE') == '1':\n            if self.training and not isinstance(self.patch_dropout, nn.Identity):\n                x, patch_indices_keep = self.patch_dropout(x)\n                self.rope.forward = partial(self.rope.forward, patch_indices_keep=patch_indices_keep)\n            else:\n                self.rope.forward = partial(self.rope.forward, patch_indices_keep=None)\n                x = self.patch_dropout(x)"
        },
        {
            "comment": "This code defines a layer norm function that is a subclass of PyTorch's LayerNorm. It also includes the forward pass for a model with blocks and patch dropout, and it returns features if requested.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":642-673",
            "content": "        else:\n            x = self.patch_dropout(x)\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for i, blk in enumerate(self.blocks):\n            if i == len(self.blocks)-1:\n                continue\n            if self.grad_checkpointing:\n                x = checkpoint(blk, x, (rel_pos_bias,))\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias)\n        if not return_all_features:\n            x = self.norm(x)\n            if self.fc_norm is not None:\n                return self.fc_norm(x.mean(1))\n            else:\n                return x[:, 0]\n        return x\n    def forward(self, x, return_all_features=False):\n        if return_all_features:\n            return self.forward_features(x, return_all_features)\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm (with cast back to input dtype).\"\"\"\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype"
        },
        {
            "comment": "This code defines a class `CLIPVisionCfg` which contains configuration parameters for the CLIP Vision model. These include the number of layers, width of the model, width of the attention heads, scaling ratio for MLP, patch size, image size, initial layer scale value (optional), dropout rate for patches during training, whether to use global average pooling instead of CLS token, and optional drop path rate. The code also checks if Nvidia apex package is installed and suggests installing it with specific options if not already available.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":674-695",
            "content": "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\ntry:\n    from apex.normalization import FusedLayerNorm\nexcept:\n    FusedLayerNorm = LayerNorm\n    print(\"Please build and install Nvidia apex package with option '--cuda_ext' according to https://github.com/NVIDIA/apex#from-source .\")\n@dataclass\nclass CLIPVisionCfg:\n    layers: Union[Tuple[int, int, int, int], int] = 12\n    width: int = 768\n    head_width: int = 64\n    mlp_ratio: float = 4.0\n    patch_size: int = 16\n    image_size: Union[Tuple[int, int], int] = 224\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    patch_dropout: float = 0. # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results\n    global_average_pool: bool = False # whether to global average pool the last embedding layer, instead of using CLS token (https://arxiv.org/abs/2205.01580)\n    drop_path_rate: Optional[float] = None  # drop path rate"
        },
        {
            "comment": "This code defines variables and a function for building a vision tower in the CLIP model. The variables control options such as using a pre-trained timm model, feature pooling methods, projection types, and other model features like fused normalization, cross attention, etc. The function _build_vision_tower builds the vision tower based on the provided embed_dim and vision_cfg parameters. If the vision_cfg has a specified eva_model_name, it uses that for building the tower.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":696-721",
            "content": "    timm_model_name: str = None  # a valid model name overrides layers, width, patch_size\n    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model\n    timm_pool: str = 'avg'  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')\n    timm_proj: str = 'linear'  # linear projection for timm model output ('linear', 'mlp', '')\n    timm_proj_bias: bool = False  # enable bias final projection\n    eva_model_name: str = None # a valid eva model name overrides layers, width, patch_size\n    qkv_bias: bool = True\n    fusedLN: bool = False\n    xattn: bool = False\n    postnorm: bool = False\n    rope: bool = False\n    pt_hw_seq_len: int = 16   # 224/14\n    intp_freq: bool = False\n    naiveswiglu: bool = False\n    subln: bool = False\ndef _build_vision_tower(\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg\n):\n    if isinstance(vision_cfg, dict):\n        vision_cfg = CLIPVisionCfg(**vision_cfg)\n    if vision_cfg.eva_model_name:\n        vision_heads = vision_cfg.width // vision_cfg.head_width"
        },
        {
            "comment": "Creating an EVAVisionTransformer model with specified configuration.\n\nHere, we are initializing an instance of the EVAVisionTransformer class with the given parameters: img_size, patch_size, num_classes, use_mean_pooling, init_values, patch_dropout, embed_dim, depth, num_heads, mlp_ratio, qkv_bias, drop_path_rate, norm_layer, xattn, rope, pt_hw_seq_len, intp_freq, naiveswiglu, and subln.\nThe norm_layer is defined with a partial function that applies FusedLayerNorm if vision_cfg.fusedLN is True, otherwise it applies the default norm_layer.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":722-743",
            "content": "        norm_layer = LayerNorm\n        visual = EVAVisionTransformer(\n            img_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            num_classes=embed_dim,\n            use_mean_pooling=vision_cfg.global_average_pool, #False\n            init_values=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            embed_dim=vision_cfg.width,\n            depth=vision_cfg.layers,\n            num_heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            qkv_bias=vision_cfg.qkv_bias,\n            drop_path_rate=vision_cfg.drop_path_rate,\n            norm_layer= partial(FusedLayerNorm, eps=1e-6) if vision_cfg.fusedLN else partial(norm_layer, eps=1e-6),\n            xattn=vision_cfg.xattn,\n            rope=vision_cfg.rope,\n            postnorm=vision_cfg.postnorm,\n            pt_hw_seq_len= vision_cfg.pt_hw_seq_len,   # 224/14\n            intp_freq= vision_cfg.intp_freq,\n            naiveswiglu= vision_cfg.naiveswiglu,\n            subln= vision_cfg.subln"
        },
        {
            "comment": "The code defines a class `Eva2LargeEncoder` which inherits from `nn.Module`. The class has an initializer that sets up the configuration for the vision transformer model and imports the model using the `_build_vision_tower` function with the provided configuration. In the forward method, it takes an image as input, passes it through the vision transformer model (`self.model`) and returns the encoded features. The returned features are sliced to exclude the first feature, which is typically a classification token or an additional positional embedding.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":744-778",
            "content": "        )\n    return visual\nclass Eva2LargeEncoder(nn.Module):\n    def __init__(self, image_size=224):\n        super(Eva2LargeEncoder, self).__init__()\n        self.config = {\n            \"embed_dim\": 768,\n            \"vision_cfg\": {\n                \"image_size\": 336,\n                \"layers\": 24,\n                \"width\": 1024,\n                \"drop_path_rate\": 0,\n                \"head_width\": 64,\n                \"mlp_ratio\": 2.6667,\n                \"patch_size\": 14,\n                \"eva_model_name\": \"eva-clip-l-14-336\",\n                \"xattn\": True,\n                \"fusedLN\": True,\n                \"rope\": True,\n                \"pt_hw_seq_len\": 16,\n                \"intp_freq\": True,\n                \"naiveswiglu\": True,\n                \"subln\": True\n            }\n        }\n        self.config['vision_cfg']['image_size'] = image_size\n        import os\n        self.model = _build_vision_tower(**self.config)\n    def forward(self, image, **kwargs): # diverge from hf version\n        encode = self.model(image, return_all_features=True)[:, 1:, :]"
        },
        {
            "comment": "Class for Cross Vision Model with Eva2LargeEncoder and position embedding.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/models/eva_clip_L_hf.py\":779-789",
            "content": "        return encode\nclass CrossVisionModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.vit = Eva2LargeEncoder(image_size=config.cross_image_size)\n        self.pos_embed = nn.Parameter(torch.zeros((self.vit.config['vision_cfg']['image_size'] // self.vit.config['vision_cfg']['patch_size']) ** 2, self.vit.config['vision_cfg']['width']))\n    def forward(self, images):\n        enc = self.vit(images)\n        return enc + self.pos_embed.unsqueeze(0)"
        }
    ]
}
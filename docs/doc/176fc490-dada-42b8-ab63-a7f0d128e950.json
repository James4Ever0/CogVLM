{
    "summary": "CogVLM and CogAgent AI models have enhanced visual, dialogue, and Grounding capabilities with HuggingFace training support. CogAgent is open-source, GUI-supported, and utilizes templates for single-round dialogues.",
    "details": [
        {
            "comment": "This code seems to be the introductory part of a README file, which is describing CogVLM and CogAgent. It mentions that there's a Chinese version available, and it provides links for more detailed technical documentation. The code also highlights the release of a new dataset called CogVLM-SFT-311K.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":0-18",
            "content": "# CogVLM & CogAgent\n\ud83d\udcd7 [\u4e2d\u6587\u7248README](./README_zh.md)\n\ud83d\udd25\ud83d\udd25\ud83d\udd25  \ud83c\udd95: ```2023/12/26```: We have released the [CogVLM-SFT-311K](dataset.md) dataset, \nwhich contains over 150,000 pieces of data that we used for **CogVLM v1.0 only** training. \nWelcome to follow and use.\n\ud83c\udf1f **Jump to detailed introduction: [Introduction to CogVLM](#introduction-to-cogvlm)\uff0c\n\ud83c\udd95 [Introduction to CogAgent](#introduction-to-cogagent)**\n\ud83d\udcd4 For more detailed usage information, please refer to: [CogVLM & CogAgent's technical documentation (in Chinese)](https://zhipu-ai.feishu.cn/wiki/LXQIwqo1OiIVTykMh9Lc3w1Fn7g) \n<table>\n  <tr>\n    <td>\n      <h2> CogVLM </h2>\n      <p> \ud83d\udcd6  Paper: <a href=\"https://arxiv.org/abs/2311.03079\">CogVLM: Visual Expert for Pretrained Language Models</a></p>\n      <p><b>CogVLM</b> is a powerful open-source visual language model (VLM). CogVLM-17B has 10 billion visual parameters and 7 billion language parameters, <b>supporting image understanding and multi-turn dialogue with a resolution of 490*490</b>.</p>\n      <p><b>"
        },
        {
            "comment": "\"CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC.\"\n\n\"CogAgent is an open-source visual language model improved based on CogVLM. CogAgent-18B has 11 billion visual parameters and 7 billion language parameters, supporting image understanding at a resolution of 1120*1120. On top of the capabilities of CogVLM, it further possesses GUI image Agent capabilities.\"\n\n\"CogAgent-18B achieves state-of-the-art generalist performance on 9 classic cross-modal benchmarks, including VQAv2, OK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. It significantly surpasses existing models on GUI operation datasets including AITW and Mind2Web.\"",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":18-24",
            "content": "CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks</b>, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC.</p>\n    </td>\n    <td>\n      <h2> CogAgent </h2>\n      <p> \ud83d\udcd6  Paper: <a href=\"https://arxiv.org/abs/2312.08914\">CogAgent: A Visual Language Model for GUI Agents </a></p>\n      <p><b>CogAgent</b> is an open-source visual language model improved based on CogVLM. CogAgent-18B has 11 billion visual parameters and 7 billion language parameters, <b>supporting image understanding at a resolution of 1120*1120</b>. <b>On top of the capabilities of CogVLM, it further possesses GUI image Agent capabilities</b>.</p>\n      <p> <b>CogAgent-18B achieves state-of-the-art generalist performance on 9 classic cross-modal benchmarks</b>, including VQAv2, OK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. <b>It significantly surpasses existing models on GUI operation datasets</b> including AITW and Mind2Web.</p>"
        },
        {
            "comment": "This code appears to be a table of contents for a README file, listing various sections and options related to CogVLM and CogAgent. It provides links and brief descriptions for each section, such as getting started with the web demo or deploying the models by yourself using different scenarios like CLI or web demo. The code also mentions finetuning, hardware requirements, and available model checkpoints.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":25-49",
            "content": "    </td>\n  </tr>\n  <tr>\n    <td colspan=\"2\" align=\"center\">\n      <p>\ud83c\udf10 Web Demo for both CogVLM and CogAgent: <a href=\"http://36.103.203.44:7861\">this link</a></p>\n    </td>\n  </tr>\n</table>\n**Table of Contents**\n- [CogVLM \\& CogAgent](#cogvlm--cogagent)\n    - [Release](#release)\n    - [Get Started](#get-started)\n        - [Option 1: Inference Using Web Demo.](#option-1-inference-using-web-demo)\n        - [Option 2\uff1aDeploy CogVLM / CogAgent by yourself](#option-2deploy-cogvlm--cogagent-by-yourself)\n            - [Situation 2.1 CLI (SAT version)](#situation-21-cli-sat-version)\n            - [Situation 2.2 CLI (Huggingface version)](#situation-22-cli-huggingface-version)\n            - [Situation 2.3 Web Demo](#situation-23-web-demo)\n        - [Option 3\uff1aFinetuning CogAgent / CogVLM](#option-3finetuning-cogagent--cogvlm)\n        - [Option 4: OpenAI Vision format](#option-4-openai-vision-format)\n        - [Hardware requirement](#hardware-requirement)\n        - [Model checkpoints](#model-checkpoints)\n    - [Introduction to CogVLM](#introduction-to-cogvlm)"
        },
        {
            "comment": "Changelog: Release updates, new web UI launched using Streamlit, and the launch of CogAgent.\n```\n\nComment for code:\nNew features added to the CogVLM model, including a web interface and an image understanding model called CogAgent.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":50-66",
            "content": "        - [Examples](#examples)\n    - [Introduction to CogAgent](#introduction-to-cogagent)\n        - [GUI Agent Examples](#gui-agent-examples)\n    - [Cookbook](#cookbook)\n        - [Task Prompts](#task-prompts)\n        - [Which --version to use](#which---version-to-use)\n        - [FAQ](#faq)\n    - [License](#license)\n    - [Citation \\& Acknowledgements](#citation--acknowledgements)\n## Release\n- \ud83d\udd25\ud83d\udd25\ud83d\udd25  **News**: ```2023/12/26```: We have released the [CogVLM-SFT-311K](dataset.md) dataset, \n  which contains over 150,000 pieces of data that we used for **CogVLM v1.0 only** training. Welcome to follow and use.\n- \ud83d\udd25\ud83d\udd25 **News**: ```2023/12/18```: **New Web UI Launched!** We have launched a new web UI based on Streamlit,\n  users can painlessly talk to CogVLM, CogAgent in our UI. Have a better user experience.\n- \ud83d\udd25 **News**: ```2023/12/15```: **CogAgent Officially Launched!** CogAgent is an image understanding model developed\n  based on CogVLM. It features **visual-based GUI Agent capabilities** and has further enhancements in image"
        },
        {
            "comment": "This code provides updates and news about the CogVLM, a versatile AI model that supports image input with 1120*1120 resolution. It has multiple abilities like multi-turn dialogue with images, GUI Agent, Grounding, etc. The code mentions recent updates like image augmentation during training for a more robust checkpoint, support for 4-bit quantization to save GPU memory, and updated versions of chat and VQA models.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":67-80",
            "content": "  understanding. It supports image input with a resolution of 1120*1120, and possesses multiple abilities including\n  multi-turn dialogue with images, GUI Agent, Grounding, and more.\n- **News**: ```2023/12/8``` We have updated the checkpoint of cogvlm-grounding-generalist to\n  cogvlm-grounding-generalist-v1.1, with image augmentation during training, therefore more robust.\n  See [details](#introduction-to-cogvlm).\n- **News**: ```2023/12/7``` CogVLM supports **4-bit quantization** now! You can inference with just **11GB** GPU memory!\n  See [details](#CLI).\n- **News**: ```2023/11/20``` We have updated the checkpoint of cogvlm-chat to cogvlm-chat-v1.1, unified the versions of\n  chat and VQA, and refreshed the SOTA on various datasets. See [details](#introduction-to-cogvlm)\n- **News**: ```2023/11/20``` We release **[cogvlm-chat](https://huggingface.co/THUDM/cogvlm-chat-hf)**, **[cogvlm-grounding-generalist](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)/[base](https://huggingface.co/T"
        },
        {
            "comment": "Code: The code snippet provides instructions for installing dependencies and using CogVLM/CogAgent in various ways. It offers two options - Option 1: Inference Using Web Demo, and Option 2: Deploy CogVLM / CogAgent by yourself. Option 1 directs users to a web demo (http://36.103.203.44:7861/) where they can try out the models without needing to install anything. Option 2 provides guidance on deploying the models using either command-line interface (CLI) or a web demo. The code also mentions that it's easy to modify the CLI scripts for specific use cases in Python code.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":80-104",
            "content": "HUDM/cogvlm-grounding-base-hf)**, **[cogvlm-base-490](https://huggingface.co/THUDM/cogvlm-base-490-hf)/[224](https://huggingface.co/THUDM/cogvlm-base-224-hf)** on \ud83e\udd17Huggingface. you can infer with transformers in [a few lines of code](#situation-22-cli-huggingface-version)now!\n- ```2023/10/27``` CogVLM bilingual version is available [online](https://chatglm.cn/)! Welcome to try it out!\n- ```2023/10/5``` CogVLM-17B released\u3002\n## Get Started\n### Option 1: Inference Using Web Demo.\n* Click here to enter [CogVLM & CogAgent Web Demo](http://36.103.203.44:7861/)\u3002\nIf you need to use Agent and Grounding functions, please refer to [Cookbook - Task Prompts](#task-prompts)\n### Option 2\uff1aDeploy CogVLM / CogAgent by yourself\nWe support two GUIs for model inference, **CLI** and **web demo** . If you want to use it in your python code, it is\neasy to modify the CLI scripts for your case.\n<!-- ### Online Web Demo\nWe provide a [web demo](http://36.103.203.44:7861/) based on [Gradio](https://gradio.app). -->\nFirst, we need to install the dependencies."
        },
        {
            "comment": "This code provides instructions for running a CLI demo using CogAgent and CogVLM models. The user can choose between different model versions, such as 'chat', 'chat_old', or 'base'. The script also supports model parallel inference by specifying `--nproc-per-node=[n]` to split the model across multiple GPUs. To use the demo, the user should navigate to the `basic_demo/` directory and run the provided commands.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":106-133",
            "content": "```bash\n# CUDA >= 11.8\npip install -r requirements.txt\npython -m spacy download en_core_web_sm\n```\n**All code for inference is located under the ``basic_demo/`` directory. Please switch to this directory first before\nproceeding with further operations.**\n#### Situation 2.1 CLI (SAT version)\nRun CLI demo via:\n```bash\n# CogAgent\npython cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogagent-vqa --version chat_old --bf16  --stream_chat\n# CogVLM\npython cli_demo_sat.py --from_pretrained cogvlm-chat --version chat_old --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogvlm-grounding-generalist --version base --bf16  --stream_chat\n```\nThe program will automatically download the sat model and interact in the command line. You can generate replies by\nentering instructions and pressing enter.\nEnter `clear` to clear the conversation history and `stop` to stop the program.\nWe also support model parallel inference, which splits model to multiple (2/4/8) GPUs. `--nproc-per-node=[n]` in the"
        },
        {
            "comment": "The code demonstrates how to run the CogVLM's SAT model for text generation. It uses torchrun command to control the number of GPUs, supports 4-bit and 8-bit quantization, and provides hyperparameters for controlling the generation process such as max_length, top_p, top_k, and temperature.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":134-159",
            "content": "following command controls the number of used GPUs.\n```\ntorchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16\n```\n- If you want to manually download the weights, you can replace the path after ``--from_pretrained`` with the model\n  path.\n- Our model supports SAT's **4-bit quantization** and **8-bit quantization**.\n  You can change ``--bf16`` to ``--fp16``, or ``--fp16 --quant 4``, or ``--fp16 --quant 8``.\n  For example\n    ```bash\n    python cli_demo_sat.py --from_pretrained cogagent-chat --fp16 --quant 8 --stream_chat\n    python cli_demo_sat.py --from_pretrained cogvlm-chat-v1.1 --fp16 --quant 4 --stream_chat\n    # In SAT version\uff0c--quant should be used with --fp16\n    ```\n- The program provides the following hyperparameters to control the generation process:\n    ```\n    usage: cli_demo_sat.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE]\n    optional arguments:\n    -h, --help            show this help message and exit"
        },
        {
            "comment": "This code provides command-line interface (CLI) examples to run demos for CogAgent and CogVLM models using the HuggingFace version. It shows how to use the --from_pretrained option with different model paths, and also mentions optional parameters such as --bf16, --fp16, or --quant 4 for changing the precision of the model.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":160-188",
            "content": "    --max_length MAX_LENGTH\n                            max length of the total sequence\n    --top_p TOP_P         top p for nucleus sampling\n    --top_k TOP_K         top k for top k sampling\n    --temperature TEMPERATURE\n                            temperature for sampling\n    ```\n- Click [here](#which---version-to-use) to view the correspondence between different models and the ``--version``\n  parameter.\n#### Situation 2.2 CLI (Huggingface version)\nRun CLI demo via:\n```bash\n# CogAgent\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-vqa-hf --bf16\n# CogVLM\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-grounding-generalist --bf16\n```\n- If you want to manually download the weights, you can replace the path after ``--from_pretrained`` with the model\n  path.\n- You can change ``--bf16`` to ``--fp16``, or ``--quant 4``. For example, our model supports Huggingface's **4-bit"
        },
        {
            "comment": "This code demonstrates how to finetune CogVLM/CogAgent for a specific task, such as Captcha Recognition using lora.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":189-218",
            "content": "  quantization**:\n    ```bash\n    python cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --quant 4\n    ```\n#### Situation 2.3 Web Demo\nWe also offer a local web demo based on Gradio. First, install Gradio by running: `pip install gradio`. Then download\nand enter this repository and run `web_demo.py`. See the next section for detailed usage:\n```bash\npython web_demo.py --from_pretrained cogagent-chat --version chat --bf16\npython web_demo.py --from_pretrained cogagent-vqa --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-chat-v1.1 --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --bf16\n```\nThe GUI of the web demo looks like:\n<div align=\"center\">\n    <img src=assets/web_demo-min.png width=70% />\n</div>\n### Option 3\uff1aFinetuning CogAgent / CogVLM\nYou may want to use CogVLM in your own task, which needs a **different output style or domain knowledge**. **All code\nfor finetuning is located under the ``finetune_demo/`` directory.**\nWe here provide a finetuning example for **Captcha Recognition** using lora."
        },
        {
            "comment": "1. Download the Captcha Images dataset from Kaggle and extract it.\n2. Split the dataset into train, validation, and test sets in 80/5/15 ratio using `split_dataset.py`.\n3. Fine-tune the model with `finetune_(cogagent/cogvlm)_lora.sh`.\n4. Merge the model to MP_SIZE using `merge_model.py`, replacing 4 with your training MP_SIZE.\n5. Evaluate the model's performance with `evaluate_(cogagent/cogvlm).sh`.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":220-252",
            "content": "1. Start by downloading the [Captcha Images dataset](https://www.kaggle.com/datasets/aadhavvignesh/captcha-images). Once\n   downloaded, extract the contents of the ZIP file.\n2. To create a train/validation/test split in the ratio of 80/5/15, execute the following:\n    ```bash\n    python utils/split_dataset.py\n    ```\n3. Start the fine-tuning process with this command:\n    ```bash\n    bash finetune_demo/finetune_(cogagent/cogvlm)_lora.sh\n    ```\n4. Merge the model to `model_parallel_size=1`: (replace the 4 below with your training `MP_SIZE`)\n    ```bash\n    torchrun --standalone --nnodes=1 --nproc-per-node=4 utils/merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(cogagent/cogvlm490/cogvlm224)\n    ```\n5. Evaluate the performance of your model.\n    ```bash\n    bash finetune_demo/evaluate_(cogagent/cogvlm).sh\n    ```\n### Option 4: OpenAI Vision format\nWe provide the same API examples as `GPT-4V`, which you can view in `openai_demo`.\n1. First, start the node\n```\npython openai_demo/openai_api.py"
        },
        {
            "comment": "This code snippet provides information about running a request example node to generate a description of an image. The output is expected to be similar to the provided text description. Hardware requirements are mentioned for model inference and finetuning, along with the recommended number of GPUs. Additionally, the code mentions that automatic weight downloads can occur when running specific scripts or manual downloads can be done as well.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":253-286",
            "content": "```\n2. Next, run the request example node, which is an example of a continuous dialogue\n```\npython openai_demo/openai_api_request.py\n```\n3. You will get output similar to the following\n```\nThis image showcases a tranquil natural scene with a wooden pathway leading through a field of lush green grass. In the distance, there are trees and some scattered structures, possibly houses or small buildings. The sky is clear with a few scattered clouds, suggesting a bright and sunny day.\n```\n### Hardware requirement\n* Model Inference:\n  For INT4 quantization: 1 * RTX 3090(24G)   (CogAgent takes ~ 12.6GB, CogVLM takes ~ 11GB)\n  For FP16: 1 * A100(80G) or 2 * RTX 3090(24G)\n* Finetuning:\n  For FP16: 4 * A100(80G) *[Recommend]* or 8* RTX 3090(24G).\n### Model checkpoints\nIf you run the `basic_demo/cli_demo*.py` from the code repository, it will automatically download SAT or Hugging Face\nweights. Alternatively, you can choose to manually download the necessary weights.\n- CogAgent\n  |   Model name    | Input resolution |"
        },
        {
            "comment": "This code appears to be a table, specifically a markdown table. It compares several models, categorizing them by their name, input resolution, and providing an introduction and links to the HuggingFace model and SAT (Semantic Analysis Toolkit) model repositories. The table is part of a larger README file for the CogVLM project, likely outlining available models and their features.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":286-294",
            "content": "                             Introduction                             | Huggingface model | SAT model |\n  | :-----------: | :----: | :----------------------------------------------------------: | :------: | :-------: |\n  | cogagent-chat |  1120  | Chat version of CogAgent. Supports GUI Agent, multiple-round  chat and visual grounding. |  [link](https://huggingface.co/THUDM/cogagent-chat-hf)       |    [link](https://huggingface.co/THUDM/CogAgent/tree/main)       |\n  | cogagent-vqa |  1120  | VQA version of CogAgent. Has stronger capabilities in single-turn visual dialogue. Recommended for VQA benchmarks. |  [link](https://huggingface.co/THUDM/cogagent-vqa-hf)       |    [link](https://huggingface.co/THUDM/CogAgent/tree/main)       |\n- CogVLM\n  |          Model name           | Input resolution |                           Introduction                            | Huggingface model | SAT model |\n  | :-------------------------: | :----: | :-------------------------------------------------------: | :------: | :-------: |"
        },
        {
            "comment": "This code provides information about different versions of the CogVLM model available on Hugging Face. It lists their names, sizes, main features, and links to their respective pages for more information and accessing the models.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":295-298",
            "content": "  |         cogvlm-chat-v1.1         |  490   |  Supports multiple rounds of chat and vqa simultaneously, with different prompts.   |  [link](https://huggingface.co/THUDM/cogvlm-chat-hf)        |    [link](https://huggingface.co/THUDM/CogVLM/tree/main)        |\n  |       cogvlm-base-224       |  224   |               The original checkpoint after text-image pretraining.               |   [link](https://huggingface.co/THUDM/cogvlm-base-224-hf)       |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |\n  |       cogvlm-base-490       |  490   |      Amplify the resolution to 490 through position encoding interpolation from `cogvlm-base-224`.      |   [link](https://huggingface.co/THUDM/cogvlm-base-490-hf)       |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |\n  | cogvlm-grounding-generalist |  490   | This checkpoint supports different visual grounding tasks, e.g. REC, Grounding Captioning, etc.  |    [link](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)      |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |"
        },
        {
            "comment": "CogVLM is an open-source, powerful VLM with 10B vision and 7B language parameters. It achieves state-of-the-art performance on 10 benchmarks and surpasses/matches PaLI-X 55B.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":300-333",
            "content": "## Introduction to CogVLM\n- CogVLM is a powerful **open-source visual language model** (**VLM**). CogVLM-17B has 10 billion vision parameters and\n  7 billion language parameters.\n- CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k\n  captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and rank the 2nd on VQAv2,\n  OKVQA, TextVQA, COCO captioning, etc., **surpassing or matching PaLI-X 55B**. CogVLM can\n  also [chat with you](http://36.103.203.44:7861) about images.\n<div align=\"center\">\n    <img src=assets/metrics-min.png width=50% />\n</div>\n<details>\n<summary>Click to view results on MM-VET, POPE, TouchStone. </summary>\n<table>\n    <tr>\n        <td>Method</td>\n        <td>LLM</td>\n        <td>MM-VET</td>\n        <td>POPE(adversarial)</td>\n        <td>TouchStone</td>\n    </tr>\n    <tr>\n        <td>BLIP-2</td>\n        <td>Vicuna-13B</td>\n        <td>22.4</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Otter</td>"
        },
        {
            "comment": "This code snippet represents a table with different language models and their corresponding performance metrics. Each row represents a model with columns for the name, base model, inference speed (perplexity), image captioning accuracy (CIDEr), and video question answering accuracy (QA F1). The table provides a comparison of various language models based on these performance metrics.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":334-387",
            "content": "        <td>MPT-7B</td>\n        <td>24.7</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>MiniGPT4</td>\n        <td>Vicuna-13B</td>\n        <td>24.4</td>\n        <td>70.4</td>\n        <td>531.7</td>\n    </tr>\n    <tr>\n        <td>InstructBLIP</td>\n        <td>Vicuna-13B</td>\n        <td>25.6</td>\n        <td>77.3</td>\n        <td>552.4</td>\n    </tr>\n    <tr>\n        <td>LLaMA-Adapter v2</td>\n        <td>LLaMA-7B</td>\n        <td>31.4</td>\n        <td>-</td>\n        <td>590.1</td>\n    </tr>\n    <tr>\n        <td>LLaVA</td>\n        <td>LLaMA2-7B</td>\n        <td>28.1</td>\n        <td>66.3</td>\n        <td>602.7</td>\n    </tr>\n    <tr>\n        <td>mPLUG-Owl</td>\n        <td>LLaMA-7B</td>\n        <td>-</td>\n        <td>66.8</td>\n        <td>605.4</td>\n    </tr>\n    <tr>\n        <td>LLaVA-1.5</td>\n        <td>Vicuna-13B</td>\n        <td>36.3</td>\n        <td>84.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Emu</td>\n        <td>LLaMA-13B</td>\n        <td>36.3</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>"
        },
        {
            "comment": "This code is displaying the results of various language models in a table format, including Qwen-VL-Chat, DreamLLM, and CogVLM. The table shows performance metrics like accuracy and speed for different tasks and datasets. The summary sections allow users to view specific results by clicking on them.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":388-442",
            "content": "    <tr>\n        <td>Qwen-VL-Chat</td>\n        <td>-</td>\n        <td>-</td>\n        <td>-</td>\n        <td>645.2</td>\n    </tr>\n    <tr>\n        <td>DreamLLM</td>\n        <td>Vicuna-7B</td>\n        <td>35.9</td>\n        <td>76.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>CogVLM</td>\n        <td>Vicuna-7B</td>\n        <td> <b>52.8</b> </td>\n        <td><b>87.6</b></td>\n        <td><b>742.0</b></td>\n    </tr>\n</table>\n</details>\n<details>\n<summary>Click to view results of cogvlm-grounding-generalist-v1.1. </summary>\n<table>\n    <tr>\n        <td></td>\n        <td>RefCOCO</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCO+</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCOg</td>\n        <td></td>\n        <td>Visual7W</td>\n    </tr>\n    <tr>\n        <td></td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>test</td>\n        <td>test</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist</td>"
        },
        {
            "comment": "This code is displaying a table comparing the performance of different models in various tasks. The model 'cogvim-grounding-generalist-v1.1' has the highest accuracy in most categories.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":443-472",
            "content": "        <td>92.51</td>\n        <td>93.95</td>\n        <td>88.73</td>\n        <td>87.52</td>\n        <td>91.81</td>\n        <td>81.43</td>\n        <td>89.46</td>\n        <td>90.09</td>\n        <td>90.96</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist-v1.1</td>\n        <td>**92.76**</td>\n        <td>**94.75**</td>\n        <td>**88.99**</td>\n        <td>**88.68**</td>\n        <td>**92.91**</td>\n        <td>**83.39**</td>\n        <td>**89.75**</td>\n        <td>**90.79**</td>\n        <td>**91.05**</td>\n    </tr>\n</table>\n</details>\n### Examples\n<!-- CogVLM is powerful for answering various types of visual questions, including **Detailed Description & Visual Question Answering**,  **Complex Counting**, **Visual Math Problem Solving**, **OCR-Free Reasonging**, **OCR-Free Visual Question Answering**, **World Knowledge**, **Referring Expression Comprehension**, **Programming with Visual Input**, **Grounding with Caption**, **Grounding Visual Question Answering**, etc. -->\n* CogVLM can accurately describe images in details with **very few hallucinations**."
        },
        {
            "comment": "CogVLM can understand and answer various types of questions with a visual grounding version. It sometimes captures more detailed content than GPT-4V. CogAgent is an open-source visual language model improved based on CogVLM, achieving state-of-the-art generalist performance on 9 classic cross-modal benchmarks.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":473-511",
            "content": "    <details>\n    <summary>Click for comparison with LLAVA-1.5 and MiniGPT-4.</summary>\n    <img src=assets/llava-comparison-min.png width=50% />\n    </details>\n    <br>\n* CogVLM can understand and answer various types of questions, and has a **visual grounding** version.\n<div align=\"center\">\n    <img src=assets/pear_grounding.png width=50% />\n</div>\n<br>\n* CogVLM sometimes captures more detailed content than GPT-4V(ision).\n<div align=\"center\">\n    <img src=assets/compare-min.png width=50% />\n</div>\n<!-- ![compare](assets/compare.png) -->\n<br> \n<details>\n<summary>Click to expand more examples.</summary>\n![Chat Examples](assets/chat.png)\n</details>\n## Introduction to CogAgent\nCogAgent is an open-source visual language model improved based on CogVLM. CogAgent-18B has 11 billion visual parameters\nand 7 billion language parameters\nCogAgent-18B achieves state-of-the-art generalist performance on 9 classic cross-modal benchmarks, including VQAv2,\nOK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. It significantly surpasses existing models on GUI"
        },
        {
            "comment": "This code describes CogAgent, an advanced AI agent with enhanced capabilities in visual input, dialogue question-answering, GUI-related tasks, and OCR-related tasks. It supports ultra-high-resolution image inputs of 1120x1120 and can handle various GUI screenshots.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":512-541",
            "content": "operation datasets such as AITW and Mind2Web.\nIn addition to all the features already present in CogVLM (visual multi-round dialogue, visual grounding), CogAgent:\n1. Supports higher resolution visual input and dialogue question-answering. **It supports ultra-high-resolution image\n   inputs of 1120x1120.**\n2. **Possesses the capabilities of a visual Agent**, being able to return a plan, next action, and specific operations\n   with coordinates for any given task on any GUI screenshot.\n3. **Enhanced GUI-related question-answering capabilities**, allowing it to handle questions about any GUI screenshot,\n   such as web pages, PC apps, mobile applications, etc.\n4. Enhanced capabilities in OCR-related tasks through improved pre-training and fine-tuning.\n<div align=\"center\">\n    <img src=assets/cogagent_function.jpg width=60% />\n</div>\n### GUI Agent Examples\n<div align=\"center\">\n    <img src=assets/cogagent_main_demo.jpg width=90% />\n</div>\n## Cookbook\n### Task Prompts\n1. **General Multi-Round Dialogue**: Say whatever you want."
        },
        {
            "comment": "This code describes the process of using a pre-defined template to ask CogAgent questions about how to complete a specific task. The user selects a template, replaces <TASK> with the desired action enclosed in double quotes, and then asks the model for advice on completing the task. If adding \"(with grounding)\", the model will provide an action representation with coordinates.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":543-559",
            "content": "2. **GUI Agent Task**: Use the [Agent template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761)\n   and replace \\<TASK\\> with the task instruction enclosed in double quotes. This query can make CogAgent infer Plan and\n   Next Action. If adding ``(with grounding)`` at the end of the query, the model will return a formalized action\n   representation with coordinates.\nFor example, to ask the model how to complete the task \"Search for CogVLM\" on a current GUI screenshot, follow these\nsteps:\n1. Randomly select a template from\n   the [Agent template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761). Here, we\n   choose ``What steps do I need to take to <TASK>?``.\n2. Replace <TASK> with the task instruction enclosed in double quotes, for\n   example, ``What steps do I need to take to \"Search for CogVLM\"?`` . Inputting this to the model yields:\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources."
        },
        {
            "comment": "Code snippet explains how to search for \"CogVLM\" on Google and provides a plan to complete this task. The next action is to type 'CogVLM' into the Google search bar. Grounded Operation shows the coordinates of where to type in the search bar ([[212,498,787,564]]). It also mentions that GUI Agent tasks are recommended to be conducted with single-round dialogues for better results and introduces three modes of grounding: image description with grounding coordinates.\n\nCode snippet is from \"CogVLM/README.md\":560-578, in the section about grounded operation and visual grounding.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":560-578",
            "content": ">\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n3. If adding ``(with grounding)`` at the end, i.e. changing the input\n   to ``What steps do I need to take to \"Search for CogVLM\"?(with grounding)``, the output of CogAgent would be:\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.\n>\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n> Grounded Operation:[combobox] Search -> TYPE: CogVLM at the box [[212,498,787,564]]\nTip: For GUI Agent tasks, it is recommended to conduct only single-round dialogues for each image for better results.\n3. **Visual Grounding**. Three modes of grounding are supported:\n    - Image description with grounding coordinates (bounding box). Use any template\n      from [caption_with_box template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L537) as model\n      input. For example:"
        },
        {
            "comment": "This code is related to image grounding, where the model receives a description of objects in an image and returns the bounding box coordinates for those objects. It uses templates from the caption2box and box2caption classes for this purpose. The coordinates are in the format [[x1, y1, x2, y2]], with the origin at the top left corner and the x-axis going right and the y-axis down.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":580-595",
            "content": "   > Can you provide a description of the image and include the coordinates [[x0,y0,x1,y1]] for each mentioned object?\n    - Returning grounding coordinates (bounding box) based on the description of objects. Use any template\n      from [caption2box template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L345),\n      replacing ``<expr>`` with the object's description. For example:\n   > Can you point out *children in blue T-shirts* in the image and provide the bounding boxes of their location?\n    - Providing a description based on bounding box coordinates. Use a template\n      from [box2caption template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L400),\n      replacing ``<objs>`` with the position coordinates. For example:\n   > Tell me what you see within the designated area *[[086,540,400,760]]* in the picture.\n**Format of coordination:** The bounding box coordinates in the model's input and output use the\nformat ``[[x1, y1, x2, y2]]``, with the origin at the top left corner, the x-axis to the right, and the y-axis"
        },
        {
            "comment": "This code snippet provides information about different versions of models and their corresponding --version specifications for text processor. It also mentions a possible solution for trouble in accessing huggingface.co and automatically downloading models using SAT.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":596-618",
            "content": "downward. (x1, y1) and (x2, y2) are the top-left and bottom-right corners, respectively, with values as relative\ncoordinates multiplied by 1000 (prefixed with zeros to three digits).\n### Which --version to use\nDue to differences in model functionalities, different model versions may have distinct ``--version`` specifications for\nthe text processor, meaning the format of the prompts used varies.\n|         model name          | --version |\n|:---------------------------:|:---------:|\n|        cogagent-chat        |   chat    |\n|        cogagent-vqa         | chat_old  |\n|         cogvlm-chat         | chat_old  |\n|      cogvlm-chat-v1.1       | chat_old  |\n| cogvlm-grounding-generalist |   base    |\n|       cogvlm-base-224       |   base    |\n|       cogvlm-base-490       |   base    |\n### FAQ\n* If you have trouble in accessing huggingface.co, you can add `--local_tokenizer /path/to/vicuna-7b-v1.5` to load the\n  tokenizer.\n* If you have trouble in automatically downloading model with \ud83d\udd28[SAT](https://github.com/THUDM/SwissArmyTransformer), try"
        },
        {
            "comment": "Downloading CogVLM model from ModelScope, HuggingFace, or Wisemodel. Use SAT for manual download and save to default location ~/.sat_models. Change SAT_HOME environment variable for custom locations. Code is open source under Apache-2.0 license, while Model License applies to CogVLM weights usage. Consider citing \"CogVLM: Visual Expert for Pretrained Language Models\" when helpful.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":619-639",
            "content": "  downloading from \ud83e\udd16[modelscope](https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary) or\n  \ud83e\udd17[huggingface](https://huggingface.co/THUDM/CogVLM) or \ud83d\udca1[wisemodel](https://www.wisemodel.cn/models/ZhipuAI/CogVLM)\n  manually.\n* Download model using \ud83d\udd28[SAT](https://github.com/THUDM/SwissArmyTransformer), the model will be saved to the default\n  location `~/.sat_models`. Change the default location by setting the environment variable `SAT_HOME`. For example, if\n  you want to save the model to `/path/to/my/models`, you can run `export SAT_HOME=/path/to/my/models` before running\n  the python command.\n## License\nThe code in this repository is open source under the [Apache-2.0 license](./LICENSE), while the use of the CogVLM model\nweights must comply with the [Model License](./MODEL_LICENSE).\n## Citation & Acknowledgements\nIf you find our work helpful, please consider citing the following papers\n```\n@misc{wang2023cogvlm,\n      title={CogVLM: Visual Expert for Pretrained Language Models}, \n      author={Weihan "
        },
        {
            "comment": "This code appears to be providing citation information for two research papers. The first paper, \"CogAgent: A Visual Language Model for GUI Agents,\" is authored by Wenyi Hong and others, with a year of 2023 and an arXiv eprint number of 2312.08914. The second paper, \"CogVLM/README.md\", also includes English image-text data from several sources: MiniGPT-4, LLAVA, LRV-Instruction, LLaVAR, and Shikr.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":639-659",
            "content": "Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2311.03079},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n@misc{hong2023cogagent,\n      title={CogAgent: A Visual Language Model for GUI Agents}, \n      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2312.08914},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\nIn the instruction fine-tuning phase of the CogVLM, there are some English image-text data from\nthe [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [LLAVA](https://github.com/haotian-liu/LLaVA), [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction), [LLaVAR](https://github.com/SALT-NLP/LLaVAR)\nand [Shikr"
        },
        {
            "comment": "This code is acknowledging the contributions of various projects and datasets to the CogVLM, expressing gratitude.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/README.md\":659-660",
            "content": "a](https://github.com/shikras/shikra) projects, as well as many classic cross-modal work datasets. We\nsincerely thank them for their contributions."
        }
    ]
}
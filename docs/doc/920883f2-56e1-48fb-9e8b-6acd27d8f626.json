{
    "summary": "The code prepares input data for language models by tokenizing, preprocessing, and adhering to length constraints, creating masks, and converting inputs to PyTorch tensors. It also defines several functions for handling image captions and other tasks.",
    "details": [
        {
            "comment": "This code defines several functions that convert chat and VQA histories into prompts for the language model. The prompt format includes old queries, responses, and new queries, depending on the history type.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/language.py\":0-27",
            "content": "from sat.model.official.llama_model import LLaMAModel, rotate_half\nfrom sat.transformer_defaults import attention_fn_default, split_tensor_along_last_dim\nimport torch.nn.functional as F\ndef base_history_to_prompt(self, query, history):\n    prompt = '<EOI>' + query\n    return prompt\ndef chat_history_to_prompt(self, query, history):\n    prompt = \"<EOI> [INST] \"\n    for i, (old_query, response) in enumerate(history):\n        prompt += old_query + \" [/INST] \" + response + \" [INST] \"\n    prompt += query + \" [/INST] \"\n    return prompt\ndef vqa_history_to_prompt(self, query, history):\n    # Only support single round chat in vqa mode\n    prompt = \"<EOI>Question: \"\n    # for i, (old_query, response) in enumerate(history):\n    #     prompt += old_query + \" Short answer: \" + response + \" Question: \"\n    prompt += query + \" Short answer:\"\n    return prompt\ndef chat_old_history_to_prompt(self, query, history):\n    prompt = \"<EOI>Question: \"\n    for i, (old_query, response) in enumerate(history):\n        prompt += old_query + \" Answer: \" + response + \"\\nQuestion: \""
        },
        {
            "comment": "The code defines a function `llama2_tokenizer` that takes a tokenizer path and a signal type as input and returns a LlamaTokenizer object. It also defines a class `llama2_text_processor` that takes a tokenizer, maximum target length, and image length as input and processes text inputs (caption and prompt). The `llama2_text_processor` class has a `__call__` method that can be called with a caption and optional prompt to process the text.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/language.py\":28-62",
            "content": "    prompt += query + \" Answer:\"\n    return prompt\n_history_to_prompt = {\n    \"base\": base_history_to_prompt,\n    \"chat\": chat_history_to_prompt,\n    \"vqa\": vqa_history_to_prompt,\n    \"chat_old\": chat_old_history_to_prompt, # for cogvlm-v1.1\n}\nfrom transformers import LlamaTokenizer\ndef llama2_tokenizer(tokenizer_path, signal_type=\"base\"):\n    tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = 32000\n    tokenizer.boi = \"[IMG]\"\n    tokenizer.eoi = \"[/IMG]\"\n    assert signal_type in [\"base\", \"chat\", \"vqa\", \"chat_old\"]\n    tokenizer.signal_type = signal_type\n    return tokenizer\nimport re\nimport numpy as np\nimport torch\nclass llama2_text_processor:\n    def __init__(self, tokenizer, max_target_length=2048, image_length=257, model=None):\n        self.tokenizer = tokenizer\n        self.max_target_length = max_target_length\n        self.image_length = image_length\n    def __call__(self, caption, prompt=\"\"):\n        if '<EOI>' not in prompt:\n            prompt = self.replace_tags_with_empty(prompt)"
        },
        {
            "comment": "The code is preparing input for the language model by splitting prompts and captions, encoding them, and building the input_ids list to be passed to the model. It also checks if the total length of the context exceeds the maximum target length, returning None if it does.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/language.py\":63-82",
            "content": "            # caption = self.replace_tags_with_empty(caption)\n            history = []\n            prompt = self.history_to_prompt(prompt, history)\n        input_ids = [self.tokenizer.bos_token_id]\n        prompt_splits = prompt.split('<EOI>')\n        caption_splits = caption.split('<EOI>')\n        if len(prompt_splits) > 0:\n            input_ids.extend(self.tokenizer.encode(prompt_splits[0], add_special_tokens=False))\n        for tokens in prompt_splits[1:]:\n            tokens_with_img = [-100] + self.tokenizer.encode(tokens, add_special_tokens=False)\n            input_ids.extend(tokens_with_img)\n        context_length = len(input_ids) + (len(prompt_splits)-1) * (self.image_length + 1)\n        if context_length > self.max_target_length - 10:\n            return None\n        if len(caption_splits) > 0:\n            input_ids.extend(self.tokenizer.encode(caption_splits[0], add_special_tokens=False))\n        for tokens in caption_splits[1:]:\n            tokens_with_img = [-100] + self.tokenizer.encode(tokens, add_special_tokens=False)"
        },
        {
            "comment": "The code preprocesses an input sequence, extends it with image tokens and ensures that the maximum length constraint is not violated. It also sets masks for embedding, vision expert, and image region of interest.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/language.py\":83-106",
            "content": "            input_ids.extend(tokens_with_img)\n        if len(input_ids) > self.max_target_length - self.image_length - 5:\n            input_ids = input_ids[:self.max_target_length - self.image_length - 5]\n        input_ids += [self.tokenizer.eos_token_id]\n        while -100 in input_ids:\n            img_idx = input_ids.index(-100)\n            input_ids = input_ids[:img_idx] + [0] * (self.image_length + 1) + [-1] + input_ids[img_idx+1:]\n        image_position = []\n        while -1 in input_ids:\n            img_idx = input_ids.index(-1)\n            input_ids[img_idx] = 0\n            image_position.append(img_idx)\n        image_embed_mask = [0] * len(input_ids)\n        vision_expert_mask = [0] * len(input_ids)\n        image_rope_mask = [0] * len(input_ids)\n        for idx in image_position:\n            image_embed_mask[idx-self.image_length-1: idx+1] = [1] * (self.image_length + 2)\n            vision_expert_mask[idx-self.image_length-1: idx] = [1] * (self.image_length + 1)\n            image_rope_mask[idx - self.image_length: idx] = [1] * self.image_length"
        },
        {
            "comment": "This code is prepping input data for a model. It pads the input list, creates attention and mask arrays, and labels. It also handles image positioning and creates position_ids for further processing.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/language.py\":107-129",
            "content": "        attention_mask = [1] * len(input_ids)\n        labels = [-100] * context_length + input_ids[context_length:]\n        pad_len = self.max_target_length - len(input_ids)\n        input_ids = input_ids + [self.tokenizer.pad_token_id] * pad_len\n        attention_mask = attention_mask + [1] * pad_len\n        vision_expert_mask = vision_expert_mask + [0] * pad_len\n        image_embed_mask = image_embed_mask + [0] * pad_len\n        image_rope_mask = image_rope_mask + [0] * pad_len\n        np_mask = np.tril(np.expand_dims(np.array(attention_mask), 0).repeat(len(attention_mask), 0))\n        labels = labels + [-100] * pad_len\n        for idx in image_position:\n            labels[idx-self.image_length-1: idx+1] = [-100] * (self.image_length + 2)\n        position_ids = []\n        pid = -1\n        for i in range(len(input_ids)):\n            if image_rope_mask[i] == 0 or (i > 0 and image_rope_mask[i] != image_rope_mask[i - 1]):\n                pid += 1\n            position_ids.append(pid)\n        input_ids = torch.tensor(input_ids).unsqueeze(0)"
        },
        {
            "comment": "This code is defining functions for creating and converting data structures related to text processing. It involves tensors, masks, and positioning of text elements in a model's input. The code also includes functions to convert history into prompts and replace certain tags with empty values in the text.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/language.py\":130-145",
            "content": "        labels = torch.tensor(labels).unsqueeze(0)\n        attention_mask = torch.from_numpy(np_mask).unsqueeze(0).unsqueeze(0)\n        image_embed_mask = torch.tensor(image_embed_mask).unsqueeze(0)\n        vision_expert_mask = torch.tensor(vision_expert_mask).unsqueeze(0)\n        image_rope_mask = torch.tensor(image_rope_mask).unsqueeze(0)\n        position_ids = torch.tensor(position_ids).unsqueeze(0)\n        context_length = torch.tensor(context_length).unsqueeze(0).long()\n        return {'input_ids': input_ids, 'labels': labels, 'position_ids': position_ids, 'attention_mask': attention_mask, 'image_embed_mask': image_embed_mask,\n                'context_length': context_length, 'image_position': image_position, 'vision_expert_mask': vision_expert_mask, 'image_rope_mask': image_rope_mask\n                }\n    def history_to_prompt(self, query, history):\n        return _history_to_prompt[self.tokenizer.signal_type](self, query, history)\n    def replace_tags_with_empty(self, text):\n        return re.sub('<pad>|<s>|</s>|<EOI>', '', text)"
        },
        {
            "comment": "This function creates masks and position IDs for a given sequence and image logits mask. It returns the tokens, attention mask, and position IDs as output.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/language.py\":147-172",
            "content": "from functools import partial\ndef get_masks_and_position_ids(seq, image_logits_mask):\n    tokens = seq.unsqueeze(0)\n    attention_mask = torch.ones((1, len(seq), len(seq)), device=tokens.device)\n    attention_mask.tril_()\n    attention_mask.unsqueeze_(1)\n    position_ids = []\n    pid = -1\n    for i in range(len(image_logits_mask[0])):\n        if image_logits_mask[0][i] == 0 or (i > 0 and image_logits_mask[0][i] != image_logits_mask[0][i - 1]):\n            pid += 1\n        position_ids.append(pid)\n    for i in range(tokens.shape[1]-image_logits_mask.shape[1]):\n        pid += 1\n        position_ids.append(pid)\n    position_ids = torch.tensor(position_ids, dtype=torch.long, device=tokens.device)\n    position_ids = position_ids.unsqueeze(0)\n    return tokens, attention_mask, position_ids\nclass llama2_text_processor_inference:\n    def __init__(self, tokenizer, max_target_length=1024, image_length=257, model=None, no_prompt=False, english=True):\n        self.tokenizer = tokenizer\n        self.max_target_length = max_target_length"
        },
        {
            "comment": "This function is setting up an object that will tokenize text input with image captions. It determines the separator based on the signal type, and initializes some attributes for handling images and end-of-input (<EOI>) tokens. If <EOI> is not found in the prompt, it replaces tags with empty strings, adds history to the prompt, encodes the prompt into input_ids, and handles each token split after <EOI>.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/language.py\":173-199",
            "content": "        self.image_length = image_length\n        if self.tokenizer.signal_type == \"chat\":\n            self.sep = \"[/INST]\"\n        elif self.tokenizer.signal_type == \"vqa\":\n            self.sep = \" Short answer:\"\n        elif self.tokenizer.signal_type == \"chat_old\":\n            self.sep = \" Answer:\"\n        else:\n            self.sep = \"<unk>\"\n        self.invalid_slices = []\n        self.no_eoi = True\n    def __call__(self, prompt=\"\"):\n        if '<EOI>' not in prompt:\n            prompt = self.replace_tags_with_empty(prompt)\n            # caption = self.replace_tags_with_empty(caption)\n            history = []\n            prompt = self.history_to_prompt(prompt, history)\n        input_ids = [self.tokenizer.bos_token_id]\n        prompt_splits = prompt.split('<EOI>')\n        if len(prompt_splits) > 0:\n            input_ids.extend(self.tokenizer.encode(prompt_splits[0], add_special_tokens=False))\n        for tokens in prompt_splits[1:]:\n            tokens_with_img = [-100] + self.tokenizer.encode(tokens, add_special_tokens=False)"
        },
        {
            "comment": "This code is preparing the input for a language model. It replaces image tokens (-100) with padding (-1), creates masks for image embeddings, and converts them into PyTorch tensors.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/language.py\":200-222",
            "content": "            input_ids.extend(tokens_with_img)\n        while -100 in input_ids:\n            img_idx = input_ids.index(-100)\n            input_ids = input_ids[:img_idx] + [0] * (self.image_length + 1) + [-1] + input_ids[img_idx + 1:]\n        image_position = []\n        while -1 in input_ids:\n            img_idx = input_ids.index(-1)\n            input_ids[img_idx] = 0\n            image_position.append(img_idx)\n        image_embed_mask = [0] * len(input_ids)\n        vision_expert_mask = [0] * len(input_ids)\n        image_rope_mask = [0] * len(input_ids)\n        for idx in image_position:\n            image_embed_mask[idx - self.image_length - 1: idx + 1] = [1] * (self.image_length + 2)\n            vision_expert_mask[idx - self.image_length - 1: idx] = [1] * (self.image_length + 1)\n            image_rope_mask[idx - self.image_length: idx] = [1] * self.image_length\n        input_ids = torch.tensor(input_ids).unsqueeze(0)\n        image_embed_mask = torch.tensor(image_embed_mask).unsqueeze(0)\n        vision_expert_mask = torch.tensor(vision_expert_mask).unsqueeze(0)"
        },
        {
            "comment": "The code defines several functions. 'image_rope_mask' function creates a tensor from image_rope_mask and returns a dictionary containing input_ids, image_embed_mask, vision_expert_mask, and image_rope_mask. The 'history_to_prompt' function takes query and history as inputs and returns the appropriate type based on tokenizer.signal_type. 'replace_tags_with_empty' replaces specific tags with an empty string in the text. 'process_response' removes the tag Industriously from response. 'get_func' defines a partial function get_masks_and_position_ids, using image_rope_mask as an argument.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/language.py\":223-237",
            "content": "        image_rope_mask = torch.tensor(image_rope_mask).unsqueeze(0)\n        return {'input_ids': input_ids, 'image_embed_mask': image_embed_mask, 'vision_expert_mask': vision_expert_mask, 'image_rope_mask': image_rope_mask}\n    def history_to_prompt(self, query, history):\n        return _history_to_prompt[self.tokenizer.signal_type](self, query, history)\n    def replace_tags_with_empty(self, text):\n        return re.sub('<pad>|<s>|</s>|<EOI>', '', text)\n    def process_response(self, response):\n        return response.replace('</s>', '')\n    def get_func(self, inputs, **kwargs):\n        get_func = partial(get_masks_and_position_ids, image_logits_mask=kwargs['image_rope_mask'])\n        return get_func"
        }
    ]
}
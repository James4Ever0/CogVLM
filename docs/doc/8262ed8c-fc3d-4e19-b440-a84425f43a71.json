{
    "summary": "The code imports modules, prepares data, defines functions for autoregressive sampling and model predictions, calculates accuracy, creates a dataset, handles command-line arguments, loads pretrained models, sets tokenizer path, enables GPU usage if available, and creates image and text processors for model evaluation.",
    "details": [
        {
            "comment": "The code imports necessary modules, sets up data collation function for fine-tuning a CogVLM model, and defines a collator that prepares examples for training or evaluation.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm_demo.py\":0-26",
            "content": "import os\nimport torch\nimport argparse\nfrom functools import partial\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom sat import mpu, get_args, get_tokenizer\nfrom sat.training.deepspeed_training import training_main\nfrom sat.helpers import print_rank0\nfrom utils.models import FineTuneTestCogVLMModel\nfrom utils.utils import llama2_text_processor, llama2_text_processor_inference, get_image_processor\ndef data_collator(examples):\n    examples = [ex for ex in examples if len(ex) > 0] # drop {}\n    for example in examples:\n        for k in example:\n            if isinstance(example[k], list):\n                example[k] = torch.tensor(example[k])\n            elif isinstance(example[k], np.ndarray):\n                example[k] = torch.from_numpy(example[k])\n    img_args = {}\n    tmp_example = examples[0]\n    for k in tmp_example['vision']:\n        if type(tmp_example['vision'][k]) is torch.Tensor:\n            img_args['vision_'+k] = torch.cat([example['vision'][k] for example in examples])"
        },
        {
            "comment": "The code is preparing data for a machine learning model by extracting relevant information from examples and broadcasting it to the correct format.\n\n1. It first checks if the example has \"vision\" and \"cross\" keys, then removes them.\n2. It creates a dictionary of image-related keys as 'vision_' + key for each example.\n3. It combines all image tensors from each example into one tensor and stores non-tensor data in their original form.\n4. The resulting dictionary contains the data prepared for model training or evaluation.\n5. A function `broadcast_auto` is defined to help with broadcasting data types, but it is not used in this code block.\n6. Another function `get_batch` is called, which uses the previously mentioned functions to prepare the data batch.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm_demo.py\":27-62",
            "content": "        else:\n            img_args['vision_'+k] = example['vision'][k]\n    for example in examples:\n        example.pop('vision')\n        if 'cross' in example:\n            example.pop('cross')\n    model_args = {}\n    tmp_example = examples[0]\n    for k in tmp_example:\n        if type(tmp_example[k]) is torch.Tensor:\n            model_args[k] = torch.cat([example[k] for example in examples])\n        else:\n            model_args[k] = tmp_example[k]\n    model_args.update(img_args)\n    return model_args\nfrom collections import defaultdict\ndef broadcast_auto(data_dict):\n    type2list = defaultdict(list)\n    other = []\n    for k in data_dict:\n        if type(data_dict[k]) is torch.Tensor:\n            type2list[data_dict[k].dtype].append(k)\n        else:\n            other.append(k)\n    new_data = {}\n    for k in type2list:\n        new_data.update(mpu.broadcast_data(type2list[k], data_dict, k))\n    for k in other:\n        new_data[k] = data_dict[k]\n    return new_data\ndef get_batch(data_iterator, args, timers):\n    # Broadcast data."
        },
        {
            "comment": "Function `chat` takes a model, tokenizer, tokens (input text), and optional arguments for autoregressive sampling such as maximum length and number of beams. It converts the input text to device-compatible format, extends it with -1 values to reach desired length, then performs autoregressive sampling using the provided model and sampling parameters.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm_demo.py\":63-90",
            "content": "    timers('data loader').start()\n    if data_iterator is not None:\n        data = next(data_iterator)\n    else:\n        data = None\n    timers('data loader').stop()\n    data_b = broadcast_auto(data)\n    for k in data_b:\n        if type(data_b[k]) is torch.Tensor and data_b[k].dtype is not torch.int32 and data_b[k].dtype is not torch.long:\n            if args.fp16:\n                data_b[k] = data_b[k].half()\n            elif args.bf16:\n                data_b[k] = data_b[k].bfloat16()\n    return data_b\nfrom torch.nn import CrossEntropyLoss\nimport numpy as np\nfrom sat.model.mixins import CachedAutoregressiveMixin\nfrom sat.generation.autoregressive_sampling import filling_sequence\nfrom sat.generation.sampling_strategies import BaseStrategy, BeamSearchStrategy\ndef chat(model, tokenizer, tokens,\n         max_length: int = 1800, num_beams=5, top_p=0.95, top_k=0, temperature=0.8, **kwargs):\n    inputs = tokens.to(model.parameters().__next__().device)[0]\n    seq = torch.cat(\n        [inputs, torch.tensor([-1] * (max_length - len(inputs)), device=inputs.device)], dim=0"
        },
        {
            "comment": "This code snippet defines a function `filling_sequence` that takes in a model, sequence, batch size, strategy, and additional kwargs. It returns the output of the model's prediction on the given sequence. The `forward_step_eval` function is a nested function that computes metrics from the predictions made by the model on a data iterator.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm_demo.py\":91-115",
            "content": "    )\n    strategy = BaseStrategy(temperature=temperature, top_p=0.4, top_k=1, end_tokens=[tokenizer.eos_token_id])\n    # strategy = BeamSearchStrategy(temperature=temperature, top_p=top_p, top_k=top_k, end_tokens=[tokenizer.eos_token_id],\n    #                               num_beams=num_beams, consider_end=True)\n    get_func = llama2_text_processor_inference.get_func(None, None, image_rope_mask=kwargs['image_rope_mask'])\n    output = filling_sequence(\n        model, seq,\n        batch_size=1,\n        strategy=strategy,\n        get_masks_and_position_ids=get_func,\n        **kwargs\n    )[0]  # drop memory\n    return output\ndef forward_step_eval(data_iterator, model, args, timers):\n    def compute_metrics(eval_preds):\n        preds, labels, device = eval_preds\n        preds = preds.unsqueeze(0)\n        if isinstance(preds, tuple):\n            preds = preds[0]\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n        if args.ignore_pad_token_for_loss:\n            # Replace -100 in the labels as we can't decode them."
        },
        {
            "comment": "This code calculates the accuracy and case-insensitive accuracy of a model's predictions against true labels, then returns these scores. It does so by first converting non-special tokens to the pad token in the 'labels' variable and decoding these labels using the tokenizer. The code then loops over pairs of predicted and true labels, incrementing appropriate score lists depending on whether they match or not. Finally, it calculates the mean for each score list and returns them as a dictionary. The code also times how long the data batch generator takes to execute.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm_demo.py\":116-146",
            "content": "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        score_dict = {\n            \"acc\": [],\n            \"acc_w/o_case\": [],\n        }\n        for pred, label in zip(decoded_preds, decoded_labels):\n            if args.rank == 0:\n                print('pred', pred, 'label', label, flush=True)\n            if pred == label:\n                score_dict['acc'].append(1.)\n            else:\n                score_dict['acc'].append(0.)\n            if pred.lower() == label.lower():\n                score_dict['acc_w/o_case'].append(1.)\n            else:\n                score_dict['acc_w/o_case'].append(0.)\n        for k, v in score_dict.items():\n            score_dict[k] = float(np.mean(v))\n        return score_dict\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)\n    timers('batch generator').stop()\n    context_len = int(data_b['context_length'][0])"
        },
        {
            "comment": "This code is preparing the input data for a model to predict an answer. It selects specific parts of the data, removes irrelevant fields, and applies a mixin to enable autoregressive behavior. The model then generates a response, which is processed further before returning a result.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm_demo.py\":147-169",
            "content": "    tokens = data_b['input_ids'][:, :context_len]\n    data_b['vision_expert_mask'] = data_b['vision_expert_mask'][:, :context_len]\n    data_b['image_embed_mask'] = data_b['image_embed_mask'][:, :context_len]\n    data_b['image_rope_mask'] = data_b['image_rope_mask'][:, :context_len]\n    data_b.pop('input_ids')\n    data_b.pop('attention_mask')\n    data_b.pop('position_ids')\n    labels = data_b.pop('labels')\n    qid = data_b.pop('question_id')\n    model.add_mixin('auto-regressive', CachedAutoregressiveMixin())\n    outputs = chat(model, tokenizer, tokens, **data_b)[0][context_len:]\n    # print(outputs)\n    model.del_mixin('auto-regressive')\n    return torch.tensor(0, device=outputs.device), {k: torch.tensor(v, device=outputs.device) for k, v in\n                                                    compute_metrics(\n                                                        (outputs.cpu(), labels.cpu(), outputs.device)).items()}\nfrom torch.nn import CrossEntropyLoss\ndef forward_step(data_iterator, model, args, timers):"
        },
        {
            "comment": "This code is defining a function that performs forward step in a machine learning model, and another function for creating dataset with image and text processors.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm_demo.py\":170-198",
            "content": "    \"\"\"Forward step.\"\"\"\n    # Get the batch.\n    timers('batch generator').start()\n    data_b = get_batch(\n        data_iterator, args, timers)\n    labels = data_b.pop('labels')\n    timers('batch generator').stop()\n    logits = model(**data_b)[0]\n    lm_logits = logits.to(torch.float32)\n    # Shift so that tokens < n predict n\n    shift_labels = labels[..., 1:].contiguous()\n    shift_logits = lm_logits[..., -1-shift_labels.size(-1):-1, :].contiguous()\n    # Flatten the tokens\n    loss_fct = CrossEntropyLoss(ignore_index=-100)\n    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n    loss = loss.to(torch.float32)\n    return loss, {'loss': loss}\nfrom utils.utils import ItemDataset\ndef create_dataset_function(image_processor, text_processor, path, args):\n    dataset = ItemDataset(image_processor, text_processor, args, path)\n    return dataset\nif __name__ == '__main__':\n    py_parser = argparse.ArgumentParser(add_help=False)\n    py_parser.add_argument('--max_length', type=int)\n    py_parser.add_argument('--ignore_pad_token_for_loss', action='store_false')"
        },
        {
            "comment": "This code is parsing command-line arguments, setting default values and loading the pretrained model for a language model. It also specifies the tokenizer path and enables GPU usage if available.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm_demo.py\":199-214",
            "content": "    py_parser.add_argument(\"--version\", type=str, default=\"chat\", help='version to interact with')\n    py_parser.add_argument(\"--from_pretrained\", type=str, default=\"cogvlm-chat\", help='pretrained ckpt')\n    py_parser.add_argument(\"--local_tokenizer\", type=str, default=\"lmsys/vicuna-7b-v1.5\", help='tokenizer path')\n    py_parser.add_argument(\"--vit_checkpoint_activations\", action='store_true')\n    py_parser = FineTuneTestCogVLMModel.add_model_specific_args(py_parser)\n    known, args_list = py_parser.parse_known_args()\n    args = get_args(args_list)\n    args = argparse.Namespace(**vars(args), **vars(known))\n    if args.use_qlora:\n        args.device = 'cpu'\n    model, args = FineTuneTestCogVLMModel.from_pretrained(args.from_pretrained, args, overwrite_args={'model_parallel_size': args.model_parallel_size} if args.model_parallel_size != 1 else {})\n    if args.use_qlora and torch.cuda.is_available():\n        model = model.to('cuda')\n    from utils.utils import llama2_tokenizer\n    tokenizer = llama2_tokenizer(args.local_tokenizer, signal_type=args.version)"
        },
        {
            "comment": "Creating image and text processors for model evaluation.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/finetune_demo/evaluate_cogvlm_demo.py\":215-218",
            "content": "    image_processor = get_image_processor(args.eva_args[\"image_size\"][0])\n    text_processor = llama2_text_processor(tokenizer, args.max_length, args.image_length)\n    training_main(args, model_cls=model, forward_step_function=forward_step, create_dataset_function=partial(create_dataset_function, image_processor, text_processor), collate_fn=data_collator, forward_step_eval=forward_step_eval)"
        }
    ]
}
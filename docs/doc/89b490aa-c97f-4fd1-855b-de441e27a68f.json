{
    "summary": "The process_image function processes images and applies optional processors, efficiently handling various inputs such as image URLs, prompts, history, slices, device conversions, and manages memory for text generation in chat mode. It also interacts with a text processor to parse or store responses, returning the processed response, updated history, and image tuple.",
    "details": [
        {
            "comment": "Function process_image takes an image path and optional img_processor and cross_img_processor functions, and returns a dictionary of image data. If the image is None or does not exist, it downloads the image from the specified URL if applicable. It then converts the image to RGB format and applies the img_processor function to generate img_dict. If cross_img_processor function is provided and not None, it also processes the same image using that function and stores the output in cross_img_dict.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/chat.py\":0-29",
            "content": "# -*- encoding: utf-8 -*-\n'''\n@File    :   chat.py\n@Time    :   2023/05/08 19:10:08\n@Author  :   Ming Ding \n@Contact :   dm18@mails.tsinghua.edu.cn\n'''\nfrom typing import Optional, Tuple, Union, List, Callable, Dict, Any\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport torch\nfrom sat.generation.autoregressive_sampling import filling_sequence, stream_filling_sequence, get_masks_and_position_ids_default\nfrom sat.generation.sampling_strategies import BaseStrategy, BeamSearchStrategy\nfrom sat.mpu import get_model_parallel_rank\ndef process_image(image_path, img_processor, cross_img_processor, image):\n    if image is None:\n        if image_path.startswith(\"http\"):\n            response = requests.get(image_path, timeout=10)\n            image = Image.open(BytesIO(response.content))\n        else:\n            image = Image.open(image_path)\n    if image is not None and isinstance(image, Image.Image):\n        pil_img = image.convert('RGB')\n        img_dict = img_processor(pil_img)\n        cross_img_dict = cross_img_processor(pil_img) if cross_img_processor is not None else {}"
        },
        {
            "comment": "This function takes an image path, model, text_processor, img_processor, query, history, cross_img_processor (optional), image (optional), max_length, top_p, top_k, temperature, repetition_penalty, invalid_slices, no_prompt (optional), and args. It processes the image and returns a tuple containing the processed image as torch_image, pil_image, and cross_image if an image is provided or None otherwise. If no prompt is given, it sets query to an empty string. It also handles history and invalid_slices parameters.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/chat.py\":30-54",
            "content": "        ret = (img_dict, pil_img, cross_img_dict)\n    else:\n        ret = image\n    return ret\ndef chat(image_path, model, text_processor, img_processor,\n        query: str, history: List[Tuple[str, str]] = None, cross_img_processor=None, image: Image = None,\n        max_length: int = 4096, top_p=0.95, top_k=5, temperature=0.95, repetition_penalty=1.0,\n        invalid_slices=[], no_prompt=False, args=None\n        ):\n    if image is None:\n        assert image_path is not None\n    if not history:\n        history = []\n    if no_prompt:\n        query = ''\n    prompt = text_processor.history_to_prompt(query, history)\n    (torch_image, pil_img, cross_image) = process_image(image_path, img_processor, cross_img_processor, image)\n    if torch_image is not None:\n        for k in torch_image:\n            if type(torch_image[k]) is torch.Tensor and torch_image[k].dtype is not torch.int and torch_image[k].dtype is not torch.long:\n                torch_image[k] = torch_image[k].to(torch.bfloat16 if args.bf16 else torch.float16)"
        },
        {
            "comment": "The code is ensuring that the tensors in torch_image, cross_image and inputs_dic are converted to specific data types (torch.bfloat16 or torch.float16) if not already, and moving them to the same device as the model's parameters for efficient computation during inference.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/chat.py\":55-71",
            "content": "            if type(torch_image[k]) is torch.Tensor:\n                torch_image[k] = torch_image[k].to(next(model.parameters()).device)\n    if cross_image is not None:\n        for k in cross_image:\n            if type(cross_image[k]) is torch.Tensor and cross_image[k].dtype is not torch.int and cross_image[k].dtype is not torch.long:\n                cross_image[k] = cross_image[k].to(torch.bfloat16 if args.bf16 else torch.float16)\n            if type(cross_image[k]) is torch.Tensor:\n                cross_image[k] = cross_image[k].to(next(model.parameters()).device)\n    inputs_dic = text_processor(prompt)\n    for k in inputs_dic:\n        if type(inputs_dic[k]) is torch.Tensor and inputs_dic[k].dtype is not torch.int and inputs_dic[k].dtype is not torch.long:\n            inputs_dic[k] = inputs_dic[k].to(torch.bfloat16 if args.bf16 else torch.float16)\n        if type(inputs_dic[k]) is torch.Tensor:\n            inputs_dic[k] = inputs_dic[k].to(next(model.parameters()).device)\n    input_ids = inputs_dic['input_ids'].to(model.parameters().__next__().device)[0]"
        },
        {
            "comment": "Checks if the input prompt exceeds the maximum length limit, then pads or truncates the input_ids and sets up the strategy for beam search.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/chat.py\":73-87",
            "content": "    if max_length-len(input_ids) <= 1:\n        response = \"The prompt exceeds the context length limit, please try again.\"\n        return response, history, (torch_image, pil_img)\n    seq = torch.cat(\n        [input_ids, torch.tensor([-1]*(max_length-len(input_ids)), device=input_ids.device)], dim=0\n    )\n    strategy = BaseStrategy(temperature=temperature, top_p=top_p, top_k=top_k, end_tokens=[text_processor.tokenizer.eos_token_id],\n                            invalid_slices=invalid_slices, repetition_penalty=repetition_penalty)\n    # use beam search to get a better result\n    # strategy = BeamSearchStrategy(temperature=temperature, top_p=top_p, top_k=top_k, end_tokens=[text_processor.tokenizer.eos_token_id],\n    #                               num_beams=5, consider_end=True, repetition_penalty=repetition_penalty)\n    get_func = text_processor.get_func(input_ids, **inputs_dic) if hasattr(text_processor, 'get_func') else get_masks_and_position_ids_default\n    img_inputs = {'vision_'+k: v for k, v in torch_image.items()}"
        },
        {
            "comment": "The code is modifying the input dictionary for a model that performs stream-filling sequence prediction. It adds cross_image data to the inputs, removes 'input_ids' from the inputs dictionary, and then merges the modified inputs with other parameters. The code also prints the model name or \"\u6a21\u578b\" in Chinese if necessary, calculates the offset for decoding input tokens, and iterates over the filling_stream to extract response tokens and decode them for display. The code also ensures that the GPU memory is cleared after each iteration using torch.cuda.empty_cache().",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/chat.py\":88-113",
            "content": "    if cross_image is not None:\n        img_inputs = {**img_inputs, **{'cross_'+k:v for k,v in cross_image.items()}}\n    inputs_dic.pop('input_ids')\n    inputs = {**img_inputs, **inputs_dic}\n    if args.stream_chat:\n        filling_stream = stream_filling_sequence(\n            model, seq,\n            batch_size=1,\n            get_masks_and_position_ids=get_func,\n            strategy=strategy,\n            **inputs\n        )\n        if get_model_parallel_rank() == 0:\n            if 'chinese' in args and not args.chinese:\n                print(\"Model: \", end='')\n            else:\n                print(\"\u6a21\u578b\uff1a\", end='')\n        offset = len(text_processor.tokenizer.decode(input_ids))\n        for tokens, mems in filling_stream:\n            torch.cuda.empty_cache()\n            tmp_response = text_processor.tokenizer.decode(tokens[0])\n            if tmp_response[-1] != \"\ufffd\":\n                if get_model_parallel_rank() == 0:\n                    tmp_response_offseted = tmp_response[offset:]\n                    if hasattr(text_processor, 'process_response'):"
        },
        {
            "comment": "This code is handling text generation for chat mode. It processes the response, prints it, and then finalizes the output by decoding it into a readable format. If not in chat mode, it fills generated things into seq, clips -1s, and converts the output to a list for decoding. Finally, it checks if there is a 'process_response' function available in the text_processor and prints the original response if present.",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/chat.py\":114-141",
            "content": "                        tmp_response_offseted = text_processor.process_response(tmp_response_offseted)\n                    print(tmp_response_offseted, end='', flush=True)\n                offset = len(tmp_response)\n        if get_model_parallel_rank() == 0:\n            print()\n        output = strategy.finalize(tokens, mems)[0]\n        response = text_processor.tokenizer.decode(output[0])\n    else:\n        output = filling_sequence(\n            model, seq,\n            batch_size=1,\n            get_masks_and_position_ids=get_func,\n            strategy=strategy,\n            **inputs\n        )[0] # drop memory\n        # ---------------\n        # port from inference_glm.py, more general than chat mode\n        # clip -1s and fill back generated things into seq\n        if type(output) is not list:\n            output_list = output.tolist()\n        else:\n            output_list = output\n        response = text_processor.tokenizer.decode(output_list[0])\n    # print('original:', response)\n    if hasattr(text_processor, 'process_response'):"
        },
        {
            "comment": "This code segment is processing the response from a text processor, splitting it if necessary, and then either parsing the response using grounding_parser or adding the response to the history list. The function also returns the processed response, the updated history, and an image tuple (torch_image, pil_img, cross_image).",
            "location": "\"/media/root/Toshiba XG3/works/CogVLM/docs/src/utils/utils/chat.py\":142-148",
            "content": "        response = text_processor.process_response(response)\n    response = response.split(text_processor.sep)[-1].strip()\n    if get_model_parallel_rank() == 0:\n        from utils.utils.grounding_parser import parse_response\n        parse_response(pil_img, response)\n    history = history + [(query, response)]\n    return response, history, (torch_image, pil_img, cross_image)"
        }
    ]
}